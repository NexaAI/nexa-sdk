{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AutoNeural - NPU-Native Multimodal Model for Automotive Cockpits\n",
        "\n",
        "This notebook demonstrates the 4 core use cases of AutoNeural:\n",
        "\n",
        "1. **In-Cabin Detection** - Detect risky behaviors and situations\n",
        "2. **Out-Cabin Awareness** - Read parking signs and surrounding context\n",
        "3. **HMI & Dashboard Understanding** - Understand dashboards and warning lights\n",
        "4. **Visual + Conversational Agentic Tasks** - Ground natural language in visual context\n",
        "\n",
        "> **Note:** AutoNeural is optimized for Qualcomm NPUs and requires proper authentication setup.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n",
        "\n",
        "### 1. Install the correct Python version\n",
        "\n",
        "If you prefer, we also offer a video tutorial for the installation. Check it out [here](https://www.youtube.com/watch?v=ziXKPRX0Ufo).\n",
        "\n",
        "NexaAI requires **Python 3.11 â€“ 3.13 (ARM64 build)** on Windows ARM.\n",
        "Please **download and install the official ARM64 Python** from the [python-3.11.1-arm64.exe](https://www.python.org/ftp/python/3.11.1/python-3.11.1-arm64.exe). Make sure you read the instructions below carefully before proceeding.\n",
        "\n",
        "> â— **IMPORTANT**: Make sure you select \"Add python.exe to PATH\" on the first screen of the installation wizard.\n",
        "\n",
        "> ðŸ›‘ Make sure you restart the terminal or your IDE after installation.\n",
        "\n",
        "> âš ï¸ Do **not** use Conda or x86 builds â€” they are incompatible with native ARM64 binaries. If you are in a conda environment, run `conda deactivate` first.\n",
        "\n",
        "Verify the installation:\n",
        "\n",
        "In case your environment path gets overriden by some environment manager, we recommend you to run the following commands to restore PATH variable from system settings.\n",
        "```powershell\n",
        "$systemPath = [Environment]::GetEnvironmentVariable('Path', 'Machine')\n",
        "$userPath   = [Environment]::GetEnvironmentVariable('Path', 'User')\n",
        "$env:Path   = \"$userPath;$systemPath\"\n",
        "```\n",
        "\n",
        "Then verify your python executable has the correct architecture and version (3.11 - 3.13)\n",
        "```sh\n",
        "python -c \"import sys, platform; print(f'Python version: {sys.version}')\"\n",
        "```\n",
        "\n",
        "Your output should look like:\n",
        "\n",
        "> Python version: 3.11.0 (main, Oct 24 2022, 18:15:22) [MSC v.1933 64 bit (ARM64)]\n",
        "\n",
        "Expected output must contain version `3.11.0` and architecture `ARM64`.\n",
        "\n",
        "If it does show `AMD64` or incorrect version, try the following:\n",
        "\n",
        "- (If you have conda installed) Run `conda deactivate` to deactivate the current conda environment.\n",
        "- (If your `python` executable points to the x86 version) You may need to make the ARM64 Python come before the x86 Python in your PATH.\n",
        "  - Hit the `Win` key, and type `env`, and hit Enter to select `Edit the system environment variables` setting.\n",
        "  - Click on `Environment Variables...` button.\n",
        "  - Select `Path` and click `Edit...`.\n",
        "  - Find your ARM64 Python installation path, and move it to the top of the list.\n",
        "  - Hit `OK` for several times to close all the dialogs and save the changes.\n",
        "- (If you forgot to select \"Add python.exe to PATH\" on the first screen of the installation wizard)\n",
        "  - Run the installation wizard again, follow the instructions to remove the current installation, and then reinstall from the Wizard. Make sure to select \"Add python.exe to PATH\" this time.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Create and activate a virtual environment\n",
        "\n",
        "`cd` to the current project root directory `cd path/to/nexa-sdk`.\n",
        "\n",
        "```sh\n",
        "python -m venv nexaai-env\n",
        "nexaai-env\\Scripts\\activate\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Install the NexaAI SDK\n",
        "\n",
        "```bash\n",
        "pip install nexaai\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 4. Select the venv as your Jupyter Notebook kernel\n",
        "\n",
        "- Depending the editor you are using, the way to change kernel might be different. For Cursor / VS Code, they are located at the top right corner of you code window.\n",
        "- Look for and select the `nexaai-env`, or the custom virtual environment you have created. The kernel should automatically reload in most IDEs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Authentication Setup\n",
        "\n",
        "Before running any examples, you need to set up your NexaAI authentication token.\n",
        "\n",
        "### Set Token in Code\n",
        "\n",
        "Replace `\"YOUR_NEXA_TOKEN_HERE\"` with your actual NexaAI token from [https://sdk.nexa.ai/](https://sdk.nexa.ai/):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Replace \"YOUR_NEXA_TOKEN_HERE\" with your actual token from https://sdk.nexa.ai/\n",
        "os.environ[\"NEXA_TOKEN\"] = \"YOUR_NEXA_TOKEN_HERE\"\n",
        "# Suppress HF warnings\n",
        "os.environ[\"HF_HUB_VERBOSITY\"] = \"error\"\n",
        "\n",
        "assert os.environ.get(\"NEXA_TOKEN\", \"\").startswith(\n",
        "    \"key/\"), \"ERROR: NEXA_TOKEN must start with 'key/'. Please check your token.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Model Loading\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from IPython.display import Image, display\n",
        "from nexaai.vlm import VLM, GenerationConfig\n",
        "from nexaai.common import ModelConfig, MultiModalMessage\n",
        "\n",
        "# Configure paths\n",
        "script_dir = Path(\".\").resolve()\n",
        "assets_dir = script_dir / \"images\"\n",
        "\n",
        "print(\"Loading AutoNeural model...\")\n",
        "vlm = VLM.from_(name_or_path=\"NexaAI/AutoNeural\", m_cfg= ModelConfig(), plugin_id=\"npu\")\n",
        "print(\"âœ… Model loaded successfully!\")\n",
        "\n",
        "def run_example(vlm: VLM, image_path: str | Path, prompt: str, max_tokens: int = 512):\n",
        "    \"\"\"Run a single VLM inference example.\"\"\"\n",
        "    image_path_str = str(image_path)\n",
        "    \n",
        "    conversation: list[MultiModalMessage] = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": \"You are a helpful automotive assistant.\",\n",
        "                    \"url\": None,\n",
        "                    \"path\": None,\n",
        "                }\n",
        "            ],\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image\", \"path\": image_path_str, \"text\": None, \"url\": None},\n",
        "                {\"type\": \"text\", \"text\": prompt, \"url\": None, \"path\": None},\n",
        "            ],\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    formatted_prompt = vlm.apply_chat_template(conversation)\n",
        "    \n",
        "    response = \"\"\n",
        "    for token in vlm.generate_stream(\n",
        "        formatted_prompt,\n",
        "        g_cfg=GenerationConfig(max_tokens=max_tokens, image_paths=[image_path_str]),\n",
        "    ):\n",
        "        print(token, end=\"\", flush=True)\n",
        "        response += token\n",
        "    \n",
        "    return response\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 1: In-Cabin Detection\n",
        "\n",
        "Detect risky behaviors and situations in the cabin, such as driver distraction, drowsiness, children seated unsafely, or passengers blocking visibility.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image_path = assets_dir / \"in-cabin.png\"\n",
        "display(Image(str(image_path), width=600))\n",
        "\n",
        "prompt = \"Any safety risks for my child right now? Explain it in great detail.\"\n",
        "\n",
        "print(\"Prompt:\", prompt)\n",
        "print(\"\\nResponse:\")\n",
        "response = run_example(vlm, image_path, prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 2: Out-Cabin Awareness\n",
        "\n",
        "Read real-world parking signs and surrounding context to answer \"Can I park here?\" type questions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image_path = assets_dir / \"out-cabin.png\"\n",
        "display(Image(str(image_path), width=600))\n",
        "\n",
        "prompt = \"What are the parking rules here? Provide detailed explanation\"\n",
        "\n",
        "print(\"Prompt:\", prompt)\n",
        "print(\"\\nResponse:\")\n",
        "response = run_example(vlm, image_path, prompt)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 3: HMI & Dashboard Understanding\n",
        "\n",
        "Understand dashboards, warning lights, and infotainment UIs and explain them in natural language.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image_path = assets_dir / \"dashboard.png\"\n",
        "display(Image(str(image_path), width=600))\n",
        "\n",
        "prompt = \"How do I address this issue? what does the issue mean?\"\n",
        "\n",
        "print(\"Prompt:\", prompt)\n",
        "print(\"\\nResponse:\")\n",
        "response = run_example(vlm, image_path, prompt)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 4: Visual + Conversational Agentic Tasks\n",
        "\n",
        "Ground natural language requests in visual context and emit structured outputs that your app can consume.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image_path = assets_dir / \"UI.png\"\n",
        "display(Image(str(image_path), width=600))\n",
        "\n",
        "prompt = \"Can you navigate me to the event?\"\n",
        "\n",
        "print(\"Prompt:\", prompt)\n",
        "print(\"\\nResponse:\")\n",
        "response = run_example(vlm, image_path, prompt)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "All examples have been completed! AutoNeural demonstrates its capabilities across:\n",
        "\n",
        "- âœ… **In-Cabin Safety** - Real-time detection of risky behaviors\n",
        "- âœ… **Out-Cabin Awareness** - Understanding parking signs and context\n",
        "- âœ… **Dashboard Intelligence** - Interpreting vehicle status and UI\n",
        "- âœ… **Agentic Tasks** - Structured visual understanding for applications\n",
        "\n",
        "For more information, visit the [AutoNeural model page](https://huggingface.co/NexaAI/AutoNeural) or check the [README](./README.md).\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
