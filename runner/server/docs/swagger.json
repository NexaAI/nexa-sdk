{
    "swagger": "2.0",
    "info": {
        "title": "Nexa AI Server",
        "contact": {},
        "version": "0.0.0"
    },
    "basePath": "/v1",
    "paths": {
        "/chat/completions": {
            "post": {
                "description": "This endpoint generates a model response for a given conversation, which can include text and images. It supports both single-turn and multi-turn conversations and can be used for various tasks like question answering, code generation, and function calling.",
                "consumes": [
                    "application/json"
                ],
                "produces": [
                    "application/json"
                ],
                "summary": "Creates a model response for the given chat conversation.",
                "parameters": [
                    {
                        "description": "Chat completion request",
                        "name": "request",
                        "in": "body",
                        "required": true,
                        "schema": {
                            "$ref": "#/definitions/handler.ChatCompletionRequest"
                        }
                    }
                ],
                "responses": {
                    "200": {
                        "description": "Successful response for non-streaming requests.",
                        "schema": {
                            "$ref": "#/definitions/openai.ChatCompletion"
                        }
                    }
                }
            }
        },
        "/completions": {
            "post": {
                "description": "Legacy completion endpoint for text generation. It is recommended to use the Chat Completions endpoint for new applications.",
                "consumes": [
                    "application/json"
                ],
                "produces": [
                    "application/json"
                ],
                "summary": "completion",
                "parameters": [
                    {
                        "description": "Completion request",
                        "name": "request",
                        "in": "body",
                        "required": true,
                        "schema": {
                            "$ref": "#/definitions/openai.CompletionNewParams"
                        }
                    }
                ],
                "responses": {
                    "200": {
                        "description": "OK",
                        "schema": {
                            "$ref": "#/definitions/openai.Completion"
                        }
                    }
                }
            }
        },
        "/embeddings": {
            "post": {
                "description": "Creates an embedding for the given input.",
                "consumes": [
                    "application/json"
                ],
                "summary": "Creates an embedding for the given input.",
                "parameters": [
                    {
                        "description": "Embedding request",
                        "name": "request",
                        "in": "body",
                        "required": true,
                        "schema": {
                            "$ref": "#/definitions/openai.EmbeddingNewParams"
                        }
                    }
                ],
                "responses": {}
            }
        },
        "/reranking": {
            "post": {
                "description": "Reranks the given documents for the given query.",
                "consumes": [
                    "application/json"
                ],
                "summary": "Reranks the given documents for the given query.",
                "parameters": [
                    {
                        "description": "Reranking request",
                        "name": "request",
                        "in": "body",
                        "required": true,
                        "schema": {
                            "$ref": "#/definitions/handler.RerankingRequest"
                        }
                    }
                ],
                "responses": {}
            }
        }
    },
    "definitions": {
        "handler.ChatCompletionRequest": {
            "type": "object"
        },
        "handler.RerankingRequest": {
            "type": "object"
        },
        "openai.ChatCompletion": {
            "type": "object",
            "properties": {
                "choices": {
                    "description": "A list of chat completion choices. Can be more than one if `n` is greater\nthan 1.",
                    "type": "array",
                    "items": {
                        "$ref": "#/definitions/openai.ChatCompletionChoice"
                    }
                },
                "created": {
                    "description": "The Unix timestamp (in seconds) of when the chat completion was created.",
                    "type": "integer"
                },
                "id": {
                    "description": "A unique identifier for the chat completion.",
                    "type": "string"
                },
                "model": {
                    "description": "The model used for the chat completion.",
                    "type": "string"
                },
                "object": {
                    "description": "The object type, which is always `chat.completion`.",
                    "type": "string"
                },
                "service_tier": {
                    "description": "Specifies the latency tier to use for processing the request. This parameter is\nrelevant for customers subscribed to the scale tier service:\n\n  - If set to 'auto', and the Project is Scale tier enabled, the system will\n    utilize scale tier credits until they are exhausted.\n  - If set to 'auto', and the Project is not Scale tier enabled, the request will\n    be processed using the default service tier with a lower uptime SLA and no\n    latency guarantee.\n  - If set to 'default', the request will be processed using the default service\n    tier with a lower uptime SLA and no latency guarantee.\n  - If set to 'flex', the request will be processed with the Flex Processing\n    service tier.\n    [Learn more](https://platform.openai.com/docs/guides/flex-processing).\n  - When not set, the default behavior is 'auto'.\n\nWhen this parameter is set, the response body will include the `service_tier`\nutilized.\n\nAny of \"auto\", \"default\", \"flex\", \"scale\".",
                    "allOf": [
                        {
                            "$ref": "#/definitions/openai.ChatCompletionServiceTier"
                        }
                    ]
                },
                "system_fingerprint": {
                    "description": "This fingerprint represents the backend configuration that the model runs with.\n\nCan be used in conjunction with the `seed` request parameter to understand when\nbackend changes have been made that might impact determinism.",
                    "type": "string"
                },
                "usage": {
                    "description": "Usage statistics for the completion request.",
                    "allOf": [
                        {
                            "$ref": "#/definitions/openai.CompletionUsage"
                        }
                    ]
                }
            }
        },
        "openai.ChatCompletionAudio": {
            "type": "object",
            "properties": {
                "data": {
                    "description": "Base64 encoded audio bytes generated by the model, in the format specified in\nthe request.",
                    "type": "string"
                },
                "expires_at": {
                    "description": "The Unix timestamp (in seconds) for when this audio response will no longer be\naccessible on the server for use in multi-turn conversations.",
                    "type": "integer"
                },
                "id": {
                    "description": "Unique identifier for this audio response.",
                    "type": "string"
                },
                "transcript": {
                    "description": "Transcript of the audio generated by the model.",
                    "type": "string"
                }
            }
        },
        "openai.ChatCompletionChoice": {
            "type": "object",
            "properties": {
                "finish_reason": {
                    "description": "The reason the model stopped generating tokens. This will be `stop` if the model\nhit a natural stop point or a provided stop sequence, `length` if the maximum\nnumber of tokens specified in the request was reached, `content_filter` if\ncontent was omitted due to a flag from our content filters, `tool_calls` if the\nmodel called a tool, or `function_call` (deprecated) if the model called a\nfunction.\n\nAny of \"stop\", \"length\", \"tool_calls\", \"content_filter\", \"function_call\".",
                    "type": "string"
                },
                "index": {
                    "description": "The index of the choice in the list of choices.",
                    "type": "integer"
                },
                "logprobs": {
                    "description": "Log probability information for the choice.",
                    "allOf": [
                        {
                            "$ref": "#/definitions/openai.ChatCompletionChoiceLogprobs"
                        }
                    ]
                },
                "message": {
                    "description": "A chat completion message generated by the model.",
                    "allOf": [
                        {
                            "$ref": "#/definitions/openai.ChatCompletionMessage"
                        }
                    ]
                }
            }
        },
        "openai.ChatCompletionChoiceLogprobs": {
            "type": "object",
            "properties": {
                "content": {
                    "description": "A list of message content tokens with log probability information.",
                    "type": "array",
                    "items": {
                        "$ref": "#/definitions/openai.ChatCompletionTokenLogprob"
                    }
                },
                "refusal": {
                    "description": "A list of message refusal tokens with log probability information.",
                    "type": "array",
                    "items": {
                        "$ref": "#/definitions/openai.ChatCompletionTokenLogprob"
                    }
                }
            }
        },
        "openai.ChatCompletionMessage": {
            "type": "object",
            "properties": {
                "annotations": {
                    "description": "Annotations for the message, when applicable, as when using the\n[web search tool](https://platform.openai.com/docs/guides/tools-web-search?api-mode=chat).",
                    "type": "array",
                    "items": {
                        "$ref": "#/definitions/openai.ChatCompletionMessageAnnotation"
                    }
                },
                "audio": {
                    "description": "If the audio output modality is requested, this object contains data about the\naudio response from the model.\n[Learn more](https://platform.openai.com/docs/guides/audio).",
                    "allOf": [
                        {
                            "$ref": "#/definitions/openai.ChatCompletionAudio"
                        }
                    ]
                },
                "content": {
                    "description": "The contents of the message.",
                    "type": "string"
                },
                "function_call": {
                    "description": "Deprecated and replaced by `tool_calls`. The name and arguments of a function\nthat should be called, as generated by the model.\n\nDeprecated: deprecated",
                    "allOf": [
                        {
                            "$ref": "#/definitions/openai.ChatCompletionMessageFunctionCall"
                        }
                    ]
                },
                "refusal": {
                    "description": "The refusal message generated by the model.",
                    "type": "string"
                },
                "role": {
                    "description": "The role of the author of this message.",
                    "type": "string"
                },
                "tool_calls": {
                    "description": "The tool calls generated by the model, such as function calls.",
                    "type": "array",
                    "items": {
                        "$ref": "#/definitions/openai.ChatCompletionMessageToolCall"
                    }
                }
            }
        },
        "openai.ChatCompletionMessageAnnotation": {
            "type": "object",
            "properties": {
                "type": {
                    "description": "The type of the URL citation. Always `url_citation`.",
                    "type": "string"
                },
                "url_citation": {
                    "description": "A URL citation when using web search.",
                    "allOf": [
                        {
                            "$ref": "#/definitions/openai.ChatCompletionMessageAnnotationURLCitation"
                        }
                    ]
                }
            }
        },
        "openai.ChatCompletionMessageAnnotationURLCitation": {
            "type": "object",
            "properties": {
                "end_index": {
                    "description": "The index of the last character of the URL citation in the message.",
                    "type": "integer"
                },
                "start_index": {
                    "description": "The index of the first character of the URL citation in the message.",
                    "type": "integer"
                },
                "title": {
                    "description": "The title of the web resource.",
                    "type": "string"
                },
                "url": {
                    "description": "The URL of the web resource.",
                    "type": "string"
                }
            }
        },
        "openai.ChatCompletionMessageFunctionCall": {
            "type": "object",
            "properties": {
                "arguments": {
                    "description": "The arguments to call the function with, as generated by the model in JSON\nformat. Note that the model does not always generate valid JSON, and may\nhallucinate parameters not defined by your function schema. Validate the\narguments in your code before calling your function.",
                    "type": "string"
                },
                "name": {
                    "description": "The name of the function to call.",
                    "type": "string"
                }
            }
        },
        "openai.ChatCompletionMessageToolCall": {
            "type": "object",
            "properties": {
                "function": {
                    "description": "The function that the model called.",
                    "allOf": [
                        {
                            "$ref": "#/definitions/openai.ChatCompletionMessageToolCallFunction"
                        }
                    ]
                },
                "id": {
                    "description": "The ID of the tool call.",
                    "type": "string"
                },
                "type": {
                    "description": "The type of the tool. Currently, only `function` is supported.",
                    "type": "string"
                }
            }
        },
        "openai.ChatCompletionMessageToolCallFunction": {
            "type": "object",
            "properties": {
                "arguments": {
                    "description": "The arguments to call the function with, as generated by the model in JSON\nformat. Note that the model does not always generate valid JSON, and may\nhallucinate parameters not defined by your function schema. Validate the\narguments in your code before calling your function.",
                    "type": "string"
                },
                "name": {
                    "description": "The name of the function to call.",
                    "type": "string"
                }
            }
        },
        "openai.ChatCompletionServiceTier": {
            "type": "string",
            "enum": [
                "auto",
                "default",
                "flex",
                "scale"
            ],
            "x-enum-varnames": [
                "ChatCompletionServiceTierAuto",
                "ChatCompletionServiceTierDefault",
                "ChatCompletionServiceTierFlex",
                "ChatCompletionServiceTierScale"
            ]
        },
        "openai.ChatCompletionStreamOptionsParam": {
            "type": "object",
            "properties": {
                "any": {},
                "include_usage": {
                    "description": "If set, an additional chunk will be streamed before the `data: [DONE]` message.\nThe `usage` field on this chunk shows the token usage statistics for the entire\nrequest, and the `choices` field will always be an empty array.\n\nAll other chunks will also include a `usage` field, but with a null value.\n**NOTE:** If the stream is interrupted, you may not receive the final usage\nchunk which contains the total token usage for the request.",
                    "allOf": [
                        {
                            "$ref": "#/definitions/param.Opt-bool"
                        }
                    ]
                }
            }
        },
        "openai.ChatCompletionTokenLogprob": {
            "type": "object",
            "properties": {
                "bytes": {
                    "description": "A list of integers representing the UTF-8 bytes representation of the token.\nUseful in instances where characters are represented by multiple tokens and\ntheir byte representations must be combined to generate the correct text\nrepresentation. Can be `null` if there is no bytes representation for the token.",
                    "type": "array",
                    "items": {
                        "type": "integer"
                    }
                },
                "logprob": {
                    "description": "The log probability of this token, if it is within the top 20 most likely\ntokens. Otherwise, the value `-9999.0` is used to signify that the token is very\nunlikely.",
                    "type": "number"
                },
                "token": {
                    "description": "The token.",
                    "type": "string"
                },
                "top_logprobs": {
                    "description": "List of the most likely tokens and their log probability, at this token\nposition. In rare cases, there may be fewer than the number of requested\n`top_logprobs` returned.",
                    "type": "array",
                    "items": {
                        "$ref": "#/definitions/openai.ChatCompletionTokenLogprobTopLogprob"
                    }
                }
            }
        },
        "openai.ChatCompletionTokenLogprobTopLogprob": {
            "type": "object",
            "properties": {
                "bytes": {
                    "description": "A list of integers representing the UTF-8 bytes representation of the token.\nUseful in instances where characters are represented by multiple tokens and\ntheir byte representations must be combined to generate the correct text\nrepresentation. Can be `null` if there is no bytes representation for the token.",
                    "type": "array",
                    "items": {
                        "type": "integer"
                    }
                },
                "logprob": {
                    "description": "The log probability of this token, if it is within the top 20 most likely\ntokens. Otherwise, the value `-9999.0` is used to signify that the token is very\nunlikely.",
                    "type": "number"
                },
                "token": {
                    "description": "The token.",
                    "type": "string"
                }
            }
        },
        "openai.Completion": {
            "type": "object",
            "properties": {
                "choices": {
                    "description": "The list of completion choices the model generated for the input prompt.",
                    "type": "array",
                    "items": {
                        "$ref": "#/definitions/openai.CompletionChoice"
                    }
                },
                "created": {
                    "description": "The Unix timestamp (in seconds) of when the completion was created.",
                    "type": "integer"
                },
                "id": {
                    "description": "A unique identifier for the completion.",
                    "type": "string"
                },
                "model": {
                    "description": "The model used for completion.",
                    "type": "string"
                },
                "object": {
                    "description": "The object type, which is always \"text_completion\"",
                    "type": "string"
                },
                "system_fingerprint": {
                    "description": "This fingerprint represents the backend configuration that the model runs with.\n\nCan be used in conjunction with the `seed` request parameter to understand when\nbackend changes have been made that might impact determinism.",
                    "type": "string"
                },
                "usage": {
                    "description": "Usage statistics for the completion request.",
                    "allOf": [
                        {
                            "$ref": "#/definitions/openai.CompletionUsage"
                        }
                    ]
                }
            }
        },
        "openai.CompletionChoice": {
            "type": "object",
            "properties": {
                "finish_reason": {
                    "description": "The reason the model stopped generating tokens. This will be `stop` if the model\nhit a natural stop point or a provided stop sequence, `length` if the maximum\nnumber of tokens specified in the request was reached, or `content_filter` if\ncontent was omitted due to a flag from our content filters.\n\nAny of \"stop\", \"length\", \"content_filter\".",
                    "allOf": [
                        {
                            "$ref": "#/definitions/openai.CompletionChoiceFinishReason"
                        }
                    ]
                },
                "index": {
                    "type": "integer"
                },
                "logprobs": {
                    "$ref": "#/definitions/openai.CompletionChoiceLogprobs"
                },
                "text": {
                    "type": "string"
                }
            }
        },
        "openai.CompletionChoiceFinishReason": {
            "type": "string",
            "enum": [
                "stop",
                "length",
                "content_filter"
            ],
            "x-enum-varnames": [
                "CompletionChoiceFinishReasonStop",
                "CompletionChoiceFinishReasonLength",
                "CompletionChoiceFinishReasonContentFilter"
            ]
        },
        "openai.CompletionChoiceLogprobs": {
            "type": "object",
            "properties": {
                "text_offset": {
                    "type": "array",
                    "items": {
                        "type": "integer"
                    }
                },
                "token_logprobs": {
                    "type": "array",
                    "items": {
                        "type": "number"
                    }
                },
                "tokens": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "top_logprobs": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "additionalProperties": {
                            "type": "number"
                        }
                    }
                }
            }
        },
        "openai.CompletionNewParams": {
            "type": "object",
            "properties": {
                "any": {},
                "best_of": {
                    "description": "Generates `best_of` completions server-side and returns the \"best\" (the one with\nthe highest log probability per token). Results cannot be streamed.\n\nWhen used with `n`, `best_of` controls the number of candidate completions and\n`n` specifies how many to return â€“ `best_of` must be greater than `n`.\n\n**Note:** Because this parameter generates many completions, it can quickly\nconsume your token quota. Use carefully and ensure that you have reasonable\nsettings for `max_tokens` and `stop`.",
                    "allOf": [
                        {
                            "$ref": "#/definitions/param.Opt-int64"
                        }
                    ]
                },
                "echo": {
                    "description": "Echo back the prompt in addition to the completion",
                    "allOf": [
                        {
                            "$ref": "#/definitions/param.Opt-bool"
                        }
                    ]
                },
                "frequency_penalty": {
                    "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on their\nexisting frequency in the text so far, decreasing the model's likelihood to\nrepeat the same line verbatim.\n\n[See more information about frequency and presence penalties.](https://platform.openai.com/docs/guides/text-generation)",
                    "allOf": [
                        {
                            "$ref": "#/definitions/param.Opt-float64"
                        }
                    ]
                },
                "logit_bias": {
                    "description": "Modify the likelihood of specified tokens appearing in the completion.\n\nAccepts a JSON object that maps tokens (specified by their token ID in the GPT\ntokenizer) to an associated bias value from -100 to 100. You can use this\n[tokenizer tool](/tokenizer?view=bpe) to convert text to token IDs.\nMathematically, the bias is added to the logits generated by the model prior to\nsampling. The exact effect will vary per model, but values between -1 and 1\nshould decrease or increase likelihood of selection; values like -100 or 100\nshould result in a ban or exclusive selection of the relevant token.\n\nAs an example, you can pass `{\"50256\": -100}` to prevent the \u003c|endoftext|\u003e token\nfrom being generated.",
                    "type": "object",
                    "additionalProperties": {
                        "type": "integer"
                    }
                },
                "logprobs": {
                    "description": "Include the log probabilities on the `logprobs` most likely output tokens, as\nwell the chosen tokens. For example, if `logprobs` is 5, the API will return a\nlist of the 5 most likely tokens. The API will always return the `logprob` of\nthe sampled token, so there may be up to `logprobs+1` elements in the response.\n\nThe maximum value for `logprobs` is 5.",
                    "allOf": [
                        {
                            "$ref": "#/definitions/param.Opt-int64"
                        }
                    ]
                },
                "max_tokens": {
                    "description": "The maximum number of [tokens](/tokenizer) that can be generated in the\ncompletion.\n\nThe token count of your prompt plus `max_tokens` cannot exceed the model's\ncontext length.\n[Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken)\nfor counting tokens.",
                    "allOf": [
                        {
                            "$ref": "#/definitions/param.Opt-int64"
                        }
                    ]
                },
                "model": {
                    "description": "ID of the model to use. You can use the\n[List models](https://platform.openai.com/docs/api-reference/models/list) API to\nsee all of your available models, or see our\n[Model overview](https://platform.openai.com/docs/models) for descriptions of\nthem.",
                    "allOf": [
                        {
                            "$ref": "#/definitions/openai.CompletionNewParamsModel"
                        }
                    ]
                },
                "n": {
                    "description": "How many completions to generate for each prompt.\n\n**Note:** Because this parameter generates many completions, it can quickly\nconsume your token quota. Use carefully and ensure that you have reasonable\nsettings for `max_tokens` and `stop`.",
                    "allOf": [
                        {
                            "$ref": "#/definitions/param.Opt-int64"
                        }
                    ]
                },
                "presence_penalty": {
                    "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on\nwhether they appear in the text so far, increasing the model's likelihood to\ntalk about new topics.\n\n[See more information about frequency and presence penalties.](https://platform.openai.com/docs/guides/text-generation)",
                    "allOf": [
                        {
                            "$ref": "#/definitions/param.Opt-float64"
                        }
                    ]
                },
                "prompt": {
                    "description": "The prompt(s) to generate completions for, encoded as a string, array of\nstrings, array of tokens, or array of token arrays.\n\nNote that \u003c|endoftext|\u003e is the document separator that the model sees during\ntraining, so if a prompt is not specified the model will generate as if from the\nbeginning of a new document.",
                    "allOf": [
                        {
                            "$ref": "#/definitions/openai.CompletionNewParamsPromptUnion"
                        }
                    ]
                },
                "seed": {
                    "description": "If specified, our system will make a best effort to sample deterministically,\nsuch that repeated requests with the same `seed` and parameters should return\nthe same result.\n\nDeterminism is not guaranteed, and you should refer to the `system_fingerprint`\nresponse parameter to monitor changes in the backend.",
                    "allOf": [
                        {
                            "$ref": "#/definitions/param.Opt-int64"
                        }
                    ]
                },
                "stop": {
                    "description": "Not supported with latest reasoning models `o3` and `o4-mini`.\n\nUp to 4 sequences where the API will stop generating further tokens. The\nreturned text will not contain the stop sequence.",
                    "allOf": [
                        {
                            "$ref": "#/definitions/openai.CompletionNewParamsStopUnion"
                        }
                    ]
                },
                "stream_options": {
                    "description": "Options for streaming response. Only set this when you set `stream: true`.",
                    "allOf": [
                        {
                            "$ref": "#/definitions/openai.ChatCompletionStreamOptionsParam"
                        }
                    ]
                },
                "suffix": {
                    "description": "The suffix that comes after a completion of inserted text.\n\nThis parameter is only supported for `gpt-3.5-turbo-instruct`.",
                    "allOf": [
                        {
                            "$ref": "#/definitions/param.Opt-string"
                        }
                    ]
                },
                "temperature": {
                    "description": "What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\nmake the output more random, while lower values like 0.2 will make it more\nfocused and deterministic.\n\nWe generally recommend altering this or `top_p` but not both.",
                    "allOf": [
                        {
                            "$ref": "#/definitions/param.Opt-float64"
                        }
                    ]
                },
                "top_p": {
                    "description": "An alternative to sampling with temperature, called nucleus sampling, where the\nmodel considers the results of the tokens with top_p probability mass. So 0.1\nmeans only the tokens comprising the top 10% probability mass are considered.\n\nWe generally recommend altering this or `temperature` but not both.",
                    "allOf": [
                        {
                            "$ref": "#/definitions/param.Opt-float64"
                        }
                    ]
                },
                "user": {
                    "description": "A unique identifier representing your end-user, which can help OpenAI to monitor\nand detect abuse.\n[Learn more](https://platform.openai.com/docs/guides/safety-best-practices#end-user-ids).",
                    "allOf": [
                        {
                            "$ref": "#/definitions/param.Opt-string"
                        }
                    ]
                }
            }
        },
        "openai.CompletionNewParamsModel": {
            "type": "string",
            "enum": [
                "gpt-3.5-turbo-instruct",
                "davinci-002",
                "babbage-002"
            ],
            "x-enum-varnames": [
                "CompletionNewParamsModelGPT3_5TurboInstruct",
                "CompletionNewParamsModelDavinci002",
                "CompletionNewParamsModelBabbage002"
            ]
        },
        "openai.CompletionNewParamsPromptUnion": {
            "type": "object",
            "properties": {
                "any": {},
                "ofArrayOfStrings": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "ofArrayOfTokenArrays": {
                    "type": "array",
                    "items": {
                        "type": "array",
                        "items": {
                            "type": "integer"
                        }
                    }
                },
                "ofArrayOfTokens": {
                    "type": "array",
                    "items": {
                        "type": "integer"
                    }
                },
                "ofString": {
                    "$ref": "#/definitions/param.Opt-string"
                }
            }
        },
        "openai.CompletionNewParamsStopUnion": {
            "type": "object",
            "properties": {
                "any": {},
                "ofString": {
                    "$ref": "#/definitions/param.Opt-string"
                },
                "ofStringArray": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            }
        },
        "openai.CompletionUsage": {
            "type": "object",
            "properties": {
                "completion_tokens": {
                    "description": "Number of tokens in the generated completion.",
                    "type": "integer"
                },
                "completion_tokens_details": {
                    "description": "Breakdown of tokens used in a completion.",
                    "allOf": [
                        {
                            "$ref": "#/definitions/openai.CompletionUsageCompletionTokensDetails"
                        }
                    ]
                },
                "prompt_tokens": {
                    "description": "Number of tokens in the prompt.",
                    "type": "integer"
                },
                "prompt_tokens_details": {
                    "description": "Breakdown of tokens used in the prompt.",
                    "allOf": [
                        {
                            "$ref": "#/definitions/openai.CompletionUsagePromptTokensDetails"
                        }
                    ]
                },
                "total_tokens": {
                    "description": "Total number of tokens used in the request (prompt + completion).",
                    "type": "integer"
                }
            }
        },
        "openai.CompletionUsageCompletionTokensDetails": {
            "type": "object",
            "properties": {
                "accepted_prediction_tokens": {
                    "description": "When using Predicted Outputs, the number of tokens in the prediction that\nappeared in the completion.",
                    "type": "integer"
                },
                "audio_tokens": {
                    "description": "Audio input tokens generated by the model.",
                    "type": "integer"
                },
                "reasoning_tokens": {
                    "description": "Tokens generated by the model for reasoning.",
                    "type": "integer"
                },
                "rejected_prediction_tokens": {
                    "description": "When using Predicted Outputs, the number of tokens in the prediction that did\nnot appear in the completion. However, like reasoning tokens, these tokens are\nstill counted in the total completion tokens for purposes of billing, output,\nand context window limits.",
                    "type": "integer"
                }
            }
        },
        "openai.CompletionUsagePromptTokensDetails": {
            "type": "object",
            "properties": {
                "audio_tokens": {
                    "description": "Audio input tokens present in the prompt.",
                    "type": "integer"
                },
                "cached_tokens": {
                    "description": "Cached tokens present in the prompt.",
                    "type": "integer"
                }
            }
        },
        "openai.EmbeddingModel": {
            "type": "string",
            "enum": [
                "text-embedding-ada-002",
                "text-embedding-3-small",
                "text-embedding-3-large"
            ],
            "x-enum-varnames": [
                "EmbeddingModelTextEmbeddingAda002",
                "EmbeddingModelTextEmbedding3Small",
                "EmbeddingModelTextEmbedding3Large"
            ]
        },
        "openai.EmbeddingNewParams": {
            "type": "object",
            "properties": {
                "any": {},
                "dimensions": {
                    "description": "The number of dimensions the resulting output embeddings should have. Only\nsupported in `text-embedding-3` and later models.",
                    "allOf": [
                        {
                            "$ref": "#/definitions/param.Opt-int64"
                        }
                    ]
                },
                "encoding_format": {
                    "description": "The format to return the embeddings in. Can be either `float` or\n[`base64`](https://pypi.org/project/pybase64/).\n\nAny of \"float\", \"base64\".",
                    "allOf": [
                        {
                            "$ref": "#/definitions/openai.EmbeddingNewParamsEncodingFormat"
                        }
                    ]
                },
                "input": {
                    "description": "Input text to embed, encoded as a string or array of tokens. To embed multiple\ninputs in a single request, pass an array of strings or array of token arrays.\nThe input must not exceed the max input tokens for the model (8192 tokens for\nall embedding models), cannot be an empty string, and any array must be 2048\ndimensions or less.\n[Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken)\nfor counting tokens. In addition to the per-input token limit, all embedding\nmodels enforce a maximum of 300,000 tokens summed across all inputs in a single\nrequest.",
                    "allOf": [
                        {
                            "$ref": "#/definitions/openai.EmbeddingNewParamsInputUnion"
                        }
                    ]
                },
                "model": {
                    "description": "ID of the model to use. You can use the\n[List models](https://platform.openai.com/docs/api-reference/models/list) API to\nsee all of your available models, or see our\n[Model overview](https://platform.openai.com/docs/models) for descriptions of\nthem.",
                    "allOf": [
                        {
                            "$ref": "#/definitions/openai.EmbeddingModel"
                        }
                    ]
                },
                "user": {
                    "description": "A unique identifier representing your end-user, which can help OpenAI to monitor\nand detect abuse.\n[Learn more](https://platform.openai.com/docs/guides/safety-best-practices#end-user-ids).",
                    "allOf": [
                        {
                            "$ref": "#/definitions/param.Opt-string"
                        }
                    ]
                }
            }
        },
        "openai.EmbeddingNewParamsEncodingFormat": {
            "type": "string",
            "enum": [
                "float",
                "base64"
            ],
            "x-enum-varnames": [
                "EmbeddingNewParamsEncodingFormatFloat",
                "EmbeddingNewParamsEncodingFormatBase64"
            ]
        },
        "openai.EmbeddingNewParamsInputUnion": {
            "type": "object",
            "properties": {
                "any": {},
                "ofArrayOfStrings": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "ofArrayOfTokenArrays": {
                    "type": "array",
                    "items": {
                        "type": "array",
                        "items": {
                            "type": "integer"
                        }
                    }
                },
                "ofArrayOfTokens": {
                    "type": "array",
                    "items": {
                        "type": "integer"
                    }
                },
                "ofString": {
                    "$ref": "#/definitions/param.Opt-string"
                }
            }
        },
        "param.Opt-bool": {
            "type": "object",
            "properties": {
                "value": {
                    "type": "boolean"
                }
            }
        },
        "param.Opt-float64": {
            "type": "object",
            "properties": {
                "value": {
                    "type": "number"
                }
            }
        },
        "param.Opt-int64": {
            "type": "object",
            "properties": {
                "value": {
                    "type": "integer"
                }
            }
        },
        "param.Opt-string": {
            "type": "object",
            "properties": {
                "value": {
                    "type": "string"
                }
            }
        }
    }
}