openapi: 3.0.3
info:
  title: Nexa AI Server
  version: 0.0.0
  description: |
    Nexa AI Server - OpenAI compatible API endpoints

paths:
  /v1/chat/completions:
    post:
      summary: Creates a model response for the given chat conversation
      description: This endpoint generates a model response for a given conversation, which can include text and images. It supports both single-turn and multi-turn conversations and can be used for various tasks like question answering, code generation, and function calling.
      operationId: PostV1ChatCompletions
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/ChatCompletionRequest"
      responses:
        "200":
          description: Successful response for non-streaming requests
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ChatCompletionResponse"

  /v1/embeddings:
    post:
      summary: Creates an embedding for the given input
      description: Creates an embedding for the given input
      operationId: PostV1Embeddings
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/EmbeddingRequest"
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/EmbeddingResponse"

  /v1/images/generations:
    post:
      summary: Creates an image given a prompt
      description: Creates an image given a prompt. This endpoint follows OpenAI DALL-E 3 API specification for compatibility.
      operationId: PostV1ImagesGenerations
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/ImageGenerationRequest"
      responses:
        "200":
          description: Successful image generation response
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ImageGenerationResponse"
        "400":
          description: Bad request - invalid parameters
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ErrorResponse"
        "404":
          description: Model not found
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ErrorResponse"
        "500":
          description: Internal server error
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ErrorResponse"

  /v1/reranking:
    post:
      summary: Rerank documents based on their relevance to a query
      description: Reranks a list of documents according to their semantic relevance to the provided query. Returns relevance scores for each document. Higher scores indicate greater relevance.
      operationId: PostV1Reranking
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/RerankingRequest"
            example:
              model: "NexaAI/jina-v2-rerank-npu"
              query: "What is machine learning?"
              documents:
                - "Machine learning is a subset of artificial intelligence."
                - "Machine learning algorithms learn patterns from data."
                - "The weather is sunny today."
                - "Deep learning is a type of machine learning."
              batch_size: 4
              normalize: true
              normalize_method: "softmax"
      responses:
        "200":
          description: Successfully reranked documents with relevance scores
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/RerankingResponse"
              example:
                result: [0.95, 0.92, 0.10, 0.88]
        "400":
          description: Bad request - invalid parameters
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ErrorResponse"
        "500":
          description: Internal server error
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ErrorResponse"

components:
  schemas:
    # ---------- Completions ----------
    CompletionRequest:
      type: object
      required: [model, prompt]
      properties:
        model:
          type: string
          description: ID of the model to use
          default: "Qwen/Qwen3-1.7B-GGUF"
        prompt:
          oneOf:
            - type: string
            - type: array
              items: { type: string }
            - type: array
              items: { type: integer }
            - type: array
              items:
                type: array
                items: { type: integer }
          description: The prompt(s) to generate completions for
          default: "print hello world"
        max_tokens:
          type: integer
          minimum: 1
          description: The maximum number of tokens that can be generated in the completion
          default: 4096
        temperature:
          type: number
          format: float
          minimum: 0
          maximum: 2
          default: 0.8
          description: What sampling temperature to use, between 0 and 2
        top_p:
          type: number
          format: float
          minimum: 0
          maximum: 1
          default: 0.95
          description: An alternative to sampling with temperature, called nucleus sampling
        top_k:
          type: number
          format: float
          minimum: 0
          maximum: 1
          default: 0.0
          description: An alternative to sampling with temperature, called top-k sampling
        min_p:
          type: number
          format: float
          minimum: 0
          maximum: 1
          default: 0.0
          description: An alternative to sampling with temperature, called nucleus sampling
        n:
          type: integer
          minimum: 1
          description: How many completions to generate for each prompt
          default: 2048
        stream:
          type: boolean
          description: Whether to stream back partial progress
          default: false
        logprobs:
          type: integer
          minimum: 0
          maximum: 5
          description: Include the log probabilities on the logprobs most likely tokens
        echo:
          type: boolean
          description: Echo back the prompt in addition to the completion
        stop:
          oneOf:
            - type: string
            - type: array
              items: { type: string }
          description: Up to 4 sequences where the API will stop generating further tokens
        repetition_penalty:
          type: number
          format: float
          minimum: 0
          maximum: 100
          default: 1.0
          description: The parameter for repetition penalty. 1.0 means no penalty
        presence_penalty:
          type: number
          format: float
          minimum: -2
          maximum: 2
          default: 0.0
          description: Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far
        frequency_penalty:
          type: number
          format: float
          minimum: -2
          maximum: 2
          default: 0.0
          description: Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far
        best_of:
          type: integer
          minimum: 1
          description: Generates best_of completions server-side and returns the "best"
        logit_bias:
          type: object
          additionalProperties:
            type: integer
          description: Modify the likelihood of specified tokens appearing in the completion
        user:
          type: string
          description: A unique identifier representing your end-user
          default: "role"

    CompletionResponse:
      type: object
      required: [choices]
      properties:
        id:
          type: string
          description: A unique identifier for the completion
        object:
          type: string
          description: The object type, which is always "text_completion"
        created:
          type: integer
          description: The Unix timestamp (in seconds) of when the completion was created
        model:
          type: string
          description: The model used for completion
        choices:
          type: array
          items:
            $ref: "#/components/schemas/CompletionChoice"
          description: The list of completion choices the model generated for the input prompt
        usage:
          $ref: "#/components/schemas/TokenUsage"

    CompletionChoice:
      type: object
      properties:
        text:
          type: string
          description: The generated text
        index:
          type: integer
          description: The index of the choice in the list of choices
        logprobs:
          $ref: "#/components/schemas/CompletionChoiceLogprobs"
        finish_reason:
          type: string
          enum: [stop, length, content_filter]
          description: The reason the model stopped generating tokens

    CompletionChoiceLogprobs:
      type: object
      properties:
        text_offset:
          type: array
          items:
            type: integer
        token_logprobs:
          type: array
          items:
            type: number
        tokens:
          type: array
          items:
            type: string
        top_logprobs:
          type: array
          items:
            type: object
            additionalProperties:
              type: number

    # ---------- Chat ----------
    ChatCompletionRequest:
      type: object
      required: [model, messages]
      properties:
        model:
          type: string
          description: ID of the model to use
          default: "Qwen/Qwen3-1.7B-GGUF"
        messages:
          type: array
          items:
            $ref: "#/components/schemas/ChatMessage"
          description: A list of messages comprising the conversation so far
        max_tokens:
          type: integer
          minimum: 1
          description: The maximum number of tokens that can be generated in the chat completion
          default: 4096
        temperature:
          type: number
          format: float
          minimum: 0
          maximum: 2
          default: 0.8
          description: What sampling temperature to use, between 0 and 2
        top_p:
          type: number
          format: float
          minimum: 0
          maximum: 1
          default: 0.95
          description: An alternative to sampling with temperature, called nucleus sampling
        n:
          type: integer
          minimum: 1
          description: How many chat completion choices to generate for each input message
          default: 2048
        stream:
          type: boolean
          description: If set, partial message deltas will be sent
          default: false
        stop:
          oneOf:
            - type: string
            - type: array
              items: { type: string }
          description: Up to 4 sequences where the API will stop generating further tokens
          default: ""
        presence_penalty:
          type: number
          format: float
          minimum: -2
          maximum: 2
          default: 0.0
          description: Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far
        frequency_penalty:
          type: number
          format: float
          minimum: -2
          maximum: 2
          default: 0.0
          description: Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far
        logit_bias:
          type: object
          additionalProperties:
            type: integer
          description: Modify the likelihood of specified tokens appearing in the completion
        user:
          type: string
          description: A unique identifier representing your end-user
          default: "role"
        tools:
          type: array
          items:
            $ref: "#/components/schemas/ChatCompletionTool"
          description: A list of tools the model may call
          default: []
        tool_choice:
          oneOf:
            - type: string
            - $ref: "#/components/schemas/ChatCompletionToolChoice"
          description: Controls which (if any) function is called by the model
          default: ""
        enable_json:
          type: boolean
          description: Whether to enable JSON response generation
          default: false
        enable_think:
          type: boolean
          description: Whether to enable thinking mode for the model
          default: true
        top_k:
          type: integer
          minimum: 0
          description: An alternative to sampling with temperature, called top-k sampling
          default: 0
        min_p:
          type: number
          format: float
          minimum: 0
          maximum: 1
          description: An alternative to sampling with temperature, called nucleus sampling
          default: 0.0
        repetition_penalty:
          type: number
          format: float
          minimum: 0
          maximum: 100
          description: The parameter for repetition penalty. 1.0 means no penalty
          default: 1.0
        grammar_path:
          type: string
          description: Path to the grammar file for structured output
          default: ""
        grammar_string:
          type: string
          description: Grammar string for structured output
          default: ""

    ChatMessage:
      type: object
      required: [role, content]
      default:
        {
          "role": "user",
          "content": "What is the weather like in Boston today?",
        }
      properties:
        role:
          type: string
          enum: [system, user, assistant, tool]
          description: The role of the author of this message
        content:
          oneOf:
            - type: string
            - type: array
              items:
                $ref: "#/components/schemas/ChatMessageContent"
          description: The contents of the message
        name:
          type: string
          description: The name of the author of this message
        tool_calls:
          type: array
          items:
            $ref: "#/components/schemas/ChatCompletionMessageToolCall"
          description: The tool calls generated by the model
        function_call:
          $ref: "#/components/schemas/ChatCompletionMessageFunctionCall"
          description: Deprecated and replaced by tool_calls

    ChatMessageContent:
      type: object
      properties:
        type:
          type: string
          enum: [text, image_url]
        text:
          type: string
        image_url:
          $ref: "#/components/schemas/ChatMessageImageURL"

    ChatMessageImageURL:
      type: object
      properties:
        url:
          type: string
        detail:
          type: string
          enum: [low, high, auto]

    ChatCompletionTool:
      type: object
      properties:
        type:
          type: string
          enum: [function]
        function:
          $ref: "#/components/schemas/ChatCompletionToolFunction"

    ChatCompletionToolFunction:
      type: object
      required: [name]
      properties:
        name:
          type: string
          description: The name of the function to be called
        description:
          type: string
          description: A description of what the function does
        parameters:
          type: object
          description: The parameters the functions accepts

    ChatCompletionToolChoice:
      type: object
      properties:
        type:
          type: string
          enum: [function]
        function:
          type: object
          properties:
            name:
              type: string

    ChatCompletionMessageToolCall:
      type: object
      properties:
        id:
          type: string
          description: The ID of the tool call
        type:
          type: string
          enum: [function]
          description: The type of the tool
        function:
          $ref: "#/components/schemas/ChatCompletionMessageToolCallFunction"

    ChatCompletionMessageToolCallFunction:
      type: object
      properties:
        name:
          type: string
          description: The name of the function to call
        arguments:
          type: string
          description: The arguments to call the function with, as generated by the model in JSON format

    ChatCompletionMessageFunctionCall:
      type: object
      properties:
        name:
          type: string
          description: The name of the function to call
        arguments:
          type: string
          description: The arguments to call the function with, as generated by the model in JSON format

    ChatCompletionResponse:
      type: object
      required: [choices]
      properties:
        id:
          type: string
          description: A unique identifier for the chat completion
        object:
          type: string
          description: The object type, which is always "chat.completion"
        created:
          type: integer
          description: The Unix timestamp (in seconds) of when the chat completion was created
        model:
          type: string
          description: The model used for the chat completion
        choices:
          type: array
          items:
            $ref: "#/components/schemas/ChatChoice"
          description: A list of chat completion choices
        usage:
          $ref: "#/components/schemas/TokenUsage"

    ChatChoice:
      type: object
      properties:
        index:
          type: integer
          description: The index of the choice in the list of choices
        message:
          $ref: "#/components/schemas/ChatMessage"
          description: A chat completion message generated by the model
        finish_reason:
          type: string
          enum: [stop, length, tool_calls, content_filter, function_call]
          description: The reason the model stopped generating tokens
        logprobs:
          $ref: "#/components/schemas/ChatCompletionChoiceLogprobs"

    ChatCompletionChoiceLogprobs:
      type: object
      properties:
        content:
          type: array
          items:
            $ref: "#/components/schemas/ChatCompletionTokenLogprob"

    ChatCompletionTokenLogprob:
      type: object
      properties:
        token:
          type: string
        logprob:
          type: number
        bytes:
          type: array
          items:
            type: integer
        top_logprobs:
          type: array
          items:
            $ref: "#/components/schemas/ChatCompletionTokenLogprobTopLogprob"

    ChatCompletionTokenLogprobTopLogprob:
      type: object
      properties:
        token:
          type: string
        logprob:
          type: number
        bytes:
          type: array
          items:
            type: integer

    # ---------- Embeddings ----------
    EmbeddingRequest:
      type: object
      required: [model, input]
      properties:
        model:
          type: string
          description: ID of the model to use
          default: "djuna/jina-embeddings-v2-small-en-Q5_K_M-GGUF"
        input:
          oneOf:
            - type: string
            - type: array
              items: { type: string }
            - type: array
              items: { type: integer }
            - type: array
              items:
                type: array
                items: { type: integer }
          description: Input text to embed
          default: "Hello, world!"
        encoding_format:
          type: string
          enum: [float, base64]
          description: The format to return the embeddings in
        dimensions:
          type: integer
          description: The number of dimensions the resulting output embeddings should have
        user:
          type: string
          description: A unique identifier representing your end-user

    EmbeddingResponse:
      type: object
      required: [data]
      properties:
        object:
          type: string
          description: The object type, which is always "list"
        data:
          type: array
          items:
            $ref: "#/components/schemas/EmbeddingVector"
          description: The list of embeddings generated by the model
        model:
          type: string
          description: The name of the model used to generate the embedding
        usage:
          $ref: "#/components/schemas/TokenUsage"

    EmbeddingVector:
      type: object
      required: [embedding]
      properties:
        object:
          type: string
          description: The object type, which is always "embedding"
        embedding:
          type: array
          items: { type: number, format: float }
          description: The embedding vector
        index:
          type: integer
          description: The index of the embedding in the list of embeddings

    # ---------- Image Generation ----------
    ImageGenerationRequest:
      type: object
      required: [model, prompt]
      properties:
        model:
          type: string
          description: ID of the model to use
          default: "nexaml/sdxl-turbo-ryzen-ai"
        prompt:
          type: string
          description: A text description of the desired image(s). The maximum length is 1000 characters.
          default: "A white cat with red eyes"
        n:
          type: integer
          minimum: 1
          maximum: 10
          description: The number of images to generate. Must be between 1 and 10.
          default: 1
        size:
          type: string
          enum: ["512x512", "1024x1024", "1792x1024", "1024x1792"]
          description: The size of the generated images. Must be one of the supported sizes.
          default: "512x512"
        quality:
          type: string
          enum: ["standard", "hd"]
          description: The quality of the image that will be generated
          default: "standard"
        style:
          type: string
          enum: ["vivid", "natural"]
          description: The style of the generated images
          default: "vivid"
        response_format:
          type: string
          enum: ["url", "b64_json"]
          description: The format in which the generated images are returned
          default: "url"
        user:
          type: string
          description: A unique identifier representing your end-user

    ImageGenerationResponse:
      type: object
      required: [created, data]
      properties:
        created:
          type: integer
          description: The Unix timestamp (in seconds) of when the image was created
        data:
          type: array
          items:
            $ref: "#/components/schemas/ImageGenerationData"
          description: The list of generated images

    ImageGenerationData:
      type: object
      properties:
        url:
          type: string
          description: The URL of the generated image, if response_format is "url"
        b64_json:
          type: string
          description: The base64-encoded JSON of the generated image, if response_format is "b64_json"
        revised_prompt:
          type: string
          description: The prompt that was used to generate the image, if there was a revision to the prompt

    # ---------- Reranking ----------
    RerankingRequest:
      type: object
      required: [model, query, documents, normalize, normalize_method]
      properties:
        model:
          type: string
          description: ID of the reranking model to use
          default: "NexaAI/jina-v2-rerank-npu"
          example: "NexaAI/jina-v2-rerank-npu"
        query:
          type: string
          description: The search query to compare documents against
          default: "What is machine learning?"
          example: "What is machine learning?"
        documents:
          type: array
          items: { type: string }
          description: A list of documents to rerank based on relevance to the query
          example:
            - "Machine learning is a subset of artificial intelligence."
            - "Machine learning algorithms learn patterns from data."
            - "The weather is sunny today."
            - "Deep learning is a type of machine learning."
        batch_size:
          type: integer
          description: The batch size to use for reranking
          default: 4
          example: 4
        normalize:
          type: boolean
          description: Whether to normalize the relevance scores
          default: true
          example: true
        normalize_method:
          type: string
          description: The normalization method to apply to scores
          enum: [softmax, linear]
          default: "softmax"
          example: "softmax"

    RerankingResponse:
      type: object
      required: [result]
      properties:
        result:
          type: array
          items:
            type: number
            format: float
          description: Array of relevance scores corresponding to each document in the input. Higher scores indicate greater relevance to the query.
          example: [0.95, 0.92, 0.10, 0.88]

    # ---------- Common ----------
    ErrorResponse:
      type: object
      required: [error]
      properties:
        error:
          type: string
          description: Error message describing what went wrong

    TokenUsage:
      type: object
      properties:
        prompt_tokens:
          type: integer
          description: Number of tokens in the prompt
        completion_tokens:
          type: integer
          description: Number of tokens in the generated completion
        total_tokens:
          type: integer
          description: Total number of tokens used in the request
