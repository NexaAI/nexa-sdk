openapi: 3.0.3
info:
  title: Nexa AI Server
  version: 0.0.0
  description: |
    Nexa AI Server - OpenAI compatible API endpoints

paths:
  /v1/chat/completions:
    post:
      summary: Creates a model response for the given chat conversation
      description: This endpoint generates a model response for a given conversation, which can include text and images. It supports both single-turn and multi-turn conversations and can be used for various tasks like question answering, code generation, and function calling.
      operationId: PostV1ChatCompletions
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/ChatCompletionRequest"
      responses:
        "200":
          description: Successful response for non-streaming requests
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ChatCompletionResponse"

  /v1/embeddings:
    post:
      summary: Creates an embedding for the given input
      description: Creates an embedding for the given input
      operationId: PostV1Embeddings
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/EmbeddingRequest"
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/EmbeddingResponse"

  /v1/images/generations:
    post:
      summary: Creates an image given a prompt
      description: Creates an image given a prompt. This endpoint follows OpenAI DALL-E 3 API specification for compatibility.
      operationId: PostV1ImagesGenerations
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/ImageGenerationRequest"
      responses:
        "200":
          description: Successful image generation response
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ImageGenerationResponse"
        "400":
          description: Bad request - invalid parameters
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ErrorResponse"
        "404":
          description: Model not found
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ErrorResponse"
        "500":
          description: Internal server error
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ErrorResponse"

  # /v1/reranking:
  #   post:
  #     summary: Reranks the given documents for the given query
  #     description: Reranks the given documents for the given query
  #     operationId: PostV1Reranking
  #     requestBody:
  #       required: true
  #       content:
  #         application/json:
  #           schema:
  #             $ref: "#/components/schemas/RerankingRequest"
  #     responses:
  #       "200":
  #         description: OK
  #         content:
  #           application/json:
  #             schema:
  #               $ref: "#/components/schemas/RerankingResponse"

components:
  schemas:
    # ---------- Completions ----------
    CompletionRequest:
      type: object
      required: [model, prompt]
      properties:
        model:
          type: string
          description: ID of the model to use
          default: "Qwen/Qwen3-1.7B-GGUF"
        prompt:
          oneOf:
            - type: string
            - type: array
              items: { type: string }
            - type: array
              items: { type: integer }
            - type: array
              items:
                type: array
                items: { type: integer }
          description: The prompt(s) to generate completions for
          default: "print hello world"
        max_tokens:
          type: integer
          minimum: 1
          description: The maximum number of tokens that can be generated in the completion
          default: 4096
        temperature:
          type: number
          format: float
          minimum: 0
          maximum: 2
          default: 0.8
          description: What sampling temperature to use, between 0 and 2
        top_p:
          type: number
          format: float
          minimum: 0
          maximum: 1
          default: 0.95
          description: An alternative to sampling with temperature, called nucleus sampling
        n:
          type: integer
          minimum: 1
          description: How many completions to generate for each prompt
          default: 2048
        stream:
          type: boolean
          description: Whether to stream back partial progress
          default: false
        logprobs:
          type: integer
          minimum: 0
          maximum: 5
          description: Include the log probabilities on the logprobs most likely tokens
        echo:
          type: boolean
          description: Echo back the prompt in addition to the completion
        stop:
          oneOf:
            - type: string
            - type: array
              items: { type: string }
          description: Up to 4 sequences where the API will stop generating further tokens
        presence_penalty:
          type: number
          format: float
          minimum: -2
          maximum: 2
          default: 0.0
          description: Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far
        frequency_penalty:
          type: number
          format: float
          minimum: -2
          maximum: 2
          default: 0.0
          description: Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far
        best_of:
          type: integer
          minimum: 1
          description: Generates best_of completions server-side and returns the "best"
        logit_bias:
          type: object
          additionalProperties:
            type: integer
          description: Modify the likelihood of specified tokens appearing in the completion
        user:
          type: string
          description: A unique identifier representing your end-user
          default: "role"

    CompletionResponse:
      type: object
      required: [choices]
      properties:
        id:
          type: string
          description: A unique identifier for the completion
        object:
          type: string
          description: The object type, which is always "text_completion"
        created:
          type: integer
          description: The Unix timestamp (in seconds) of when the completion was created
        model:
          type: string
          description: The model used for completion
        choices:
          type: array
          items:
            $ref: "#/components/schemas/CompletionChoice"
          description: The list of completion choices the model generated for the input prompt
        usage:
          $ref: "#/components/schemas/TokenUsage"

    CompletionChoice:
      type: object
      properties:
        text:
          type: string
          description: The generated text
        index:
          type: integer
          description: The index of the choice in the list of choices
        logprobs:
          $ref: "#/components/schemas/CompletionChoiceLogprobs"
        finish_reason:
          type: string
          enum: [stop, length, content_filter]
          description: The reason the model stopped generating tokens

    CompletionChoiceLogprobs:
      type: object
      properties:
        text_offset:
          type: array
          items:
            type: integer
        token_logprobs:
          type: array
          items:
            type: number
        tokens:
          type: array
          items:
            type: string
        top_logprobs:
          type: array
          items:
            type: object
            additionalProperties:
              type: number

    # ---------- Chat ----------
    ChatCompletionRequest:
      type: object
      required: [model, messages]
      properties:
        model:
          type: string
          description: ID of the model to use
          default: "Qwen/Qwen3-1.7B-GGUF"
        messages:
          type: array
          items:
            $ref: "#/components/schemas/ChatMessage"
          description: A list of messages comprising the conversation so far
        max_tokens:
          type: integer
          minimum: 1
          description: The maximum number of tokens that can be generated in the chat completion
          default: 4096
        temperature:
          type: number
          format: float
          minimum: 0
          maximum: 2
          default: 0.8
          description: What sampling temperature to use, between 0 and 2
        top_p:
          type: number
          format: float
          minimum: 0
          maximum: 1
          default: 0.95
          description: An alternative to sampling with temperature, called nucleus sampling
        n:
          type: integer
          minimum: 1
          description: How many chat completion choices to generate for each input message
          default: 2048
        stream:
          type: boolean
          description: If set, partial message deltas will be sent
          default: false
        stop:
          oneOf:
            - type: string
            - type: array
              items: { type: string }
          description: Up to 4 sequences where the API will stop generating further tokens
          default: ""
        presence_penalty:
          type: number
          format: float
          minimum: -2
          maximum: 2
          default: 0.0
          description: Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far
        frequency_penalty:
          type: number
          format: float
          minimum: -2
          maximum: 2
          default: 0.0
          description: Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far
        logit_bias:
          type: object
          additionalProperties:
            type: integer
          description: Modify the likelihood of specified tokens appearing in the completion
        user:
          type: string
          description: A unique identifier representing your end-user
          default: "role"
        tools:
          type: array
          items:
            $ref: "#/components/schemas/ChatCompletionTool"
          description: A list of tools the model may call
          default: []
        tool_choice:
          oneOf:
            - type: string
            - $ref: "#/components/schemas/ChatCompletionToolChoice"
          description: Controls which (if any) function is called by the model
          default: ""

    ChatMessage:
      type: object
      required: [role, content]
      default:
        {
          "role": "user",
          "content": "What is the weather like in Boston today?",
        }
      properties:
        role:
          type: string
          enum: [system, user, assistant, tool]
          description: The role of the author of this message
        content:
          oneOf:
            - type: string
            - type: array
              items:
                $ref: "#/components/schemas/ChatMessageContent"
          description: The contents of the message
        name:
          type: string
          description: The name of the author of this message
        tool_calls:
          type: array
          items:
            $ref: "#/components/schemas/ChatCompletionMessageToolCall"
          description: The tool calls generated by the model
        function_call:
          $ref: "#/components/schemas/ChatCompletionMessageFunctionCall"
          description: Deprecated and replaced by tool_calls

    ChatMessageContent:
      type: object
      properties:
        type:
          type: string
          enum: [text, image_url]
        text:
          type: string
        image_url:
          $ref: "#/components/schemas/ChatMessageImageURL"

    ChatMessageImageURL:
      type: object
      properties:
        url:
          type: string
        detail:
          type: string
          enum: [low, high, auto]

    ChatCompletionTool:
      type: object
      properties:
        type:
          type: string
          enum: [function]
        function:
          $ref: "#/components/schemas/ChatCompletionToolFunction"

    ChatCompletionToolFunction:
      type: object
      required: [name]
      properties:
        name:
          type: string
          description: The name of the function to be called
        description:
          type: string
          description: A description of what the function does
        parameters:
          type: object
          description: The parameters the functions accepts

    ChatCompletionToolChoice:
      type: object
      properties:
        type:
          type: string
          enum: [function]
        function:
          type: object
          properties:
            name:
              type: string

    ChatCompletionMessageToolCall:
      type: object
      properties:
        id:
          type: string
          description: The ID of the tool call
        type:
          type: string
          enum: [function]
          description: The type of the tool
        function:
          $ref: "#/components/schemas/ChatCompletionMessageToolCallFunction"

    ChatCompletionMessageToolCallFunction:
      type: object
      properties:
        name:
          type: string
          description: The name of the function to call
        arguments:
          type: string
          description: The arguments to call the function with, as generated by the model in JSON format

    ChatCompletionMessageFunctionCall:
      type: object
      properties:
        name:
          type: string
          description: The name of the function to call
        arguments:
          type: string
          description: The arguments to call the function with, as generated by the model in JSON format

    ChatCompletionResponse:
      type: object
      required: [choices]
      properties:
        id:
          type: string
          description: A unique identifier for the chat completion
        object:
          type: string
          description: The object type, which is always "chat.completion"
        created:
          type: integer
          description: The Unix timestamp (in seconds) of when the chat completion was created
        model:
          type: string
          description: The model used for the chat completion
        choices:
          type: array
          items:
            $ref: "#/components/schemas/ChatChoice"
          description: A list of chat completion choices
        usage:
          $ref: "#/components/schemas/TokenUsage"

    ChatChoice:
      type: object
      properties:
        index:
          type: integer
          description: The index of the choice in the list of choices
        message:
          $ref: "#/components/schemas/ChatMessage"
          description: A chat completion message generated by the model
        finish_reason:
          type: string
          enum: [stop, length, tool_calls, content_filter, function_call]
          description: The reason the model stopped generating tokens
        logprobs:
          $ref: "#/components/schemas/ChatCompletionChoiceLogprobs"

    ChatCompletionChoiceLogprobs:
      type: object
      properties:
        content:
          type: array
          items:
            $ref: "#/components/schemas/ChatCompletionTokenLogprob"

    ChatCompletionTokenLogprob:
      type: object
      properties:
        token:
          type: string
        logprob:
          type: number
        bytes:
          type: array
          items:
            type: integer
        top_logprobs:
          type: array
          items:
            $ref: "#/components/schemas/ChatCompletionTokenLogprobTopLogprob"

    ChatCompletionTokenLogprobTopLogprob:
      type: object
      properties:
        token:
          type: string
        logprob:
          type: number
        bytes:
          type: array
          items:
            type: integer

    # ---------- Embeddings ----------
    EmbeddingRequest:
      type: object
      required: [model, input]
      properties:
        model:
          type: string
          description: ID of the model to use
          default: "djuna/jina-embeddings-v2-small-en-Q5_K_M-GGUF"
        input:
          oneOf:
            - type: string
            - type: array
              items: { type: string }
            - type: array
              items: { type: integer }
            - type: array
              items:
                type: array
                items: { type: integer }
          description: Input text to embed
          default: "Hello, world!"
        encoding_format:
          type: string
          enum: [float, base64]
          description: The format to return the embeddings in
        dimensions:
          type: integer
          description: The number of dimensions the resulting output embeddings should have
        user:
          type: string
          description: A unique identifier representing your end-user

    EmbeddingResponse:
      type: object
      required: [data]
      properties:
        object:
          type: string
          description: The object type, which is always "list"
        data:
          type: array
          items:
            $ref: "#/components/schemas/EmbeddingVector"
          description: The list of embeddings generated by the model
        model:
          type: string
          description: The name of the model used to generate the embedding
        usage:
          $ref: "#/components/schemas/TokenUsage"

    EmbeddingVector:
      type: object
      required: [embedding]
      properties:
        object:
          type: string
          description: The object type, which is always "embedding"
        embedding:
          type: array
          items: { type: number, format: float }
          description: The embedding vector
        index:
          type: integer
          description: The index of the embedding in the list of embeddings

    # ---------- Image Generation ----------
    ImageGenerationRequest:
      type: object
      required: [model, prompt]
      properties:
        model:
          type: string
          description: ID of the model to use
          default: "nexaml/sdxl-turbo-ryzen-ai"
        prompt:
          type: string
          description: A text description of the desired image(s). The maximum length is 1000 characters.
          default: "A white cat with red eyes"
        n:
          type: integer
          minimum: 1
          maximum: 10
          description: The number of images to generate. Must be between 1 and 10.
          default: 1
        size:
          type: string
          enum: ["512x512", "1024x1024", "1792x1024", "1024x1792"]
          description: The size of the generated images. Must be one of the supported sizes.
          default: "512x512"
        quality:
          type: string
          enum: ["standard", "hd"]
          description: The quality of the image that will be generated
          default: "standard"
        style:
          type: string
          enum: ["vivid", "natural"]
          description: The style of the generated images
          default: "vivid"
        response_format:
          type: string
          enum: ["url", "b64_json"]
          description: The format in which the generated images are returned
          default: "url"
        user:
          type: string
          description: A unique identifier representing your end-user

    ImageGenerationResponse:
      type: object
      required: [created, data]
      properties:
        created:
          type: integer
          description: The Unix timestamp (in seconds) of when the image was created
        data:
          type: array
          items:
            $ref: "#/components/schemas/ImageGenerationData"
          description: The list of generated images

    ImageGenerationData:
      type: object
      properties:
        url:
          type: string
          description: The URL of the generated image, if response_format is "url"
        b64_json:
          type: string
          description: The base64-encoded JSON of the generated image, if response_format is "b64_json"
        revised_prompt:
          type: string
          description: The prompt that was used to generate the image, if there was a revision to the prompt

    # ---------- Reranking ----------
    RerankingRequest:
      type: object
      required: [model, query, documents]
      properties:
        model:
          type: string
          description: ID of the model to use
        query:
          type: string
          description: The search query
        documents:
          type: array
          items: { type: string }
          description: A list of documents to rerank
        top_n:
          type: integer
          description: The number of most relevant documents to return
        return_metadata:
          type: boolean
          description: Whether to return document metadata
        return_documents:
          type: boolean
          description: Whether to return the original documents

    RerankingResponse:
      type: object
      required: [results]
      properties:
        id:
          type: string
          description: A unique identifier for the reranking request
        results:
          type: array
          items:
            $ref: "#/components/schemas/RerankResult"
          description: The reranked documents
        model:
          type: string
          description: The name of the model used for reranking
        usage:
          $ref: "#/components/schemas/TokenUsage"

    RerankResult:
      type: object
      required: [index, score]
      properties:
        index:
          type: integer
          description: The index of the document in the original list
        score:
          type: number
          format: float
          description: The relevance score of the document
        document:
          type: string
          description: The original document text
        metadata:
          type: object
          description: Additional metadata about the document

    # ---------- Common ----------
    ErrorResponse:
      type: object
      required: [error]
      properties:
        error:
          type: string
          description: Error message describing what went wrong

    TokenUsage:
      type: object
      properties:
        prompt_tokens:
          type: integer
          description: Number of tokens in the prompt
        completion_tokens:
          type: integer
          description: Number of tokens in the generated completion
        total_tokens:
          type: integer
          description: Total number of tokens used in the request
