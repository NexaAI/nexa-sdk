{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NexaAI NPU Inference Examples\n",
        "\n",
        "This notebook demonstrates how to use the NexaAI SDK for various AI inference tasks on NPU devices, including:\n",
        "\n",
        "- **LLM (Large Language Model)**: Text generation and conversation\n",
        "- **VLM (Vision Language Model)**: Multimodal understanding and generation\n",
        "- **Embedder**: Text vectorization and similarity computation\n",
        "- **Reranker**: Document reranking\n",
        "- **CV (Computer Vision)**: Image processing and OCR tasks\n",
        "- **ASR (Automatic Speech Recognition)**: Speech-to-text transcription\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- Python 3.13\n",
        "- Windows ARM64 (Snapdragon X Elite) or NPU-capable device\n",
        "- NexaAI SDK installed: `pip install nexaai`\n",
        "- NEXA_TOKEN environment variable set (see next section)\n",
        "\n",
        "## Authentication Setup\n",
        "\n",
        "Before running any examples, you need to set up your NexaAI authentication token.\n",
        "\n",
        "### Set Token in Code\n",
        "\n",
        "Replace `\"YOUR_NEXA_TOKEN_HERE\"` with your actual NexaAI token from [https://sdk.nexa.ai/](https://sdk.nexa.ai/):\n",
        "\n",
        "```bash\n",
        "import os\n",
        "os.environ[\"NEXA_TOKEN\"] = \"YOUR_NEXA_TOKEN_HERE\"\n",
        "```\n",
        "\n",
        "### Alternative: Set Environment Variable\n",
        "\n",
        "You can also set the token as an environment variable before starting the notebook:\n",
        "\n",
        "**Windows:**\n",
        "\n",
        "```cmd\n",
        "set NEXA_TOKEN=YOUR_NEXA_TOKEN_HERE\n",
        "```\n",
        "\n",
        "**Linux/macOS:**\n",
        "\n",
        "```bash\n",
        "export NEXA_TOKEN=\"YOUR_NEXA_TOKEN_HERE\"\n",
        "```\n"
        "# CV model\n",
        "nexa pull NexaAI/paddleocr-npu\n",
        "\n",
        "# ASR model\n",
        "nexa pull NexaAI/parakeet-npu\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Replace \"YOUR_NEXA_TOKEN_HERE\" with your actual token from https://sdk.nexa.ai/\n",
        "os.environ[\"NEXA_TOKEN\"] = \"YOUR_NEXA_TOKEN_HERE\"\n",
        "import io\n",
        "import time\n",
        "import numpy as np\n",
        "from typing import List, Optional\n",
        "\n",
        "# NexaAI SDK imports\n",
        "from nexaai.llm import LLM, GenerationConfig\n",
        "from nexaai.vlm import VLM\n",
        "from nexaai.embedder import Embedder, EmbeddingConfig\n",
        "from nexaai.rerank import Reranker, RerankConfig\n",
        "from nexaai.cv import CV, CVConfig, CVCapabilities\n",
        "from nexaai.asr import ASR, ASRConfig\n",
        "from nexaai.common import ModelConfig, ChatMessage, MultiModalMessage, MultiModalMessageContent\n",
        "\n",
        "if os.environ.get(\"NEXA_TOKEN\") and os.environ[\"NEXA_TOKEN\"] != \"YOUR_NEXA_TOKEN_HERE\":\n",
        "    print(\"NEXA_TOKEN is set successfully!\")\n",
        "else:\n",
        "    print(\"Please set your NEXA_TOKEN before running the examples.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. LLM (Large Language Model) NPU Inference\n",
        "\n",
        "Using NPU-accelerated large language models for text generation and conversation. Llama3.2-3B-NPU-Turbo is specifically optimized for NPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import io\n",
        "\n",
        "from nexaai.common import GenerationConfig, ModelConfig, ChatMessage\n",
        "from nexaai.llm import LLM\n",
        "\n",
        "\n",
        "def llm_npu_example():\n",
        "    \"\"\"LLM NPU inference example\"\"\"\n",
        "    print(\"=== LLM NPU Inference Example ===\")\n",
        "\n",
        "    # Model configuration\n",
        "    model_name = \"NexaAI/Llama3.2-3B-NPU-Turbo\"\n",
        "    plugin_id = \"npu\"\n",
        "    device = \"npu\"\n",
        "    max_tokens = 100\n",
        "    system_message = \"You are a helpful assistant.\"\n",
        "\n",
        "    print(f\"Loading model: {model_name}\")\n",
        "    print(f\"Using plugin: {plugin_id}\")\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    # Create model instance\n",
        "    m_cfg = ModelConfig()\n",
        "    llm = LLM.from_(model_name, plugin_id=plugin_id, device_id=device, m_cfg=m_cfg)\n",
        "\n",
        "    # Create conversation history\n",
        "    conversation = [ChatMessage(role=\"system\", content=system_message)]\n",
        "\n",
        "    # Example conversations\n",
        "    test_prompts = [\n",
        "        \"What is artificial intelligence?\",\n",
        "        \"Explain the benefits of on-device AI processing.\",\n",
        "        \"How does NPU acceleration work?\"\n",
        "    ]\n",
        "\n",
        "    for i, prompt in enumerate(test_prompts, 1):\n",
        "        print(f\"\\n--- Conversation {i} ---\")\n",
        "        print(f\"User: {prompt}\")\n",
        "\n",
        "        # Add user message\n",
        "        conversation.append(ChatMessage(role=\"user\", content=prompt))\n",
        "\n",
        "        # Apply chat template\n",
        "        formatted_prompt = llm.apply_chat_template(conversation)\n",
        "\n",
        "        # Generate response\n",
        "        print(\"Assistant: \", end=\"\", flush=True)\n",
        "        response_buffer = io.StringIO()\n",
        "\n",
        "        for token in llm.generate_stream(formatted_prompt, g_cfg=GenerationConfig(max_tokens=max_tokens)):\n",
        "            print(token, end=\"\", flush=True)\n",
        "            response_buffer.write(token)\n",
        "\n",
        "        # Get profiling data\n",
        "        profiling_data = llm.get_profiling_data()\n",
        "        if profiling_data:\n",
        "            print(f\"\\nProfiling data: {profiling_data}\")\n",
        "\n",
        "        # Add assistant response to conversation history\n",
        "        conversation.append(ChatMessage(role=\"assistant\", content=response_buffer.getvalue()))\n",
        "        print(\"\\n\" + \"=\" *50)\n",
        "\n",
        "\n",
        "llm_npu_example()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. VLM (Vision Language Model) NPU Inference\n",
        "\n",
        "Using NPU-accelerated vision language models for multimodal understanding and generation. OmniNeural-4B supports joint processing of images and text.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import io\n",
        "\n",
        "from nexaai.vlm import VLM\n",
        "from nexaai.common import GenerationConfig, ModelConfig, MultiModalMessage, MultiModalMessageContent\n",
        "\n",
        "\n",
        "def vlm_npu_example():\n",
        "    \"\"\"VLM NPU inference example\"\"\"\n",
        "    print(\"=== VLM NPU Inference Example ===\")\n",
        "\n",
        "    # Model configuration\n",
        "    model_name = \"NexaAI/OmniNeural-4B\"\n",
        "    plugin_id = \"npu\"\n",
        "    device = \"npu\"\n",
        "    max_tokens = 100\n",
        "    system_message = \"You are a helpful assistant that can understand images and text.\"\n",
        "\n",
        "    print(f\"Loading model: {model_name}\")\n",
        "    print(f\"Using plugin: {plugin_id}\")\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    # Create model instance\n",
        "    m_cfg = ModelConfig()\n",
        "    vlm = VLM.from_(name_or_path=model_name, m_cfg=m_cfg, plugin_id=plugin_id, device_id=device)\n",
        "\n",
        "    # Create conversation history\n",
        "    conversation = [MultiModalMessage(role=\"system\",\n",
        "                                      content=[MultiModalMessageContent(type=\"text\", text=system_message)])]\n",
        "\n",
        "    # Example multimodal conversations\n",
        "    test_cases = [\n",
        "        {\n",
        "            \"text\": \"What do you see in this image?\",\n",
        "            \"image_path\": None  # Replace with actual image path if available\n",
        "        },\n",
        "        {\n",
        "            \"text\": \"Describe the main objects and their relationships.\",\n",
        "            \"image_path\": None\n",
        "        },\n",
        "        {\n",
        "            \"text\": \"What is artificial intelligence and how does it relate to computer vision?\",\n",
        "            \"image_path\": None\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    for i, case in enumerate(test_cases, 1):\n",
        "        print(f\"\\n--- Multimodal Conversation {i} ---\")\n",
        "        print(f\"User: {case['text']}\")\n",
        "\n",
        "        # Build message content\n",
        "        contents = [MultiModalMessageContent(type=\"text\", text=case['text'])]\n",
        "\n",
        "        # Add image content if available\n",
        "        if case['image_path'] and os.path.exists(case['image_path']):\n",
        "            contents.append(MultiModalMessageContent(type=\"image\", text=case['image_path']))\n",
        "            print(f\"Including image: {case['image_path']}\")\n",
        "\n",
        "        # Add user message\n",
        "        conversation.append(MultiModalMessage(role=\"user\", content=contents))\n",
        "\n",
        "        # Apply chat template\n",
        "        formatted_prompt = vlm.apply_chat_template(conversation)\n",
        "\n",
        "        # Generate response\n",
        "        print(\"Assistant: \", end=\"\", flush=True)\n",
        "        response_buffer = io.StringIO()\n",
        "\n",
        "        # Prepare image and audio paths\n",
        "        image_paths = [case['image_path']] if case['image_path'] and os.path.exists(case['image_path']) else None\n",
        "        audio_paths = None\n",
        "\n",
        "        for token in vlm.generate_stream(formatted_prompt,\n",
        "                                         g_cfg=GenerationConfig(max_tokens=max_tokens,\n",
        "                                                                image_paths=image_paths,\n",
        "                                                                audio_paths=audio_paths)):\n",
        "            print(token, end=\"\", flush=True)\n",
        "            response_buffer.write(token)\n",
        "\n",
        "        # Get profiling data\n",
        "        profiling_data = vlm.get_profiling_data()\n",
        "        if profiling_data:\n",
        "            print(f\"\\nProfiling data: {profiling_data}\")\n",
        "\n",
        "        # Add assistant response to conversation history\n",
        "        conversation.append(MultiModalMessage(role=\"assistant\",\n",
        "                                              content=[MultiModalMessageContent(type=\"text\", text=response_buffer.getvalue())]))\n",
        "        print(\"\\n\" + \"=\" *50)\n",
        "\n",
        "\n",
        "vlm_npu_example()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Embedder NPU Inference\n",
        "\n",
        "Using NPU-accelerated embedding models for text vectorization and similarity computation. embeddinggemma-300m-npu is a lightweight embedding model specifically optimized for NPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from nexaai.embedder import Embedder, EmbeddingConfig\n",
        "\n",
        "# Embedder NPU Inference Example\n",
        "\n",
        "\n",
        "def embedder_npu_example():\n",
        "    \"\"\"Embedder NPU inference example\"\"\"\n",
        "    print(\"=== Embedder NPU Inference Example ===\")\n",
        "\n",
        "    # Model configuration\n",
        "    model_name = \"NexaAI/embeddinggemma-300m-npu\"\n",
        "    plugin_id = \"npu\"\n",
        "    batch_size = 2\n",
        "\n",
        "    print(f\"Loading model: {model_name}\")\n",
        "    print(f\"Using plugin: {plugin_id}\")\n",
        "    print(f\"Batch size: {batch_size}\")\n",
        "\n",
        "    # Create embedder instance\n",
        "    embedder = Embedder.from_(name_or_path=model_name, plugin_id=plugin_id)\n",
        "    print('Embedder loaded successfully!')\n",
        "\n",
        "    # Get embedding dimension\n",
        "    dim = embedder.get_embedding_dim()\n",
        "    print(f\"Embedding dimension: {dim}\")\n",
        "\n",
        "    # Example texts\n",
        "    texts = [\n",
        "        \"On-device AI is a type of AI that is processed on the device itself, rather than in the cloud.\",\n",
        "        \"Nexa AI allows you to run state-of-the-art AI models locally on CPU, GPU, or NPU.\",\n",
        "        \"A ragdoll is a breed of cat that is known for its long, flowing hair and gentle personality.\",\n",
        "        \"The capital of France is Paris.\",\n",
        "        \"NPU acceleration provides significant performance improvements for AI workloads.\"\n",
        "    ]\n",
        "\n",
        "    query = \"what is on device AI\"\n",
        "\n",
        "    print(f\"\\n=== Generating Embeddings ===\")\n",
        "    print(f\"Processing {len(texts)} texts...\")\n",
        "\n",
        "    # Generate embeddings\n",
        "    embeddings = embedder.generate(\n",
        "        texts=texts,\n",
        "        config=EmbeddingConfig(batch_size=batch_size)\n",
        "    )\n",
        "\n",
        "    print(f\"Successfully generated {len(embeddings)} embeddings\")\n",
        "\n",
        "    # Display embedding information\n",
        "    print(f\"\\n=== Embedding Details ===\")\n",
        "    for i, (text, embedding) in enumerate(zip(texts, embeddings)):\n",
        "        print(f\"\\nText {i +1}:\")\n",
        "        print(f\"  Content: {text}\")\n",
        "        print(f\"  Embedding dimension: {len(embedding)}\")\n",
        "        print(f\"  First 10 elements: {embedding[:10]}\")\n",
        "        print(\"-\" * 70)\n",
        "\n",
        "    # Query processing\n",
        "    print(f\"\\n=== Query Processing ===\")\n",
        "    print(f\"Query: '{query}'\")\n",
        "\n",
        "    query_embedding = embedder.generate(\n",
        "        texts=[query],\n",
        "        config=EmbeddingConfig(batch_size=1)\n",
        "    )[0]\n",
        "\n",
        "    print(f\"Query embedding dimension: {len(query_embedding)}\")\n",
        "\n",
        "    # Similarity analysis\n",
        "    print(f\"\\n=== Similarity Analysis (Inner Product) ===\")\n",
        "    similarities = []\n",
        "\n",
        "    for i, (text, embedding) in enumerate(zip(texts, embeddings)):\n",
        "        query_vec = np.array(query_embedding)\n",
        "        text_vec = np.array(embedding)\n",
        "        inner_product = np.dot(query_vec, text_vec)\n",
        "        similarities.append((i, text, inner_product))\n",
        "\n",
        "        print(f\"\\nText {i +1}:\")\n",
        "        print(f\"  Content: {text}\")\n",
        "        print(f\"  Inner product with query: {inner_product:.6f}\")\n",
        "        print(\"-\" * 70)\n",
        "\n",
        "    # Sort and display most similar texts\n",
        "    similarities.sort(key=lambda x: x[2], reverse=True)\n",
        "\n",
        "    print(f\"\\n=== Similarity Ranking Results ===\")\n",
        "    for rank, (idx, text, score) in enumerate(similarities, 1):\n",
        "        print(f\"Rank {rank}: [{score:.6f}] {text}\")\n",
        "\n",
        "    return embeddings, query_embedding, similarities\n",
        "\n",
        "\n",
        "embeddings, query_emb, similarities = embedder_npu_example()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. CV (Computer Vision) NPU Inference\n",
        "\n",
        "Using NPU-accelerated computer vision models for image processing tasks such as OCR (Optical Character Recognition). CV models can perform text detection and recognition on images with NPU acceleration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CV NPU Inference Example\n",
        "def cv_npu_example():\n",
        "    \"\"\"CV NPU inference example\"\"\"\n",
        "    print(\"=== CV NPU Inference Example ===\")\n",
        "    \n",
        "    # Model configuration\n",
        "    model_name = \"NexaAI/paddleocr-npu\"  # Example CV model for OCR\n",
        "    plugin_id = \"npu\"\n",
        "    device = \"npu\"\n",
        "    \n",
        "    print(f\"Loading model: {model_name}\")\n",
        "    print(f\"Using plugin: {plugin_id}\")\n",
        "    print(f\"Device: {device}\")\n",
        "    \n",
        "    # Create CV instance\n",
        "    cv = CV.from_(name_or_path=model_name, plugin_id=plugin_id, device_id=device)\n",
        "    print('CV model loaded successfully!')\n",
        "    \n",
        "    # Example image file (replace with your actual image file)\n",
        "    image_file = \"path/to/your/image.jpg\"  # Replace with actual image file path\n",
        "    \n",
        "    print(f\"\\nNote: Please update the image_file path to point to your image file\")\n",
        "    print(f\"Current image_file: {image_file}\")\n",
        "    \n",
        "    # Check if image file exists\n",
        "    if not os.path.exists(image_file):\n",
        "        print(f\"Error: Image file not found: {image_file}\")\n",
        "        print(\"Please provide a valid image file path to test CV functionality.\")\n",
        "        return None\n",
        "    \n",
        "    # Basic CV configuration for OCR\n",
        "    config = CVConfig(\n",
        "        capabilities=CVCapabilities.OCR,\n",
        "        det_model_path=None,  # Will use default detection model\n",
        "        rec_model_path=None   # Will use default recognition model\n",
        "    )\n",
        "    \n",
        "    print(f\"\\n=== Starting CV Inference ===\")\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Perform CV inference (OCR)\n",
        "    result = cv.infer(image_path=image_file, config=config)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    inference_time = end_time - start_time\n",
        "    \n",
        "    # Display results\n",
        "    print(f\"\\n=== CV Inference Results ===\")\n",
        "    print(f\"Number of detected text regions: {result.result_count}\")\n",
        "    print(f\"Processing time: {inference_time:.2f} seconds\")\n",
        "    \n",
        "    # Display OCR results\n",
        "    if result.results:\n",
        "        print(f\"\\nDetected text:\")\n",
        "        for i, cv_result in enumerate(result.results):\n",
        "            if cv_result.text:\n",
        "                confidence = cv_result.confidence\n",
        "                text = cv_result.text.strip()\n",
        "                print(f\"  {i+1}. [{confidence:.2f}] {text}\")\n",
        "                \n",
        "                # Display bounding box if available\n",
        "                if cv_result.bbox:\n",
        "                    bbox = cv_result.bbox\n",
        "                    print(f\"      Bounding box: x={bbox.x:.1f}, y={bbox.y:.1f}, \"\n",
        "                          f\"w={bbox.width:.1f}, h={bbox.height:.1f}\")\n",
        "    else:\n",
        "        print(\"No text detected in the image.\")\n",
        "    \n",
        "    # Get profiling data\n",
        "    profiling_data = cv.get_profiling_data()\n",
        "    if profiling_data:\n",
        "        print(f\"\\nProfiling data: {profiling_data}\")\n",
        "    \n",
        "    return result\n",
        "\n",
        "# Run CV example\n",
        "cv_result = cv_npu_example()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. ASR (Automatic Speech Recognition) NPU Inference\n",
        "\n",
        "Using NPU-accelerated speech recognition models for speech-to-text transcription. parakeet-npu provides high-quality speech recognition with NPU acceleration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "from nexaai.asr import ASR, ASRConfig\n",
        "\n",
        "\n",
        "def asr_npu_example():\n",
        "    \"\"\"ASR NPU inference example\"\"\"\n",
        "    print(\"=== ASR NPU Inference Example ===\")\n",
        "\n",
        "    # Model configuration\n",
        "    model_name = \"NexaAI/parakeet-npu\"\n",
        "    plugin_id = \"npu\"\n",
        "    device = \"npu\"\n",
        "\n",
        "    print(f\"Loading model: {model_name}\")\n",
        "    print(f\"Using plugin: {plugin_id}\")\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    # Create ASR instance\n",
        "    asr = ASR.from_(name_or_path=model_name, plugin_id=plugin_id, device_id=device)\n",
        "    print('ASR model loaded successfully!')\n",
        "\n",
        "    # Example audio file (replace with your actual audio file)\n",
        "    audio_file = \"Users/mengshengwu/workspace/nexa-sdk/temp/jfk.wav\"  # Replace with actual audio file path\n",
        "\n",
        "    print(f\"\\nNote: Please update the audio_file path to point to your audio file\")\n",
        "    print(f\"Current audio_file: {audio_file}\")\n",
        "\n",
        "    # Check if audio file exists\n",
        "    if not os.path.exists(audio_file):\n",
        "        print(f\"Error: Audio file not found: {audio_file}\")\n",
        "        print(\"Please provide a valid audio file path to test ASR functionality.\")\n",
        "        return None\n",
        "\n",
        "    # Basic ASR configuration\n",
        "    config = ASRConfig(\n",
        "        timestamps=\"segment\",  # Get segment-level timestamps\n",
        "        beam_size=5,\n",
        "        stream=False\n",
        "    )\n",
        "\n",
        "    print(f\"\\n=== Starting Transcription ===\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Perform transcription\n",
        "    result = asr.transcribe(audio_path=audio_file, language=\"en\", config=config)\n",
        "\n",
        "    end_time = time.time()\n",
        "    transcription_time = end_time - start_time\n",
        "\n",
        "    # Display results\n",
        "    print(f\"\\n=== Transcription Results ===\")\n",
        "    print(f\"Transcription: {result.transcript}\")\n",
        "    print(f\"Processing time: {transcription_time:.2f} seconds\")\n",
        "\n",
        "    # Display segment information if available\n",
        "    if hasattr(result, 'segments') and result.segments:\n",
        "        print(f\"\\nSegments ({len(result.segments)}):\")\n",
        "        for i, segment in enumerate(result.segments[:3]):  # Show first 3 segments\n",
        "            start_time = segment.get('start', 'N/A')\n",
        "            end_time = segment.get('end', 'N/A')\n",
        "            text = segment.get('text', '').strip()\n",
        "            print(f\"  {i +1}. [{start_time:.2f}s - {end_time:.2f}s] {text}\")\n",
        "        if len(result.segments) > 3:\n",
        "            print(f\"  ... and {len(result.segments) - 3} more segments\")\n",
        "\n",
        "    # Get profiling data\n",
        "    profiling_data = asr.get_profiling_data()\n",
        "    if profiling_data:\n",
        "        print(f\"\\nProfiling data: {profiling_data}\")\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "result = asr_npu_example()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Reranker NPU Inference\n",
        "\n",
        "Using NPU-accelerated reranking models for document reranking. jina-v2-rerank-npu can perform precise similarity-based document ranking based on queries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nexaai.rerank import Reranker, RerankConfig\n",
        "\n",
        "\n",
        "def reranker_npu_example():\n",
        "    \"\"\"Reranker NPU inference example\"\"\"\n",
        "    print(\"=== Reranker NPU Inference Example ===\")\n",
        "\n",
        "    # Model configuration\n",
        "    model_name = \"NexaAI/jina-v2-rerank-npu\"\n",
        "    plugin_id = \"npu\"\n",
        "    batch_size = 4\n",
        "\n",
        "    print(f\"Loading model: {model_name}\")\n",
        "    print(f\"Using plugin: {plugin_id}\")\n",
        "    print(f\"Batch size: {batch_size}\")\n",
        "\n",
        "    # Create reranker instance\n",
        "    reranker = Reranker.from_(name_or_path=model_name, plugin_id=plugin_id)\n",
        "    print('Reranker loaded successfully!')\n",
        "\n",
        "    # Example queries and documents\n",
        "    queries = [\n",
        "        \"Where is on-device AI?\",\n",
        "        \"What is NPU acceleration?\",\n",
        "        \"How does machine learning work?\",\n",
        "        \"Tell me about computer vision\"\n",
        "    ]\n",
        "\n",
        "    documents = [\n",
        "        \"On-device AI is a type of AI that is processed on the device itself, rather than in the cloud.\",\n",
        "        \"NPU acceleration provides significant performance improvements for AI workloads on specialized hardware.\",\n",
        "        \"Edge computing brings computation and data storage closer to the sources of data.\",\n",
        "        \"A ragdoll is a breed of cat that is known for its long, flowing hair and gentle personality.\",\n",
        "        \"The capital of France is Paris, a beautiful city known for its art and culture.\",\n",
        "        \"Machine learning is a subset of artificial intelligence that enables computers to learn without being explicitly programmed.\",\n",
        "        \"Computer vision is a field of artificial intelligence that trains computers to interpret and understand visual information.\",\n",
        "        \"Deep learning uses neural networks with multiple layers to model and understand complex patterns in data.\"\n",
        "    ]\n",
        "\n",
        "    print(f\"\\n=== Document Reranking Test ===\")\n",
        "    print(f\"Number of documents: {len(documents)}\")\n",
        "\n",
        "    # Rerank for each query\n",
        "    for i, query in enumerate(queries, 1):\n",
        "        print(f\"\\n--- Query {i} ---\")\n",
        "        print(f\"Query: '{query}'\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        # Perform reranking\n",
        "        scores = reranker.rerank(\n",
        "            query=query,\n",
        "            documents=documents,\n",
        "            config=RerankConfig(batch_size=batch_size)\n",
        "        )\n",
        "\n",
        "        # Create (document, score) pairs and sort\n",
        "        doc_scores = list(zip(documents, scores))\n",
        "        doc_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Display ranking results\n",
        "        print(\"Reranking results:\")\n",
        "        for rank, (doc, score) in enumerate(doc_scores, 1):\n",
        "            print(f\"  {rank:2d}. [{score:.4f}] {doc}\")\n",
        "\n",
        "        # Display most relevant documents\n",
        "        print(f\"\\nMost relevant documents (top 3):\")\n",
        "        for rank, (doc, score) in enumerate(doc_scores[:3], 1):\n",
        "            print(f\"  {rank}. {doc}\")\n",
        "\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "    return reranker\n",
        "\n",
        "\n",
        "reranker = reranker_npu_example()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
