{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NexaAI macOS Inference Examples\n",
        "\n",
        "This notebook demonstrates how to use the NexaAI SDK for various AI inference tasks on macOS, including:\n",
        "\n",
        "- **LLM (Large Language Model)**: Text generation and conversation\n",
        "- **VLM (Vision Language Model)**: Multimodal understanding and generation\n",
        "- **Embedder**: Text vectorization and similarity computation\n",
        "- **Reranker**: Document reranking\n",
        "- **ASR (Automatic Speech Recognition)**: Speech-to-text transcription\n",
        "- **CV (Computer Vision)**: OCR/text recognition\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "### 1. Install the correct Python version\n",
        "\n",
        "NexaAI requires **Python 3.10** on macOS\n",
        "\n",
        "Verify the installation:\n",
        "\n",
        "```sh\n",
        "python -c \"import sys, platform; print(f'Python version: {sys.version}')\"\n",
        "```\n",
        "\n",
        "Your output should look like:\n",
        "\n",
        "> Python version: 3.10.18 (main, Jun 3 2025, 18:23:41) [Clang 17.0.0 (clang-1700.0.13.5)]\n",
        "\n",
        "Expected output must contain version `3.10.x`\n",
        "\n",
        "Here are suggested ways to install Python 3.10:\n",
        "\n",
        "**Option 1: Using Homebrew**\n",
        "\n",
        "If you don't have Homebrew, first install it from [https://brew.sh/](https://brew.sh/). Then, in your Terminal:\n",
        "\n",
        "```sh\n",
        "brew install python@3.10\n",
        "```\n",
        "\n",
        "**Option 2: Using Anaconda**\n",
        "\n",
        "```sh\n",
        "conda create -n nexaai python=3.10\n",
        "conda activate nexaai\n",
        "```\n",
        "\n",
        "After installation, you may need to access Python 3.10 using `python3.10`:\n",
        "\n",
        "```sh\n",
        "python3.10 --version\n",
        "```\n",
        "\n",
        "### 2. Create and activate a virtual environment\n",
        "\n",
        "`cd` to the current project root directory `cd path/to/nexa-sdk`.\n",
        "\n",
        "```sh\n",
        "python -m venv nexaai-env\n",
        "source nexaai-env/bin/activate\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Install the NexaAI SDK\n",
        "\n",
        "```bash\n",
        "pip install 'nexaai[mlx]'\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 4. Select the venv as your Jupyter Notebook kernel\n",
        "\n",
        "- Depending on the editor you are using, the way to change kernel might be different. For Cursor / VS Code, they are located at the top right corner of your code window.\n",
        "- Look for and select the `nexaai-env`, or the custom virtual environment you have created. The kernel should automatically reload in most IDEs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Verification of Kernel\n",
        "\n",
        "Run the following code to ensure you have the right kernel running.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import platform\n",
        "\n",
        "current_ver = sys.version_info\n",
        "arch = platform.machine()\n",
        "\n",
        "if current_ver.major != 3 or current_ver.minor != 10:\n",
        "    print(f\"❌ Error: Python {current_ver.major}.{current_ver.minor} detected\")\n",
        "    print(\"✅ Required: Python 3.10\")\n",
        "    print(\"Please install Python 3.10 and restart the kernel.\")\n",
        "    sys.exit(1)\n",
        "else:\n",
        "    print(\"✅ Python 3.10 running natively on Apple Silicon - Ready to proceed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Authentication Setup\n",
        "\n",
        "Before running any examples, you need to set up your NexaAI authentication token.\n",
        "\n",
        "### Set Token in Code\n",
        "\n",
        "Replace `\"YOUR_NEXA_TOKEN_HERE\"` with your actual NexaAI token from [https://sdk.nexa.ai/](https://sdk.nexa.ai/):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Replace \"YOUR_NEXA_TOKEN_HERE\" with your actual token from https://sdk.nexa.ai/\n",
        "os.environ[\"NEXA_TOKEN\"] = \"YOUR_NEXA_TOKEN_HERE\"\n",
        "\n",
        "assert os.environ.get(\"NEXA_TOKEN\", \"\").startswith(\n",
        "    \"key/\"), \"ERROR: NEXA_TOKEN must start with 'key/'. Please check your token.\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. LLM (Large Language Model) Inference\n",
        "\n",
        "Using -accelerated large language models for text generation and conversation. Llama3.2-3B--Turbo is specifically optimized for .\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import io\n",
        "import os\n",
        "\n",
        "from nexaai.common import GenerationConfig, ModelConfig, ChatMessage\n",
        "from nexaai.llm import LLM\n",
        "\n",
        "\n",
        "def llm_example():\n",
        "    \"\"\"LLM Inference example\"\"\"\n",
        "    print(\"=== LLM Inference Example ===\")\n",
        "\n",
        "    # Model configuration\n",
        "\n",
        "    # Use huggingface Repo ID\n",
        "    model_name = \"NexaAI/Qwen3-1.7B-4bit-MLX\"\n",
        "\n",
        "    max_tokens = 100\n",
        "    system_message = \"You are a helpful assistant.\"\n",
        "\n",
        "    print(f\"Loading model: {model_name}\")\n",
        "\n",
        "    # Create model instance\n",
        "    m_cfg = ModelConfig()\n",
        "    llm = LLM.from_(name_or_path=model_name, m_cfg=m_cfg)\n",
        "\n",
        "    # Create conversation history\n",
        "    conversation = [ChatMessage(role=\"system\", content=system_message)]\n",
        "\n",
        "    # Example conversations\n",
        "    test_prompts = [\n",
        "        \"What is artificial intelligence?\",\n",
        "        \"Explain the benefits of on-device AI processing.\",\n",
        "        \"How does NPU acceleration work?\"\n",
        "    ]\n",
        "\n",
        "    for i, prompt in enumerate(test_prompts, 1):\n",
        "        print(f\"\\n--- Conversation {i} ---\")\n",
        "        print(f\"User: {prompt}\")\n",
        "\n",
        "        # Add user message\n",
        "        conversation.append(ChatMessage(role=\"user\", content=prompt))\n",
        "\n",
        "        # Apply chat template\n",
        "        formatted_prompt = llm.apply_chat_template(conversation)\n",
        "\n",
        "        # Generate response\n",
        "        print(\"Assistant: \", end=\"\", flush=True)\n",
        "        response_buffer = io.StringIO()\n",
        "\n",
        "        for token in llm.generate_stream(formatted_prompt, g_cfg=GenerationConfig(max_tokens=max_tokens)):\n",
        "            print(token, end=\"\", flush=True)\n",
        "            response_buffer.write(token)\n",
        "\n",
        "        # Get profiling data\n",
        "        profiling_data = llm.get_profiling_data()\n",
        "        if profiling_data:\n",
        "            print(f\"\\nProfiling data: {profiling_data}\")\n",
        "\n",
        "        # Add assistant response to conversation history\n",
        "        conversation.append(ChatMessage(role=\"assistant\", content=response_buffer.getvalue()))\n",
        "        print(\"\\n\" + \"=\" * 50)\n",
        "\n",
        "\n",
        "llm_example()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. VLM (Vision Language Model) Inference\n",
        "\n",
        "Using vision language models for multimodal understanding and generation. OmniNeural-4B supports joint processing of images and text.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import io\n",
        "\n",
        "from nexaai.vlm import VLM\n",
        "from nexaai.common import GenerationConfig, ModelConfig, MultiModalMessage, MultiModalMessageContent\n",
        "\n",
        "\n",
        "def vlm_example():\n",
        "    \"\"\"VLM Inference example\"\"\"\n",
        "    print(\"=== VLM Inference Example ===\")\n",
        "\n",
        "    # Model configuration\n",
        "\n",
        "    # Use huggingface repo ID\n",
        "    model_name = \"NexaAI/gemma-3n-E2B-it-4bit-MLX\"\n",
        "\n",
        "    plugin_id = \"metal\"\n",
        "    \n",
        "    max_tokens = 100\n",
        "    system_message = \"You are a helpful assistant that can understand images and text.\"\n",
        "    image_path = '/your/image/path'  # Replace with actual image path if available\n",
        "\n",
        "    print(f\"Loading model: {model_name}\")\n",
        "    print(f\"Using plugin: {plugin_id}\")\n",
        "\n",
        "    # Check for image existence\n",
        "    if not (image_path and os.path.exists(image_path)):\n",
        "        print(\n",
        "            f\"\\033[93mWARNING: The specified image_path ('{image_path}') does not exist or was not provided. Multimodal prompts will not include image input.\\033[0m\")\n",
        "\n",
        "    # Create model instance\n",
        "    m_cfg = ModelConfig()\n",
        "    vlm = VLM.from_(name_or_path=model_name, m_cfg=m_cfg, plugin_id=plugin_id)\n",
        "\n",
        "    # Create conversation history\n",
        "    conversation = [MultiModalMessage(role=\"system\",\n",
        "                                      content=[MultiModalMessageContent(type=\"text\", text=system_message)])]\n",
        "\n",
        "    # Example multimodal conversations\n",
        "    test_cases = [\n",
        "        {\n",
        "            \"text\": \"What do you see in this image?\",\n",
        "            \"image_path\": image_path\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    for i, case in enumerate(test_cases, 1):\n",
        "        print(f\"\\n--- Multimodal Conversation {i} ---\")\n",
        "        print(f\"User: {case['text']}\")\n",
        "\n",
        "        # Build message content\n",
        "        contents = [MultiModalMessageContent(type=\"text\", text=case['text'])]\n",
        "\n",
        "        # Add image content if available\n",
        "        if case['image_path'] and os.path.exists(case['image_path']):\n",
        "            contents.append(MultiModalMessageContent(type=\"image\", path=case['image_path']))\n",
        "            print(f\"Including image: {case['image_path']}\")\n",
        "\n",
        "        # Add user message\n",
        "        conversation.append(MultiModalMessage(role=\"user\", content=contents))\n",
        "\n",
        "        # Apply chat template\n",
        "        formatted_prompt = vlm.apply_chat_template(conversation)\n",
        "\n",
        "        # Generate response\n",
        "        print(\"Assistant: \", end=\"\", flush=True)\n",
        "        response_buffer = io.StringIO()\n",
        "\n",
        "        # Prepare image and audio paths\n",
        "        image_paths = [case['image_path']] if case['image_path'] and os.path.exists(case['image_path']) else None\n",
        "        audio_paths = None\n",
        "\n",
        "        for token in vlm.generate_stream(formatted_prompt,\n",
        "                                         g_cfg=GenerationConfig(max_tokens=max_tokens,\n",
        "                                                                image_paths=image_paths,\n",
        "                                                                audio_paths=audio_paths)):\n",
        "            print(token, end=\"\", flush=True)\n",
        "            response_buffer.write(token)\n",
        "\n",
        "        # Get profiling data\n",
        "        profiling_data = vlm.get_profiling_data()\n",
        "        if profiling_data:\n",
        "            print(f\"\\nProfiling data: {profiling_data}\")\n",
        "\n",
        "        # Add assistant response to conversation history\n",
        "        conversation.append(MultiModalMessage(role=\"assistant\",\n",
        "                                              content=[MultiModalMessageContent(type=\"text\", text=response_buffer.getvalue())]))\n",
        "        print(\"\\n\" + \"=\" * 50)\n",
        "\n",
        "\n",
        "vlm_example()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Embedder Inference\n",
        "\n",
        "Using embedding models for text vectorization and similarity computation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from nexaai.embedder import Embedder, EmbeddingConfig\n",
        "\n",
        "# Embedder Inference Example\n",
        "\n",
        "\n",
        "def embedder_example():\n",
        "    \"\"\"Embedder Inference example\"\"\"\n",
        "    print(\"=== Embedder Inference Example ===\")\n",
        "\n",
        "    # Model configuration\n",
        "\n",
        "    # Use huggingface repo ID for MLX Community embedding model\n",
        "    model_name = \"mlx-community/embeddinggemma-300m-bf16\"\n",
        "\n",
        "    plugin_id = \"metal\"\n",
        "    batch_size = 2\n",
        "\n",
        "    print(f\"Loading model: {model_name}\")\n",
        "    print(f\"Using plugin: {plugin_id}\")\n",
        "    print(f\"Batch size: {batch_size}\")\n",
        "\n",
        "    # Create embedder instance\n",
        "    embedder = Embedder.from_(name_or_path=model_name, plugin_id=plugin_id)\n",
        "    print('Embedder loaded successfully!')\n",
        "\n",
        "    # Get embedding dimension\n",
        "    dim = embedder.get_embedding_dim()\n",
        "    print(f\"Embedding dimension: {dim}\")\n",
        "\n",
        "    # Example texts\n",
        "    texts = [\n",
        "        \"On-device AI is a type of AI that is processed on the device itself, rather than in the cloud.\",\n",
        "        \"Nexa AI allows you to run state-of-the-art AI models locally on CPU, GPU, or NPU.\",\n",
        "        \"A ragdoll is a breed of cat that is known for its long, flowing hair and gentle personality.\",\n",
        "        \"The capital of France is Paris.\",\n",
        "        \"NPU acceleration provides significant performance improvements for AI workloads.\"\n",
        "    ]\n",
        "\n",
        "    query = \"what is on device AI\"\n",
        "\n",
        "    print(f\"\\n=== Generating Embeddings ===\")\n",
        "    print(f\"Processing {len(texts)} texts...\")\n",
        "\n",
        "    # Generate embeddings\n",
        "    embeddings = embedder.generate(\n",
        "        texts=texts,\n",
        "        config=EmbeddingConfig(batch_size=batch_size)\n",
        "    )\n",
        "\n",
        "    print(f\"Successfully generated {len(embeddings)} embeddings\")\n",
        "\n",
        "    # Display embedding information\n",
        "    print(f\"\\n=== Embedding Details ===\")\n",
        "    for i, (text, embedding) in enumerate(zip(texts, embeddings)):\n",
        "        print(f\"\\nText {i + 1}:\")\n",
        "        print(f\"  Content: {text}\")\n",
        "        print(f\"  Embedding dimension: {len(embedding)}\")\n",
        "        print(f\"  First 10 elements: {embedding[:10]}\")\n",
        "        print(\"-\" * 70)\n",
        "\n",
        "    # Query processing\n",
        "    print(f\"\\n=== Query Processing ===\")\n",
        "    print(f\"Query: '{query}'\")\n",
        "\n",
        "    query_embedding = embedder.generate(\n",
        "        texts=[query],\n",
        "        config=EmbeddingConfig(batch_size=1)\n",
        "    )[0]\n",
        "\n",
        "    print(f\"Query embedding dimension: {len(query_embedding)}\")\n",
        "\n",
        "    # Similarity analysis\n",
        "    print(f\"\\n=== Similarity Analysis (Inner Product) ===\")\n",
        "    similarities = []\n",
        "\n",
        "    for i, (text, embedding) in enumerate(zip(texts, embeddings)):\n",
        "        query_vec = np.array(query_embedding)\n",
        "        text_vec = np.array(embedding)\n",
        "        inner_product = np.dot(query_vec, text_vec)\n",
        "        similarities.append((i, text, inner_product))\n",
        "\n",
        "        print(f\"\\nText {i + 1}:\")\n",
        "        print(f\"  Content: {text}\")\n",
        "        print(f\"  Inner product with query: {inner_product:.6f}\")\n",
        "        print(\"-\" * 70)\n",
        "\n",
        "    # Sort and display most similar texts\n",
        "    similarities.sort(key=lambda x: x[2], reverse=True)\n",
        "\n",
        "    print(f\"\\n=== Similarity Ranking Results ===\")\n",
        "    for rank, (idx, text, score) in enumerate(similarities, 1):\n",
        "        print(f\"Rank {rank}: [{score:.6f}] {text}\")\n",
        "\n",
        "    return embeddings, query_embedding, similarities\n",
        "\n",
        "\n",
        "embeddings, query_emb, similarities = embedder_example()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. ASR (Automatic Speech Recognition) Inference\n",
        "\n",
        "Using speech recognition models for speech-to-text transcription.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "from nexaai.asr import ASR, ASRConfig\n",
        "\n",
        "\n",
        "def asr_example():\n",
        "    \"\"\"ASR Inference example\"\"\"\n",
        "    print(\"=== ASR Inference Example ===\")\n",
        "\n",
        "    # Model configuration\n",
        "\n",
        "    # Use huggingface Repo ID\n",
        "    model_name = \"NexaAI/parakeet-tdt-0.6b-v2-MLX\"\n",
        "\n",
        "    plugin_id = \"metal\"\n",
        "    # Example audio file (replace with your actual audio file)\n",
        "    audio_file = r\"path/to/audio\"  # Replace with actual audio file path\n",
        "\n",
        "    print(f\"Loading model: {model_name}\")\n",
        "    print(f\"Using plugin: {plugin_id}\")\n",
        "\n",
        "    assert os.path.exists(\n",
        "        audio_file), f\"ERROR: The specified audio_file ('{audio_file}') does not exist. Please provide a valid audio file path to test ASR functionality.\"\n",
        "\n",
        "    # Create ASR instance\n",
        "    asr = ASR.from_(name_or_path=model_name, plugin_id=plugin_id)\n",
        "    print('ASR model loaded successfully!')\n",
        "\n",
        "    print(f\"\\nNote: Please update the audio_file path to point to your audio file\")\n",
        "    print(f\"Current audio_file: {audio_file}\")\n",
        "\n",
        "    # Check if audio file exists\n",
        "    if not os.path.exists(audio_file):\n",
        "        print(f\"Error: Audio file not found: {audio_file}\")\n",
        "        print(\"Please provide a valid audio file path to test ASR functionality.\")\n",
        "        return None\n",
        "\n",
        "    # Basic ASR configuration\n",
        "    config = ASRConfig(\n",
        "        timestamps=\"segment\",  # Get segment-level timestamps\n",
        "        beam_size=5,\n",
        "        stream=False\n",
        "    )\n",
        "\n",
        "    print(f\"\\n=== Starting Transcription ===\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Perform transcription\n",
        "    result = asr.transcribe(audio_path=audio_file, language=\"en\", config=config)\n",
        "\n",
        "    end_time = time.time()\n",
        "    transcription_time = end_time - start_time\n",
        "\n",
        "    # Display results\n",
        "    print(f\"\\n=== Transcription Results ===\")\n",
        "    print(f\"Transcription: {result.transcript}\")\n",
        "    print(f\"Processing time: {transcription_time:.2f} seconds\")\n",
        "\n",
        "    # Display segment information if available\n",
        "    if hasattr(result, 'segments') and result.segments:\n",
        "        print(f\"\\nSegments ({len(result.segments)}):\")\n",
        "        for i, segment in enumerate(result.segments[:3]):  # Show first 3 segments\n",
        "            start_time = segment.get('start', 'N/A')\n",
        "            end_time = segment.get('end', 'N/A')\n",
        "            text = segment.get('text', '').strip()\n",
        "            print(f\"  {i +1}. [{start_time:.2f}s - {end_time:.2f}s] {text}\")\n",
        "        if len(result.segments) > 3:\n",
        "            print(f\"  ... and {len(result.segments) - 3} more segments\")\n",
        "\n",
        "    # Get profiling data\n",
        "    profiling_data = asr.get_profiling_data()\n",
        "    if profiling_data:\n",
        "        print(f\"\\nProfiling data: {profiling_data}\")\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "result = asr_example()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Reranker Inference\n",
        "\n",
        "Using reranking models for document reranking. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nexaai.rerank import Reranker, RerankConfig\n",
        "\n",
        "\n",
        "def reranker_example():\n",
        "    \"\"\"Reranker Inference example\"\"\"\n",
        "    print(\"=== Reranker Inference Example ===\")\n",
        "\n",
        "    # Model configuration\n",
        "\n",
        "    # Use huggingface repo ID\n",
        "    model_name = \"NexaAI/jina-v2-rerank-mlx\"\n",
        "\n",
        "    plugin_id = \"metal\"\n",
        "    batch_size = 4\n",
        "\n",
        "    print(f\"Loading model: {model_name}\")\n",
        "    print(f\"Using plugin: {plugin_id}\")\n",
        "    print(f\"Batch size: {batch_size}\")\n",
        "\n",
        "    # Create reranker instance\n",
        "    reranker = Reranker.from_(name_or_path=model_name, plugin_id=plugin_id)\n",
        "    print('Reranker loaded successfully!')\n",
        "\n",
        "    # Example queries and documents\n",
        "    queries = [\n",
        "        \"Where is on-device AI?\",\n",
        "        \"What is NPU acceleration?\",\n",
        "        \"How does machine learning work?\",\n",
        "        \"Tell me about computer vision\"\n",
        "    ]\n",
        "\n",
        "    documents = [\n",
        "        \"On-device AI is a type of AI that is processed on the device itself, rather than in the cloud.\",\n",
        "        \"NPU acceleration provides significant performance improvements for AI workloads on specialized hardware.\",\n",
        "        \"Edge computing brings computation and data storage closer to the sources of data.\",\n",
        "        \"A ragdoll is a breed of cat that is known for its long, flowing hair and gentle personality.\",\n",
        "        \"The capital of France is Paris, a beautiful city known for its art and culture.\",\n",
        "        \"Machine learning is a subset of artificial intelligence that enables computers to learn without being explicitly programmed.\",\n",
        "        \"Computer vision is a field of artificial intelligence that trains computers to interpret and understand visual information.\",\n",
        "        \"Deep learning uses neural networks with multiple layers to model and understand complex patterns in data.\"\n",
        "    ]\n",
        "\n",
        "    print(f\"\\n=== Document Reranking Test ===\")\n",
        "    print(f\"Number of documents: {len(documents)}\")\n",
        "\n",
        "    # Rerank for each query\n",
        "    for i, query in enumerate(queries, 1):\n",
        "        print(f\"\\n--- Query {i} ---\")\n",
        "        print(f\"Query: '{query}'\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        # Perform reranking\n",
        "        scores = reranker.rerank(\n",
        "            query=query,\n",
        "            documents=documents,\n",
        "            config=RerankConfig(batch_size=batch_size)\n",
        "        )\n",
        "\n",
        "        # Create (document, score) pairs and sort\n",
        "        doc_scores = list(zip(documents, scores))\n",
        "        doc_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Display ranking results\n",
        "        print(\"Reranking results:\")\n",
        "        for rank, (doc, score) in enumerate(doc_scores, 1):\n",
        "            print(f\"  {rank:2d}. [{score:.4f}] {doc}\")\n",
        "\n",
        "        # Display most relevant documents\n",
        "        print(f\"\\nMost relevant documents (top 3):\")\n",
        "        for rank, (doc, score) in enumerate(doc_scores[:3], 1):\n",
        "            print(f\"  {rank}. {doc}\")\n",
        "\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "    return reranker\n",
        "\n",
        "\n",
        "reranker = reranker_example()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Computer Vision (CV) Inference\n",
        "\n",
        "Run computer vision tasks (e.g., OCR/text recognition) on images.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from nexaai.cv import CVCapabilities, CVModel, CVModelConfig, CVResults\n",
        "\n",
        "\n",
        "def cv_ocr_example():\n",
        "\n",
        "    # Use huggingface repo ID\n",
        "    model_name = \"NexaAI/paddle-ocr-mlx\"\n",
        "\n",
        "    image_path = r\"path/to/image\"\n",
        "\n",
        "    config = CVModelConfig(capabilities=CVCapabilities.OCR)\n",
        "    cv = CVModel.from_(name_or_path=model_name, config=config, plugin_id='metal')\n",
        "\n",
        "    assert os.path.exists(image_path), f\"ERROR: Image file not found: {image_path}\"\n",
        "\n",
        "    results = cv.infer(image_path)\n",
        "\n",
        "    print(f\"Number of results: {results.result_count}\")\n",
        "    for result in results.results:\n",
        "        print(f\"[{result.confidence:.2f}] {result.text}\")\n",
        "\n",
        "\n",
        "cv_ocr_example()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
