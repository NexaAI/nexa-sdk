{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NexaAI NPU Inference Examples\n",
        "\n",
        "This notebook demonstrates how to use the NexaAI SDK for various AI inference tasks on NPU devices, including:\n",
        "\n",
        "- **LLM (Large Language Model)**: Text generation and conversation\n",
        "- **VLM (Vision Language Model)**: Multimodal understanding and generation\n",
        "- **Embedder**: Text vectorization and similarity computation\n",
        "- **Reranker**: Document reranking\n",
        "- **ASR (Automatic Speech Recognition)**: Speech-to-text transcription\n",
        "- **CV (Computer Vision)**: OCR/text recognition\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "### 1. Install the correct Python version\n",
        "NexaAI requires **Python 3.11 – 3.13 (ARM64 build)** on Windows ARM.\n",
        "Please **download and install the official ARM64 Python** from the [python-3.11.1-arm64.exe](https://www.python.org/ftp/python/3.11.1/python-3.11.1-arm64.exe).\n",
        "\n",
        "> ⚠️ Do **not** use Conda or x86 builds — they are incompatible with native ARM64 binaries.\n",
        "\n",
        "Verify the installation:\n",
        "\n",
        "```sh\n",
        "python -c \"import sys, platform; print(f'Python version: {sys.version} | Architecture: {platform.machine()}')\"\n",
        "```\n",
        "\n",
        "Expected output should contain `arm64`.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Create and activate a virtual environment\n",
        "\n",
        "```sh\n",
        "python -m venv nexaai-env\n",
        "nexaai-env\\Scripts\\activate\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Install the NexaAI SDK\n",
        "\n",
        "```bash\n",
        "pip install nexaai\n",
        "```\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Authentication Setup\n",
        "\n",
        "Before running any examples, you need to set up your NexaAI authentication token.\n",
        "\n",
        "### Set Token in Code\n",
        "\n",
        "Replace `\"YOUR_NEXA_TOKEN_HERE\"` with your actual NexaAI token from [https://sdk.nexa.ai/](https://sdk.nexa.ai/):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Replace \"YOUR_NEXA_TOKEN_HERE\" with your actual token from https://sdk.nexa.ai/\n",
        "os.environ[\"NEXA_TOKEN\"] = \"YOUR_NEXA_TOKEN_HERE\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. LLM (Large Language Model) NPU Inference\n",
        "\n",
        "Using NPU-accelerated large language models for text generation and conversation. Llama3.2-3B-NPU-Turbo is specifically optimized for NPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import io\n",
        "\n",
        "from nexaai.common import GenerationConfig, ModelConfig, ChatMessage\n",
        "from nexaai.llm import LLM\n",
        "\n",
        "\n",
        "def llm_npu_example():\n",
        "    \"\"\"LLM NPU inference example\"\"\"\n",
        "    print(\"=== LLM NPU Inference Example ===\")\n",
        "\n",
        "    # Model configuration\n",
        "    model_name = \"NexaAI/Llama3.2-3B-NPU-Turbo\"\n",
        "    plugin_id = \"npu\"\n",
        "    device = \"npu\"\n",
        "    max_tokens = 100\n",
        "    system_message = \"You are a helpful assistant.\"\n",
        "\n",
        "    print(f\"Loading model: {model_name}\")\n",
        "    print(f\"Using plugin: {plugin_id}\")\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    # Create model instance\n",
        "    m_cfg = ModelConfig()\n",
        "    llm = LLM.from_(model_name, plugin_id=plugin_id, device_id=device, m_cfg=m_cfg)\n",
        "\n",
        "    # Create conversation history\n",
        "    conversation = [ChatMessage(role=\"system\", content=system_message)]\n",
        "\n",
        "    # Example conversations\n",
        "    test_prompts = [\n",
        "        \"What is artificial intelligence?\",\n",
        "        \"Explain the benefits of on-device AI processing.\",\n",
        "        \"How does NPU acceleration work?\"\n",
        "    ]\n",
        "\n",
        "    for i, prompt in enumerate(test_prompts, 1):\n",
        "        print(f\"\\n--- Conversation {i} ---\")\n",
        "        print(f\"User: {prompt}\")\n",
        "\n",
        "        # Add user message\n",
        "        conversation.append(ChatMessage(role=\"user\", content=prompt))\n",
        "\n",
        "        # Apply chat template\n",
        "        formatted_prompt = llm.apply_chat_template(conversation)\n",
        "\n",
        "        # Generate response\n",
        "        print(\"Assistant: \", end=\"\", flush=True)\n",
        "        response_buffer = io.StringIO()\n",
        "\n",
        "        for token in llm.generate_stream(formatted_prompt, g_cfg=GenerationConfig(max_tokens=max_tokens)):\n",
        "            print(token, end=\"\", flush=True)\n",
        "            response_buffer.write(token)\n",
        "\n",
        "        # Get profiling data\n",
        "        profiling_data = llm.get_profiling_data()\n",
        "        if profiling_data:\n",
        "            print(f\"\\nProfiling data: {profiling_data}\")\n",
        "\n",
        "        # Add assistant response to conversation history\n",
        "        conversation.append(ChatMessage(role=\"assistant\", content=response_buffer.getvalue()))\n",
        "        print(\"\\n\" + \"=\" *50)\n",
        "\n",
        "\n",
        "llm_npu_example()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. VLM (Vision Language Model) NPU Inference\n",
        "\n",
        "Using NPU-accelerated vision language models for multimodal understanding and generation. OmniNeural-4B supports joint processing of images and text.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import io\n",
        "\n",
        "from nexaai.vlm import VLM\n",
        "from nexaai.common import GenerationConfig, ModelConfig, MultiModalMessage, MultiModalMessageContent\n",
        "\n",
        "\n",
        "def vlm_npu_example():\n",
        "    \"\"\"VLM NPU inference example\"\"\"\n",
        "    print(\"=== VLM NPU Inference Example ===\")\n",
        "\n",
        "    # Model configuration\n",
        "    model_name = \"NexaAI/OmniNeural-4B\"\n",
        "    plugin_id = \"npu\"\n",
        "    device = \"npu\"\n",
        "    max_tokens = 100\n",
        "    system_message = \"You are a helpful assistant that can understand images and text.\"\n",
        "\n",
        "    print(f\"Loading model: {model_name}\")\n",
        "    print(f\"Using plugin: {plugin_id}\")\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    # Create model instance\n",
        "    m_cfg = ModelConfig()\n",
        "    vlm = VLM.from_(name_or_path=model_name, m_cfg=m_cfg, plugin_id=plugin_id, device_id=device)\n",
        "\n",
        "    # Create conversation history\n",
        "    conversation = [MultiModalMessage(role=\"system\",\n",
        "                                      content=[MultiModalMessageContent(type=\"text\", text=system_message)])]\n",
        "\n",
        "    # Example multimodal conversations\n",
        "    test_cases = [\n",
        "        {\n",
        "            \"text\": \"What do you see in this image?\",\n",
        "            \"image_path\": '/your/image/path'  # Replace with actual image path if available\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    for i, case in enumerate(test_cases, 1):\n",
        "        print(f\"\\n--- Multimodal Conversation {i} ---\")\n",
        "        print(f\"User: {case['text']}\")\n",
        "\n",
        "        # Build message content\n",
        "        contents = [MultiModalMessageContent(type=\"text\", text=case['text'])]\n",
        "\n",
        "        # Add image content if available\n",
        "        if case['image_path'] and os.path.exists(case['image_path']):\n",
        "            contents.append(MultiModalMessageContent(type=\"image\", path=case['image_path']))\n",
        "            print(f\"Including image: {case['image_path']}\")\n",
        "\n",
        "        # Add user message\n",
        "        conversation.append(MultiModalMessage(role=\"user\", content=contents))\n",
        "\n",
        "        # Apply chat template\n",
        "        formatted_prompt = vlm.apply_chat_template(conversation)\n",
        "\n",
        "        # Generate response\n",
        "        print(\"Assistant: \", end=\"\", flush=True)\n",
        "        response_buffer = io.StringIO()\n",
        "\n",
        "        # Prepare image and audio paths\n",
        "        image_paths = [case['image_path']] if case['image_path'] and os.path.exists(case['image_path']) else None\n",
        "        audio_paths = None\n",
        "\n",
        "        for token in vlm.generate_stream(formatted_prompt,\n",
        "                                         g_cfg=GenerationConfig(max_tokens=max_tokens,\n",
        "                                                                image_paths=image_paths,\n",
        "                                                                audio_paths=audio_paths)):\n",
        "            print(token, end=\"\", flush=True)\n",
        "            response_buffer.write(token)\n",
        "\n",
        "        # Get profiling data\n",
        "        profiling_data = vlm.get_profiling_data()\n",
        "        if profiling_data:\n",
        "            print(f\"\\nProfiling data: {profiling_data}\")\n",
        "\n",
        "        # Add assistant response to conversation history\n",
        "        conversation.append(MultiModalMessage(role=\"assistant\",\n",
        "                                              content=[MultiModalMessageContent(type=\"text\", text=response_buffer.getvalue())]))\n",
        "        print(\"\\n\" + \"=\" *50)\n",
        "\n",
        "\n",
        "vlm_npu_example()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Embedder NPU Inference\n",
        "\n",
        "Using NPU-accelerated embedding models for text vectorization and similarity computation. embeddinggemma-300m-npu is a lightweight embedding model specifically optimized for NPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from nexaai.embedder import Embedder, EmbeddingConfig\n",
        "\n",
        "# Embedder NPU Inference Example\n",
        "\n",
        "\n",
        "def embedder_npu_example():\n",
        "    \"\"\"Embedder NPU inference example\"\"\"\n",
        "    print(\"=== Embedder NPU Inference Example ===\")\n",
        "\n",
        "    # Model configuration\n",
        "    model_name = \"NexaAI/embeddinggemma-300m-npu\"\n",
        "    plugin_id = \"npu\"\n",
        "    batch_size = 2\n",
        "\n",
        "    print(f\"Loading model: {model_name}\")\n",
        "    print(f\"Using plugin: {plugin_id}\")\n",
        "    print(f\"Batch size: {batch_size}\")\n",
        "\n",
        "    # Create embedder instance\n",
        "    embedder = Embedder.from_(name_or_path=model_name, plugin_id=plugin_id)\n",
        "    print('Embedder loaded successfully!')\n",
        "\n",
        "    # Get embedding dimension\n",
        "    dim = embedder.get_embedding_dim()\n",
        "    print(f\"Embedding dimension: {dim}\")\n",
        "\n",
        "    # Example texts\n",
        "    texts = [\n",
        "        \"On-device AI is a type of AI that is processed on the device itself, rather than in the cloud.\",\n",
        "        \"Nexa AI allows you to run state-of-the-art AI models locally on CPU, GPU, or NPU.\",\n",
        "        \"A ragdoll is a breed of cat that is known for its long, flowing hair and gentle personality.\",\n",
        "        \"The capital of France is Paris.\",\n",
        "        \"NPU acceleration provides significant performance improvements for AI workloads.\"\n",
        "    ]\n",
        "\n",
        "    query = \"what is on device AI\"\n",
        "\n",
        "    print(f\"\\n=== Generating Embeddings ===\")\n",
        "    print(f\"Processing {len(texts)} texts...\")\n",
        "\n",
        "    # Generate embeddings\n",
        "    embeddings = embedder.generate(\n",
        "        texts=texts,\n",
        "        config=EmbeddingConfig(batch_size=batch_size)\n",
        "    )\n",
        "\n",
        "    print(f\"Successfully generated {len(embeddings)} embeddings\")\n",
        "\n",
        "    # Display embedding information\n",
        "    print(f\"\\n=== Embedding Details ===\")\n",
        "    for i, (text, embedding) in enumerate(zip(texts, embeddings)):\n",
        "        print(f\"\\nText {i + 1}:\")\n",
        "        print(f\"  Content: {text}\")\n",
        "        print(f\"  Embedding dimension: {len(embedding)}\")\n",
        "        print(f\"  First 10 elements: {embedding[:10]}\")\n",
        "        print(\"-\" * 70)\n",
        "\n",
        "    # Query processing\n",
        "    print(f\"\\n=== Query Processing ===\")\n",
        "    print(f\"Query: '{query}'\")\n",
        "\n",
        "    query_embedding = embedder.generate(\n",
        "        texts=[query],\n",
        "        config=EmbeddingConfig(batch_size=1)\n",
        "    )[0]\n",
        "\n",
        "    print(f\"Query embedding dimension: {len(query_embedding)}\")\n",
        "\n",
        "    # Similarity analysis\n",
        "    print(f\"\\n=== Similarity Analysis (Inner Product) ===\")\n",
        "    similarities = []\n",
        "\n",
        "    for i, (text, embedding) in enumerate(zip(texts, embeddings)):\n",
        "        query_vec = np.array(query_embedding)\n",
        "        text_vec = np.array(embedding)\n",
        "        inner_product = np.dot(query_vec, text_vec)\n",
        "        similarities.append((i, text, inner_product))\n",
        "\n",
        "        print(f\"\\nText {i + 1}:\")\n",
        "        print(f\"  Content: {text}\")\n",
        "        print(f\"  Inner product with query: {inner_product:.6f}\")\n",
        "        print(\"-\" * 70)\n",
        "\n",
        "    # Sort and display most similar texts\n",
        "    similarities.sort(key=lambda x: x[2], reverse=True)\n",
        "\n",
        "    print(f\"\\n=== Similarity Ranking Results ===\")\n",
        "    for rank, (idx, text, score) in enumerate(similarities, 1):\n",
        "        print(f\"Rank {rank}: [{score:.6f}] {text}\")\n",
        "\n",
        "    return embeddings, query_embedding, similarities\n",
        "\n",
        "\n",
        "embeddings, query_emb, similarities = embedder_npu_example()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. ASR (Automatic Speech Recognition) NPU Inference\n",
        "\n",
        "Using NPU-accelerated speech recognition models for speech-to-text transcription. parakeet-npu provides high-quality speech recognition with NPU acceleration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "from nexaai.asr import ASR, ASRConfig\n",
        "\n",
        "\n",
        "def asr_npu_example():\n",
        "    \"\"\"ASR NPU inference example\"\"\"\n",
        "    print(\"=== ASR NPU Inference Example ===\")\n",
        "\n",
        "    # Model configuration\n",
        "    model_name = \"NexaAI/parakeet-npu\"\n",
        "    plugin_id = \"npu\"\n",
        "    device = \"npu\"\n",
        "\n",
        "    print(f\"Loading model: {model_name}\")\n",
        "    print(f\"Using plugin: {plugin_id}\")\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    # Create ASR instance\n",
        "    asr = ASR.from_(name_or_path=model_name, plugin_id=plugin_id, device_id=device)\n",
        "    print('ASR model loaded successfully!')\n",
        "\n",
        "    # Example audio file (replace with your actual audio file)\n",
        "    audio_file = \"Users/mengshengwu/workspace/nexa-sdk/temp/jfk.wav\"  # Replace with actual audio file path\n",
        "\n",
        "    print(f\"\\nNote: Please update the audio_file path to point to your audio file\")\n",
        "    print(f\"Current audio_file: {audio_file}\")\n",
        "\n",
        "    # Check if audio file exists\n",
        "    if not os.path.exists(audio_file):\n",
        "        print(f\"Error: Audio file not found: {audio_file}\")\n",
        "        print(\"Please provide a valid audio file path to test ASR functionality.\")\n",
        "        return None\n",
        "\n",
        "    # Basic ASR configuration\n",
        "    config = ASRConfig(\n",
        "        timestamps=\"segment\",  # Get segment-level timestamps\n",
        "        beam_size=5,\n",
        "        stream=False\n",
        "    )\n",
        "\n",
        "    print(f\"\\n=== Starting Transcription ===\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Perform transcription\n",
        "    result = asr.transcribe(audio_path=audio_file, language=\"en\", config=config)\n",
        "\n",
        "    end_time = time.time()\n",
        "    transcription_time = end_time - start_time\n",
        "\n",
        "    # Display results\n",
        "    print(f\"\\n=== Transcription Results ===\")\n",
        "    print(f\"Transcription: {result.transcript}\")\n",
        "    print(f\"Processing time: {transcription_time:.2f} seconds\")\n",
        "\n",
        "    # Display segment information if available\n",
        "    if hasattr(result, 'segments') and result.segments:\n",
        "        print(f\"\\nSegments ({len(result.segments)}):\")\n",
        "        for i, segment in enumerate(result.segments[:3]):  # Show first 3 segments\n",
        "            start_time = segment.get('start', 'N/A')\n",
        "            end_time = segment.get('end', 'N/A')\n",
        "            text = segment.get('text', '').strip()\n",
        "            print(f\"  {i +1}. [{start_time:.2f}s - {end_time:.2f}s] {text}\")\n",
        "        if len(result.segments) > 3:\n",
        "            print(f\"  ... and {len(result.segments) - 3} more segments\")\n",
        "\n",
        "    # Get profiling data\n",
        "    profiling_data = asr.get_profiling_data()\n",
        "    if profiling_data:\n",
        "        print(f\"\\nProfiling data: {profiling_data}\")\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "result = asr_npu_example()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Reranker NPU Inference\n",
        "\n",
        "Using NPU-accelerated reranking models for document reranking. jina-v2-rerank-npu can perform precise similarity-based document ranking based on queries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nexaai.rerank import Reranker, RerankConfig\n",
        "\n",
        "\n",
        "def reranker_npu_example():\n",
        "    \"\"\"Reranker NPU inference example\"\"\"\n",
        "    print(\"=== Reranker NPU Inference Example ===\")\n",
        "\n",
        "    # Model configuration\n",
        "    model_name = \"NexaAI/jina-v2-rerank-npu\"\n",
        "    plugin_id = \"npu\"\n",
        "    batch_size = 4\n",
        "\n",
        "    print(f\"Loading model: {model_name}\")\n",
        "    print(f\"Using plugin: {plugin_id}\")\n",
        "    print(f\"Batch size: {batch_size}\")\n",
        "\n",
        "    # Create reranker instance\n",
        "    reranker = Reranker.from_(name_or_path=model_name, plugin_id=plugin_id)\n",
        "    print('Reranker loaded successfully!')\n",
        "\n",
        "    # Example queries and documents\n",
        "    queries = [\n",
        "        \"Where is on-device AI?\",\n",
        "        \"What is NPU acceleration?\",\n",
        "        \"How does machine learning work?\",\n",
        "        \"Tell me about computer vision\"\n",
        "    ]\n",
        "\n",
        "    documents = [\n",
        "        \"On-device AI is a type of AI that is processed on the device itself, rather than in the cloud.\",\n",
        "        \"NPU acceleration provides significant performance improvements for AI workloads on specialized hardware.\",\n",
        "        \"Edge computing brings computation and data storage closer to the sources of data.\",\n",
        "        \"A ragdoll is a breed of cat that is known for its long, flowing hair and gentle personality.\",\n",
        "        \"The capital of France is Paris, a beautiful city known for its art and culture.\",\n",
        "        \"Machine learning is a subset of artificial intelligence that enables computers to learn without being explicitly programmed.\",\n",
        "        \"Computer vision is a field of artificial intelligence that trains computers to interpret and understand visual information.\",\n",
        "        \"Deep learning uses neural networks with multiple layers to model and understand complex patterns in data.\"\n",
        "    ]\n",
        "\n",
        "    print(f\"\\n=== Document Reranking Test ===\")\n",
        "    print(f\"Number of documents: {len(documents)}\")\n",
        "\n",
        "    # Rerank for each query\n",
        "    for i, query in enumerate(queries, 1):\n",
        "        print(f\"\\n--- Query {i} ---\")\n",
        "        print(f\"Query: '{query}'\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        # Perform reranking\n",
        "        scores = reranker.rerank(\n",
        "            query=query,\n",
        "            documents=documents,\n",
        "            config=RerankConfig(batch_size=batch_size)\n",
        "        )\n",
        "\n",
        "        # Create (document, score) pairs and sort\n",
        "        doc_scores = list(zip(documents, scores))\n",
        "        doc_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Display ranking results\n",
        "        print(\"Reranking results:\")\n",
        "        for rank, (doc, score) in enumerate(doc_scores, 1):\n",
        "            print(f\"  {rank:2d}. [{score:.4f}] {doc}\")\n",
        "\n",
        "        # Display most relevant documents\n",
        "        print(f\"\\nMost relevant documents (top 3):\")\n",
        "        for rank, (doc, score) in enumerate(doc_scores[:3], 1):\n",
        "            print(f\"  {rank}. {doc}\")\n",
        "\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "    return reranker\n",
        "\n",
        "\n",
        "reranker = reranker_npu_example()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ## 6. Computer Vision (CV) NPU Inference\n",
        "#\n",
        "# Run NPU-accelerated computer vision tasks (e.g., OCR/text recognition) on images. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from nexaai.cv import CVCapabilities, CVModel, CVModelConfig, CVResults\n",
        "\n",
        "def cv_ocr_example():\n",
        "    config = CVModelConfig(capabilities=CVCapabilities.OCR)\n",
        "    cv = CVModel.from_(name_or_path=\"NexaAI/paddleocr-npu\", config=config, plugin_id='npu')\n",
        "    image = '/path/to/image'\n",
        "\n",
        "    if not os.path.exists(image):\n",
        "        raise FileNotFoundError(f\"Image file not found: {image}\")\n",
        "        \n",
        "    results = cv.infer(image)\n",
        "\n",
        "    print(f\"Number of results: {results.result_count}\")\n",
        "    for result in results.results:\n",
        "        print(f\"[{result.confidence:.2f}] {result.text}\")\n",
        "\n",
        "\n",
        "\n",
        "cv_ocr_example()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
