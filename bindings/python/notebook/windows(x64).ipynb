{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NexaAI windows(x64) Inference Examples\n",
        "\n",
        "This notebook demonstrates how to use the NexaAI SDK for various AI inference tasks on windows(x64), including:\n",
        "\n",
        "- **LLM (Large Language Model)**: Text generation and conversation\n",
        "- **VLM (Vision Language Model)**: Multimodal understanding and generation\n",
        "- **Embedder**: Text vectorization and similarity computation\n",
        "- **Reranker**: Document reranking\n",
        "- **ASR (Automatic Speech Recognition)**: Speech-to-text transcription\n",
        "- **CV (Computer Vision)**: OCR/text recognition\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "### 1. Install the correct Python version\n",
        "\n",
        "NexaAI requires **Python 3.10** on windows(x64)\n",
        "\n",
        "Verify the installation:\n",
        "\n",
        "```sh\n",
        "python -c \"import sys, platform; print(f'Python version: {sys.version}')\"\n",
        "```\n",
        "\n",
        "Your output should look like:\n",
        "\n",
        "> Python version: 3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)]\n",
        "\n",
        "Expected output must contain version `3.10.x\n",
        "\n",
        "Here are suggested ways to install Python 3.10:\n",
        "\n",
        "**Using Anaconda**\n",
        "\n",
        "```sh\n",
        "conda create -n nexaai python=3.10\n",
        "conda activate nexaai\n",
        "```\n",
        "\n",
        "After installation, you may need to access Python 3.10 using `python3.10`:\n",
        "\n",
        "```sh\n",
        "python3.10 --version\n",
        "```\n",
        "\n",
        "### 2. Create and activate a virtual environment\n",
        "\n",
        "`cd` to the current project root directory `cd path/to/nexa-sdk`.\n",
        "\n",
        "```sh\n",
        "python -m venv nexaai-env\n",
        "source nexaai-env/bin/activate\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Install the NexaAI SDK\n",
        "\n",
        "```bash\n",
        "pip install 'nexaai[mlx]'\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 4. Select the venv as your Jupyter Notebook kernel\n",
        "\n",
        "- Depending on the editor you are using, the way to change kernel might be different. For Cursor / VS Code, they are located at the top right corner of your code window.\n",
        "- Look for and select the `nexaai-env`, or the custom virtual environment you have created. The kernel should automatically reload in most IDEs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Verification of Kernel\n",
        "\n",
        "Run the following code to ensure you have the right kernel running.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import platform\n",
        "\n",
        "current_ver = sys.version_info\n",
        "arch = platform.machine()\n",
        "\n",
        "if current_ver.major != 3 or current_ver.minor != 10:\n",
        "    print(f\"❌ Error: Python {current_ver.major}.{current_ver.minor} detected\")\n",
        "    print(\"✅ Required: Python 3.10\")\n",
        "    print(\"Please install Python 3.10 and restart the kernel.\")\n",
        "    sys.exit(1)\n",
        "else:\n",
        "    print(\"✅ Python 3.10 ready to proceed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. LLM (Large Language Model) Inference\n",
        "\n",
        "Using -accelerated large language models for text generation and conversation. Llama3.2-3B--Turbo is specifically optimized for .\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import io\n",
        "import os\n",
        "\n",
        "from nexaai.common import GenerationConfig, ModelConfig, ChatMessage\n",
        "from nexaai.llm import LLM\n",
        "\n",
        "\n",
        "def llm_example():\n",
        "    \"\"\"LLM Inference example\"\"\"\n",
        "    print(\"=== LLM Inference Example ===\")\n",
        "\n",
        "    # Model configuration\n",
        "\n",
        "    model_name = \"Qwen/Qwen3-1.7B-GGUF/Qwen3-1.7B-Q8_0.gguf\"\n",
        "\n",
        "    plugin_id = \"cpu_gpu\"\n",
        "    max_tokens = 100\n",
        "    system_message = \"You are a helpful assistant.\"\n",
        "\n",
        "    print(f\"Loading model: {model_name}\")\n",
        "\n",
        "    # Create model instance\n",
        "    m_cfg = ModelConfig()\n",
        "    llm = LLM.from_(name_or_path=model_name, m_cfg=m_cfg, plugin_id=plugin_id)\n",
        "\n",
        "    # Create conversation history\n",
        "    conversation = [ChatMessage(role=\"system\", content=system_message)]\n",
        "\n",
        "    # Example conversations\n",
        "    test_prompts = [\n",
        "        \"What is artificial intelligence?\",\n",
        "        \"Explain the benefits of on-device AI processing.\",\n",
        "        \"How does NPU acceleration work?\"\n",
        "    ]\n",
        "\n",
        "    for i, prompt in enumerate(test_prompts, 1):\n",
        "        print(f\"\\n--- Conversation {i} ---\")\n",
        "        print(f\"User: {prompt}\")\n",
        "\n",
        "        # Add user message\n",
        "        conversation.append(ChatMessage(role=\"user\", content=prompt))\n",
        "\n",
        "        # Apply chat template\n",
        "        formatted_prompt = llm.apply_chat_template(conversation)\n",
        "\n",
        "        # Generate response\n",
        "        print(\"Assistant: \", end=\"\", flush=True)\n",
        "        response_buffer = io.StringIO()\n",
        "\n",
        "        for token in llm.generate_stream(formatted_prompt, g_cfg=GenerationConfig(max_tokens=max_tokens)):\n",
        "            print(token, end=\"\", flush=True)\n",
        "            response_buffer.write(token)\n",
        "\n",
        "        # Get profiling data\n",
        "        profiling_data = llm.get_profiling_data()\n",
        "        if profiling_data:\n",
        "            print(f\"\\nProfiling data: {profiling_data}\")\n",
        "\n",
        "        # Add assistant response to conversation history\n",
        "        conversation.append(ChatMessage(role=\"assistant\", content=response_buffer.getvalue()))\n",
        "        print(\"\\n\" + \"=\" * 50)\n",
        "\n",
        "\n",
        "llm_example()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. VLM (Vision Language Model) Inference\n",
        "\n",
        "Using vision language models for multimodal understanding and generation. OmniNeural-4B supports joint processing of images and text.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import io\n",
        "\n",
        "from nexaai.vlm import VLM\n",
        "from nexaai.common import GenerationConfig, ModelConfig, MultiModalMessage, MultiModalMessageContent\n",
        "\n",
        "\n",
        "def vlm_example():\n",
        "    \"\"\"VLM Inference example\"\"\"\n",
        "    print(\"=== VLM Inference Example ===\")\n",
        "\n",
        "    # Model configuration\n",
        "\n",
        "    model_name = \"ggml-org/gemma-3-4b-it-GGUF/gemma-3-4b-it-Q4_K_M.gguf\"\n",
        "\n",
        "    plugin_id = \"cpu_gpu\"\n",
        "    max_tokens = 100\n",
        "    system_message = \"You are a helpful assistant that can understand images and text.\"\n",
        "    image_path = '/your/image/path'  # Replace with actual image path if available\n",
        "\n",
        "    print(f\"Loading model: {model_name}\")\n",
        "    print(f\"Using plugin: {plugin_id}\")\n",
        "\n",
        "    # Check for image existence\n",
        "    if not (image_path and os.path.exists(image_path)):\n",
        "        print(\n",
        "            f\"\\033[93mWARNING: The specified image_path ('{image_path}') does not exist or was not provided. Multimodal prompts will not include image input.\\033[0m\")\n",
        "\n",
        "    # Create model instance\n",
        "    m_cfg = ModelConfig()\n",
        "    vlm = VLM.from_(name_or_path=model_name, m_cfg=m_cfg, plugin_id=plugin_id)\n",
        "\n",
        "    # Create conversation history\n",
        "    conversation = [MultiModalMessage(role=\"system\",\n",
        "                                      content=[MultiModalMessageContent(type=\"text\", text=system_message)])]\n",
        "\n",
        "    # Example multimodal conversations\n",
        "    test_cases = [\n",
        "        {\n",
        "            \"text\": \"What do you see in this image?\",\n",
        "            \"image_path\": image_path\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    for i, case in enumerate(test_cases, 1):\n",
        "        print(f\"\\n--- Multimodal Conversation {i} ---\")\n",
        "        print(f\"User: {case['text']}\")\n",
        "\n",
        "        # Build message content\n",
        "        contents = [MultiModalMessageContent(type=\"text\", text=case['text'])]\n",
        "\n",
        "        # Add image content if available\n",
        "        if case['image_path'] and os.path.exists(case['image_path']):\n",
        "            contents.append(MultiModalMessageContent(type=\"image\", path=case['image_path']))\n",
        "            print(f\"Including image: {case['image_path']}\")\n",
        "\n",
        "        # Add user message\n",
        "        conversation.append(MultiModalMessage(role=\"user\", content=contents))\n",
        "\n",
        "        # Apply chat template\n",
        "        formatted_prompt = vlm.apply_chat_template(conversation)\n",
        "\n",
        "        # Generate response\n",
        "        print(\"Assistant: \", end=\"\", flush=True)\n",
        "        response_buffer = io.StringIO()\n",
        "\n",
        "        # Prepare image and audio paths\n",
        "        image_paths = [case['image_path']] if case['image_path'] and os.path.exists(case['image_path']) else None\n",
        "        audio_paths = None\n",
        "\n",
        "        for token in vlm.generate_stream(formatted_prompt,\n",
        "                                         g_cfg=GenerationConfig(max_tokens=max_tokens,\n",
        "                                                                image_paths=image_paths,\n",
        "                                                                audio_paths=audio_paths)):\n",
        "            print(token, end=\"\", flush=True)\n",
        "            response_buffer.write(token)\n",
        "\n",
        "        # Get profiling data\n",
        "        profiling_data = vlm.get_profiling_data()\n",
        "        if profiling_data:\n",
        "            print(f\"\\nProfiling data: {profiling_data}\")\n",
        "\n",
        "        # Add assistant response to conversation history\n",
        "        conversation.append(MultiModalMessage(role=\"assistant\",\n",
        "                                              content=[MultiModalMessageContent(type=\"text\", text=response_buffer.getvalue())]))\n",
        "        print(\"\\n\" + \"=\" * 50)\n",
        "\n",
        "\n",
        "vlm_example()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Embedder Inference\n",
        "\n",
        "Using embedding models for text vectorization and similarity computation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Embedder Inference Example ===\n",
            "Loading model: djuna/jina-embeddings-v2-small-en-Q5_K_M-GGUF/jina-embeddings-v2-small-en-q5_k_m.gguf\n",
            "Using plugin: cpu_gpu\n",
            "Batch size: 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "f:\\workspace\\company\\nexaai\\nexa-sdk\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:979: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
            "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Attempting to download nexa.manifest from djuna/jina-embeddings-v2-small-en-Q5_K_M-GGUF...\n",
            "[INFO] nexa.manifest not found in djuna/jina-embeddings-v2-small-en-Q5_K_M-GGUF, will create locally\n",
            "[OK] Successfully created nexa.manifest for djuna/jina-embeddings-v2-small-en-Q5_K_M-GGUF\n",
            "Embedder loaded successfully!\n",
            "Embedding dimension: 512\n",
            "\n",
            "=== Generating Embeddings ===\n",
            "Processing 5 texts...\n",
            "Successfully generated 5 embeddings\n",
            "\n",
            "=== Embedding Details ===\n",
            "\n",
            "Text 1:\n",
            "  Content: On-device AI is a type of AI that is processed on the device itself, rather than in the cloud.\n",
            "  Embedding dimension: 512\n",
            "  First 10 elements: [-0.04828435 -0.06880371 -0.00595966 -0.00048472  0.01435765 -0.02124726\n",
            "  0.04017022  0.06111896  0.00422432  0.00086029]\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Text 2:\n",
            "  Content: Nexa AI allows you to run state-of-the-art AI models locally on CPU, GPU, or NPU.\n",
            "  Embedding dimension: 512\n",
            "  First 10 elements: [-0.03423774 -0.06670449  0.0073319   0.0253497  -0.02921125 -0.01526904\n",
            "  0.03073953  0.06452441 -0.01268294  0.04022756]\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Text 3:\n",
            "  Content: A ragdoll is a breed of cat that is known for its long, flowing hair and gentle personality.\n",
            "  Embedding dimension: 512\n",
            "  First 10 elements: [-0.02059605 -0.03419322 -0.02397553  0.05312475  0.01025576  0.0115067\n",
            " -0.01412297  0.06845784 -0.04020507  0.00317529]\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Text 4:\n",
            "  Content: The capital of France is Paris.\n",
            "  Embedding dimension: 512\n",
            "  First 10 elements: [-0.06762518 -0.04266268  0.01120668  0.02044768 -0.00836239 -0.00759394\n",
            "  0.01564865  0.06303822 -0.04065036  0.0352169 ]\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Text 5:\n",
            "  Content: NPU acceleration provides significant performance improvements for AI workloads.\n",
            "  Embedding dimension: 512\n",
            "  First 10 elements: [-0.02779846 -0.03865884  0.00125065  0.04632435 -0.00093052 -0.01247025\n",
            "  0.00277293  0.07364899  0.01103177  0.03270822]\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "=== Query Processing ===\n",
            "Query: 'what is on device AI'\n",
            "Query embedding dimension: 512\n",
            "\n",
            "=== Similarity Analysis (Inner Product) ===\n",
            "\n",
            "Text 1:\n",
            "  Content: On-device AI is a type of AI that is processed on the device itself, rather than in the cloud.\n",
            "  Inner product with query: 0.939030\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Text 2:\n",
            "  Content: Nexa AI allows you to run state-of-the-art AI models locally on CPU, GPU, or NPU.\n",
            "  Inner product with query: 0.819101\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Text 3:\n",
            "  Content: A ragdoll is a breed of cat that is known for its long, flowing hair and gentle personality.\n",
            "  Inner product with query: 0.638035\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Text 4:\n",
            "  Content: The capital of France is Paris.\n",
            "  Inner product with query: 0.628324\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Text 5:\n",
            "  Content: NPU acceleration provides significant performance improvements for AI workloads.\n",
            "  Inner product with query: 0.769383\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "=== Similarity Ranking Results ===\n",
            "Rank 1: [0.939030] On-device AI is a type of AI that is processed on the device itself, rather than in the cloud.\n",
            "Rank 2: [0.819101] Nexa AI allows you to run state-of-the-art AI models locally on CPU, GPU, or NPU.\n",
            "Rank 3: [0.769383] NPU acceleration provides significant performance improvements for AI workloads.\n",
            "Rank 4: [0.638035] A ragdoll is a breed of cat that is known for its long, flowing hair and gentle personality.\n",
            "Rank 5: [0.628324] The capital of France is Paris.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from nexaai.embedder import Embedder, EmbeddingConfig\n",
        "\n",
        "# Embedder Inference Example\n",
        "\n",
        "def embedder_example():\n",
        "    \"\"\"Embedder Inference example\"\"\"\n",
        "    print(\"=== Embedder Inference Example ===\")\n",
        "\n",
        "    # Model configuration\n",
        "\n",
        "    model_name = \"djuna/jina-embeddings-v2-small-en-Q5_K_M-GGUF/jina-embeddings-v2-small-en-q5_k_m.gguf\"\n",
        "\n",
        "    plugin_id = \"cpu_gpu\"\n",
        "    batch_size = 2\n",
        "\n",
        "    print(f\"Loading model: {model_name}\")\n",
        "    print(f\"Using plugin: {plugin_id}\")\n",
        "    print(f\"Batch size: {batch_size}\")\n",
        "\n",
        "    # Create embedder instance\n",
        "    embedder = Embedder.from_(name_or_path=model_name, plugin_id=plugin_id)\n",
        "    print('Embedder loaded successfully!')\n",
        "\n",
        "    # Get embedding dimension\n",
        "    dim = embedder.get_embedding_dim()\n",
        "    print(f\"Embedding dimension: {dim}\")\n",
        "\n",
        "    # Example texts\n",
        "    texts = [\n",
        "        \"On-device AI is a type of AI that is processed on the device itself, rather than in the cloud.\",\n",
        "        \"Nexa AI allows you to run state-of-the-art AI models locally on CPU, GPU, or NPU.\",\n",
        "        \"A ragdoll is a breed of cat that is known for its long, flowing hair and gentle personality.\",\n",
        "        \"The capital of France is Paris.\",\n",
        "        \"NPU acceleration provides significant performance improvements for AI workloads.\"\n",
        "    ]\n",
        "\n",
        "    query = \"what is on device AI\"\n",
        "\n",
        "    print(f\"\\n=== Generating Embeddings ===\")\n",
        "    print(f\"Processing {len(texts)} texts...\")\n",
        "\n",
        "    # Generate embeddings\n",
        "    embeddings = embedder.generate(\n",
        "        texts=texts,\n",
        "        config=EmbeddingConfig(batch_size=batch_size)\n",
        "    )\n",
        "\n",
        "    print(f\"Successfully generated {len(embeddings)} embeddings\")\n",
        "\n",
        "    # Display embedding information\n",
        "    print(f\"\\n=== Embedding Details ===\")\n",
        "    for i, (text, embedding) in enumerate(zip(texts, embeddings)):\n",
        "        print(f\"\\nText {i + 1}:\")\n",
        "        print(f\"  Content: {text}\")\n",
        "        print(f\"  Embedding dimension: {len(embedding)}\")\n",
        "        print(f\"  First 10 elements: {embedding[:10]}\")\n",
        "        print(\"-\" * 70)\n",
        "\n",
        "    # Query processing\n",
        "    print(f\"\\n=== Query Processing ===\")\n",
        "    print(f\"Query: '{query}'\")\n",
        "\n",
        "    query_embedding = embedder.generate(\n",
        "        texts=[query],\n",
        "        config=EmbeddingConfig(batch_size=1)\n",
        "    )[0]\n",
        "\n",
        "    print(f\"Query embedding dimension: {len(query_embedding)}\")\n",
        "\n",
        "    # Similarity analysis\n",
        "    print(f\"\\n=== Similarity Analysis (Inner Product) ===\")\n",
        "    similarities = []\n",
        "\n",
        "    for i, (text, embedding) in enumerate(zip(texts, embeddings)):\n",
        "        query_vec = np.array(query_embedding)\n",
        "        text_vec = np.array(embedding)\n",
        "        inner_product = np.dot(query_vec, text_vec)\n",
        "        similarities.append((i, text, inner_product))\n",
        "\n",
        "        print(f\"\\nText {i + 1}:\")\n",
        "        print(f\"  Content: {text}\")\n",
        "        print(f\"  Inner product with query: {inner_product:.6f}\")\n",
        "        print(\"-\" * 70)\n",
        "\n",
        "    # Sort and display most similar texts\n",
        "    similarities.sort(key=lambda x: x[2], reverse=True)\n",
        "\n",
        "    print(f\"\\n=== Similarity Ranking Results ===\")\n",
        "    for rank, (idx, text, score) in enumerate(similarities, 1):\n",
        "        print(f\"Rank {rank}: [{score:.6f}] {text}\")\n",
        "\n",
        "    return embeddings, query_embedding, similarities\n",
        "\n",
        "\n",
        "embeddings, query_emb, similarities = embedder_example()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Reranker Inference\n",
        "\n",
        "Using reranking models for document reranking. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Reranker Inference Example ===\n",
            "Loading model: pqnet/bge-reranker-v2-m3-Q8_0-GGUF/a\n",
            "Using plugin: cpu_gpu\n",
            "Batch size: 4\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Could not load model from 'name_or_path=pqnet/bge-reranker-v2-m3-Q8_0-GGUF/a': File 'a' not found in repository 'pqnet/bge-reranker-v2-m3-Q8_0-GGUF'. Available files: ['.gitattributes', 'README.md', 'bge-reranker-v2-m3-q8_0.gguf']",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "File \u001b[1;32mf:\\workspace\\company\\nexaai\\nexa-sdk\\.venv\\lib\\site-packages\\nexaai\\utils\\model_manager.py:1437\u001b[0m, in \u001b[0;36m_download_model_if_needed\u001b[1;34m(model_path, param_name, progress_callback, token, is_mmproj, **kwargs)\u001b[0m\n\u001b[0;32m   1436\u001b[0m \u001b[38;5;66;03m# Download the model\u001b[39;00m\n\u001b[1;32m-> 1437\u001b[0m downloaded_path \u001b[38;5;241m=\u001b[39m download_from_huggingface(\n\u001b[0;32m   1438\u001b[0m     repo_id\u001b[38;5;241m=\u001b[39mrepo_id,\n\u001b[0;32m   1439\u001b[0m     file_name\u001b[38;5;241m=\u001b[39mfile_name,\n\u001b[0;32m   1440\u001b[0m     local_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# Use default cache directory\u001b[39;00m\n\u001b[0;32m   1441\u001b[0m     enable_transfer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   1442\u001b[0m     progress_callback\u001b[38;5;241m=\u001b[39mprogress_callback,\n\u001b[0;32m   1443\u001b[0m     show_progress\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   1444\u001b[0m     token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   1445\u001b[0m     is_mmproj\u001b[38;5;241m=\u001b[39mis_mmproj,\n\u001b[0;32m   1446\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m   1447\u001b[0m )\n\u001b[0;32m   1449\u001b[0m \u001b[38;5;66;03m# Extract model info from the downloaded manifest\u001b[39;00m\n",
            "File \u001b[1;32mf:\\workspace\\company\\nexaai\\nexa-sdk\\.venv\\lib\\site-packages\\nexaai\\utils\\model_manager.py:1323\u001b[0m, in \u001b[0;36mdownload_from_huggingface\u001b[1;34m(repo_id, file_name, local_dir, enable_transfer, progress_callback, show_progress, token, custom_endpoint, force_download, is_mmproj, **kwargs)\u001b[0m\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;66;03m# Use the downloader to perform the download\u001b[39;00m\n\u001b[1;32m-> 1323\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m downloader\u001b[38;5;241m.\u001b[39mdownload(\n\u001b[0;32m   1324\u001b[0m     repo_id\u001b[38;5;241m=\u001b[39mrepo_id,\n\u001b[0;32m   1325\u001b[0m     file_name\u001b[38;5;241m=\u001b[39mfile_name,\n\u001b[0;32m   1326\u001b[0m     local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[0;32m   1327\u001b[0m     progress_callback\u001b[38;5;241m=\u001b[39mprogress_callback,\n\u001b[0;32m   1328\u001b[0m     show_progress\u001b[38;5;241m=\u001b[39mshow_progress,\n\u001b[0;32m   1329\u001b[0m     force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[0;32m   1330\u001b[0m     is_mmproj\u001b[38;5;241m=\u001b[39mis_mmproj,\n\u001b[0;32m   1331\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m   1332\u001b[0m )\n",
            "File \u001b[1;32mf:\\workspace\\company\\nexaai\\nexa-sdk\\.venv\\lib\\site-packages\\nexaai\\utils\\model_manager.py:1192\u001b[0m, in \u001b[0;36mHuggingFaceDownloader.download\u001b[1;34m(self, repo_id, file_name, local_dir, progress_callback, show_progress, force_download, is_mmproj, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(file_name, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   1191\u001b[0m     \u001b[38;5;66;03m# Download specific single file\u001b[39;00m\n\u001b[1;32m-> 1192\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_file_exists_in_repo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_single_file(\n\u001b[0;32m   1194\u001b[0m         repo_id, file_name, local_dir, progress_tracker, force_download, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m   1195\u001b[0m     )\n",
            "File \u001b[1;32mf:\\workspace\\company\\nexaai\\nexa-sdk\\.venv\\lib\\site-packages\\nexaai\\utils\\model_manager.py:807\u001b[0m, in \u001b[0;36mHuggingFaceDownloader._validate_file_exists_in_repo\u001b[1;34m(self, file_name, info, repo_id, progress_tracker)\u001b[0m\n\u001b[0;32m    806\u001b[0m     progress_tracker\u001b[38;5;241m.\u001b[39mstop_tracking()\n\u001b[1;32m--> 807\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_msg)\n",
            "\u001b[1;31mValueError\u001b[0m: File 'a' not found in repository 'pqnet/bge-reranker-v2-m3-Q8_0-GGUF'. Available files: ['.gitattributes', 'README.md', 'bge-reranker-v2-m3-q8_0.gguf']",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[3], line 78\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m80\u001b[39m)\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m reranker\n\u001b[1;32m---> 78\u001b[0m reranker \u001b[38;5;241m=\u001b[39m \u001b[43mreranker_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[3], line 21\u001b[0m, in \u001b[0;36mreranker_example\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Create reranker instance\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m reranker \u001b[38;5;241m=\u001b[39m \u001b[43mReranker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplugin_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplugin_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReranker loaded successfully!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Example queries and documents\u001b[39;00m\n",
            "File \u001b[1;32mf:\\workspace\\company\\nexaai\\nexa-sdk\\.venv\\lib\\site-packages\\nexaai\\utils\\model_manager.py:1546\u001b[0m, in \u001b[0;36mauto_download_model.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1543\u001b[0m             kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m model_name\n\u001b[0;32m   1545\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1546\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e  \u001b[38;5;66;03m# Re-raise the error from _download_model_if_needed\u001b[39;00m\n\u001b[0;32m   1548\u001b[0m \u001b[38;5;66;03m# Download mmproj_path if needed\u001b[39;00m\n\u001b[0;32m   1549\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmproj_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[1;32mf:\\workspace\\company\\nexaai\\nexa-sdk\\.venv\\lib\\site-packages\\nexaai\\utils\\model_manager.py:1529\u001b[0m, in \u001b[0;36mauto_download_model.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1527\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name_or_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1528\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1529\u001b[0m         downloaded_name_path, model_name, plugin_id \u001b[38;5;241m=\u001b[39m _download_model_if_needed(\n\u001b[0;32m   1530\u001b[0m             name_or_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname_or_path\u001b[39m\u001b[38;5;124m'\u001b[39m, progress_callback, token, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m   1531\u001b[0m         )\n\u001b[0;32m   1533\u001b[0m         \u001b[38;5;66;03m# Replace name_or_path with downloaded path\u001b[39;00m\n\u001b[0;32m   1534\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m is_name_positional:\n",
            "File \u001b[1;32mf:\\workspace\\company\\nexaai\\nexa-sdk\\.venv\\lib\\site-packages\\nexaai\\utils\\model_manager.py:1462\u001b[0m, in \u001b[0;36m_download_model_if_needed\u001b[1;34m(model_path, param_name, progress_callback, token, is_mmproj, **kwargs)\u001b[0m\n\u001b[0;32m   1458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m downloaded_path, model_name, plugin_id\n\u001b[0;32m   1460\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1461\u001b[0m     \u001b[38;5;66;03m# Only handle download-related errors\u001b[39;00m\n\u001b[1;32m-> 1462\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not load model from \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mRuntimeError\u001b[0m: Could not load model from 'name_or_path=pqnet/bge-reranker-v2-m3-Q8_0-GGUF/a': File 'a' not found in repository 'pqnet/bge-reranker-v2-m3-Q8_0-GGUF'. Available files: ['.gitattributes', 'README.md', 'bge-reranker-v2-m3-q8_0.gguf']"
          ]
        }
      ],
      "source": [
        "from nexaai.rerank import Reranker, RerankConfig\n",
        "\n",
        "\n",
        "def reranker_example():\n",
        "    \"\"\"Reranker Inference example\"\"\"\n",
        "    print(\"=== Reranker Inference Example ===\")\n",
        "\n",
        "    # Model configuration\n",
        "\n",
        "    # Use huggingface repo ID\n",
        "    model_name = \"pqnet/bge-reranker-v2-m3-Q8_0-GGUF/bge-reranker-v2-m3-q8_0.gguf\"\n",
        "\n",
        "    plugin_id = \"cpu_gpu\"\n",
        "    batch_size = 4\n",
        "\n",
        "    print(f\"Loading model: {model_name}\")\n",
        "    print(f\"Using plugin: {plugin_id}\")\n",
        "    print(f\"Batch size: {batch_size}\")\n",
        "\n",
        "    # Create reranker instance\n",
        "    reranker = Reranker.from_(name_or_path=model_name, plugin_id=plugin_id)\n",
        "    print('Reranker loaded successfully!')\n",
        "\n",
        "    # Example queries and documents\n",
        "    queries = [\n",
        "        \"Where is on-device AI?\",\n",
        "        \"What is NPU acceleration?\",\n",
        "        \"How does machine learning work?\",\n",
        "        \"Tell me about computer vision\"\n",
        "    ]\n",
        "\n",
        "    documents = [\n",
        "        \"On-device AI is a type of AI that is processed on the device itself, rather than in the cloud.\",\n",
        "        \"NPU acceleration provides significant performance improvements for AI workloads on specialized hardware.\",\n",
        "        \"Edge computing brings computation and data storage closer to the sources of data.\",\n",
        "        \"A ragdoll is a breed of cat that is known for its long, flowing hair and gentle personality.\",\n",
        "        \"The capital of France is Paris, a beautiful city known for its art and culture.\",\n",
        "        \"Machine learning is a subset of artificial intelligence that enables computers to learn without being explicitly programmed.\",\n",
        "        \"Computer vision is a field of artificial intelligence that trains computers to interpret and understand visual information.\",\n",
        "        \"Deep learning uses neural networks with multiple layers to model and understand complex patterns in data.\"\n",
        "    ]\n",
        "\n",
        "    print(f\"\\n=== Document Reranking Test ===\")\n",
        "    print(f\"Number of documents: {len(documents)}\")\n",
        "\n",
        "    # Rerank for each query\n",
        "    for i, query in enumerate(queries, 1):\n",
        "        print(f\"\\n--- Query {i} ---\")\n",
        "        print(f\"Query: '{query}'\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        # Perform reranking\n",
        "        scores = reranker.rerank(\n",
        "            query=query,\n",
        "            documents=documents,\n",
        "            config=RerankConfig(batch_size=batch_size)\n",
        "        )\n",
        "\n",
        "        # Create (document, score) pairs and sort\n",
        "        doc_scores = list(zip(documents, scores))\n",
        "        doc_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Display ranking results\n",
        "        print(\"Reranking results:\")\n",
        "        for rank, (doc, score) in enumerate(doc_scores, 1):\n",
        "            print(f\"  {rank:2d}. [{score:.4f}] {doc}\")\n",
        "\n",
        "        # Display most relevant documents\n",
        "        print(f\"\\nMost relevant documents (top 3):\")\n",
        "        for rank, (doc, score) in enumerate(doc_scores[:3], 1):\n",
        "            print(f\"  {rank}. {doc}\")\n",
        "\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "    return reranker\n",
        "\n",
        "\n",
        "reranker = reranker_example()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
