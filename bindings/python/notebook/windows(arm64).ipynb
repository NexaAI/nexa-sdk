{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NexaAI NPU Inference Examples\n",
        "\n",
        "This notebook demonstrates how to use the NexaAI SDK for various AI inference tasks on NPU devices, including:\n",
        "\n",
        "- **LLM (Large Language Model)**: Text generation and conversation\n",
        "- **VLM (Vision Language Model)**: Multimodal understanding and generation\n",
        "- **Embedder**: Text vectorization and similarity computation\n",
        "- **Reranker**: Document reranking\n",
        "- **ASR (Automatic Speech Recognition)**: Speech-to-text transcription\n",
        "- **CV (Computer Vision)**: OCR/text recognition\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "### 1. Install the correct Python version\n",
        "\n",
        "If you prefer, we also offer a video tutorial for the installation. Check it out [here](https://www.youtube.com/watch?v=ziXKPRX0Ufo).\n",
        "\n",
        "NexaAI requires **Python 3.11 â€“ 3.13 (ARM64 build)** on Windows ARM.\n",
        "Please **download and install the official ARM64 Python** from the [python-3.11.1-arm64.exe](https://www.python.org/ftp/python/3.11.1/python-3.11.1-arm64.exe). Make sure you read the instructions below carefully before proceeding.\n",
        "\n",
        "> â— **IMPORTANT**: Make sure you select \"Add python.exe to PATH\" on the first screen of the installation wizard.\n",
        "\n",
        "> ðŸ›‘ Make sure you restart the terminal or your IDE after installation.\n",
        "\n",
        "> âš ï¸ Do **not** use Conda or x86 builds â€” they are incompatible with native ARM64 binaries. If you are in a conda environment, run `conda deactivate` first.\n",
        "\n",
        "Verify the installation:\n",
        "\n",
        "In case your environment path gets overriden by some environment manager, we recommend you to run the following commands to restore PATH variable from system settings.\n",
        "```powershell\n",
        "$systemPath = [Environment]::GetEnvironmentVariable('Path', 'Machine')\n",
        "$userPath   = [Environment]::GetEnvironmentVariable('Path', 'User')\n",
        "$env:Path   = \"$userPath;$systemPath\"\n",
        "```\n",
        "\n",
        "Then verify your python executable has the correct architecture and version (3.11 - 3.13)\n",
        "```sh\n",
        "python -c \"import sys, platform; print(f'Python version: {sys.version}')\"\n",
        "```\n",
        "\n",
        "Your output should look like:\n",
        "\n",
        "> Python version: 3.11.0 (main, Oct 24 2022, 18:15:22) [MSC v.1933 64 bit (ARM64)]\n",
        "\n",
        "Expected output must contain version `3.11.0` and architecture `ARM64`.\n",
        "\n",
        "If it does show `AMD64` or incorrect version, try the following:\n",
        "\n",
        "- (If you have conda installed) Run `conda deactivate` to deactivate the current conda environment.\n",
        "- (If your `python` executable points to the x86 version) You may need to make the ARM64 Python come before the x86 Python in your PATH.\n",
        "  - Hit the `Win` key, and type `env`, and hit Enter to select `Edit the system environment variables` setting.\n",
        "  - Click on `Environment Variables...` button.\n",
        "  - Select `Path` and click `Edit...`.\n",
        "  - Find your ARM64 Python installation path, and move it to the top of the list.\n",
        "  - Hit `OK` for several times to close all the dialogs and save the changes.\n",
        "- (If you forgot to select \"Add python.exe to PATH\" on the first screen of the installation wizard)\n",
        "  - Run the installation wizard again, follow the instructions to remove the current installation, and then reinstall from the Wizard. Make sure to select \"Add python.exe to PATH\" this time.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Create and activate a virtual environment\n",
        "\n",
        "`cd` to the current project root directory `cd path/to/nexa-sdk`.\n",
        "\n",
        "```sh\n",
        "python -m venv nexaai-env\n",
        "nexaai-env\\Scripts\\activate\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Install the NexaAI SDK\n",
        "\n",
        "```bash\n",
        "pip install nexaai -v\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 4. Select the venv as your Jupyter Notebook kernel\n",
        "\n",
        "- Depending the editor you are using, the way to change kernel might be different. For Cursor / VS Code, they are located at the top right corner of you code window.\n",
        "- Look for and select the `nexaai-env`, or the custom virtual environment you have created. The kernel should automatically reload in most IDEs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Verification of Kernel\n",
        "\n",
        "Run the following code to ensure you have the right kernel running.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import platform\n",
        "\n",
        "# ANSI color codes\n",
        "RED = \"\\033[91m\"\n",
        "GREEN = \"\\033[92m\"\n",
        "YELLOW = \"\\033[93m\"\n",
        "BOLD = \"\\033[1m\"\n",
        "RESET = \"\\033[0m\"\n",
        "\n",
        "min_ver = (3, 11)\n",
        "max_ver = (3, 13)\n",
        "current_ver = sys.version_info\n",
        "arch = platform.machine()\n",
        "\n",
        "if not (min_ver <= (current_ver.major, current_ver.minor) < max_ver) or arch.lower() != \"arm64\":\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(f\"{BOLD}{RED}WARNING: Your Python version or architecture is not compatible.{RESET}\")\n",
        "    print(f\"Detected version: {current_ver.major}.{current_ver.minor}, architecture: {arch}\")\n",
        "    print(f\"{YELLOW}Required: Python 3.11 - 3.13 & architecture 'arm64'.{RESET}\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"{RED}DO NOT continue to the following code!{RESET}\\n\")\n",
        "    print(\"To install arm64 Python:\")\n",
        "    print(\"  - Download Python 3.11-3.13 for arm64 from https://www.python.org/downloads/\")\n",
        "    print(\"  - Install and verify by running: python3 --version and python3 -c 'import platform; print(platform.machine())'\")\n",
        "    print(\"  - Launch Jupyter and make sure to select the arm64 Python kernel in 'Kernel > Change kernel'.\")\n",
        "    sys.exit(1)\n",
        "else:\n",
        "    print(f\"{GREEN}[VERIFICATION PASSED] Python version and architecture are correct. You may continue to the following sections.{RESET}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Authentication Setup\n",
        "\n",
        "Before running any examples, you need to set up your NexaAI authentication token.\n",
        "\n",
        "### Set Token in Code\n",
        "\n",
        "Replace `\"YOUR_NEXA_TOKEN_HERE\"` with your actual NexaAI token from [https://sdk.nexa.ai/](https://sdk.nexa.ai/):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Replace \"YOUR_NEXA_TOKEN_HERE\" with your actual token from https://sdk.nexa.ai/\n",
        "os.environ[\"NEXA_TOKEN\"] = \"YOUR_NEXA_TOKEN_HERE\"\n",
        "# Suppress HF warnings\n",
        "os.environ[\"HF_HUB_VERBOSITY\"] = \"error\"\n",
        "\n",
        "assert os.environ.get(\"NEXA_TOKEN\", \"\").startswith(\n",
        "    \"key/\"), \"ERROR: NEXA_TOKEN must start with 'key/'. Please check your token.\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. LLM (Large Language Model) NPU Inference\n",
        "\n",
        "Using NPU-accelerated large language models for text generation and conversation. Llama3.2-3B-NPU-Turbo is specifically optimized for NPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import io\n",
        "import os\n",
        "\n",
        "from nexaai import LLM, GenerationConfig, ModelConfig, LlmChatMessage\n",
        "\n",
        "\n",
        "def llm_npu_example():\n",
        "    \"\"\"LLM NPU inference example\"\"\"\n",
        "    print(\"=== LLM NPU Inference Example ===\")\n",
        "\n",
        "    # Model configuration\n",
        "\n",
        "    # Use huggingface Repo ID\n",
        "    model_name = \"NexaAI/Llama3.2-3B-NPU-Turbo\"\n",
        "    # Alternatively, use local path\n",
        "    # model_name = os.path.expanduser(r\"~\\.cache\\nexa.ai\\nexa_sdk\\models\\NexaAI\\Llama3.2-3B-NPU-Turbo\\weights-1-3.nexa\")\n",
        "\n",
        "    plugin_id = \"npu\"\n",
        "    max_tokens = 100\n",
        "    system_message = \"You are a helpful assistant.\"\n",
        "\n",
        "    print(f\"Loading model: {model_name}\")\n",
        "    print(f\"Using plugin: {plugin_id}\")\n",
        "\n",
        "    # Create model instance\n",
        "    config = ModelConfig()\n",
        "    llm = LLM.from_(model=model_name, plugin_id=plugin_id, config=config)\n",
        "\n",
        "    # Create conversation history\n",
        "    conversation = [LlmChatMessage(role=\"system\", content=system_message)]\n",
        "\n",
        "    # Example conversations\n",
        "    test_prompts = [\n",
        "        \"What is artificial intelligence?\",\n",
        "        \"Explain the benefits of on-device AI processing.\",\n",
        "        \"How does NPU acceleration work?\"\n",
        "    ]\n",
        "\n",
        "    for i, prompt in enumerate(test_prompts, 1):\n",
        "        print(f\"\\n--- Conversation {i} ---\")\n",
        "        print(f\"User: {prompt}\")\n",
        "\n",
        "        # Add user message\n",
        "        conversation.append(LlmChatMessage(role=\"user\", content=prompt))\n",
        "\n",
        "        # Apply chat template\n",
        "        formatted_prompt = llm.apply_chat_template(conversation)\n",
        "\n",
        "        # Generate response\n",
        "        print(\"Assistant: \", end=\"\", flush=True)\n",
        "        response_buffer = io.StringIO()\n",
        "\n",
        "        gen = llm.generate_stream(formatted_prompt, GenerationConfig(max_tokens=max_tokens))\n",
        "        result = None\n",
        "        try:\n",
        "            while True:\n",
        "                token = next(gen)\n",
        "                print(token, end=\"\", flush=True)\n",
        "                response_buffer.write(token)\n",
        "        except StopIteration as e:\n",
        "            result = e.value\n",
        "\n",
        "        # Get profiling data\n",
        "        if result and hasattr(result, \"profile_data\") and result.profile_data:\n",
        "            print(f\"\\n{result.profile_data}\")\n",
        "\n",
        "        # Add assistant response to conversation history\n",
        "        conversation.append(LlmChatMessage(role=\"assistant\", content=response_buffer.getvalue()))\n",
        "        print(\"\\n\" + \"=\" * 50)\n",
        "\n",
        "\n",
        "llm_npu_example()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. VLM (Vision Language Model) NPU Inference\n",
        "\n",
        "Using NPU-accelerated vision language models for multimodal understanding and generation. OmniNeural-4B supports joint processing of images and text.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import io\n",
        "\n",
        "from nexaai import (\n",
        "    GenerationConfig,\n",
        "    ModelConfig,\n",
        "    VlmChatMessage,\n",
        "    VlmContent,\n",
        ")\n",
        "from nexaai.vlm import VLM\n",
        "\n",
        "\n",
        "def vlm_npu_example():\n",
        "    \"\"\"VLM NPU inference example\"\"\"\n",
        "    print(\"=== VLM NPU Inference Example ===\")\n",
        "\n",
        "    # Model configuration\n",
        "\n",
        "    # Use huggingface repo ID\n",
        "    model_name = \"NexaAI/OmniNeural-4B\"\n",
        "    # Alternatively, use local path\n",
        "    # model_name = os.path.expanduser(r\"~\\.cache\\nexa.ai\\nexa_sdk\\models\\NexaAI\\OmniNeural-4B\\weights-1-8.nexa\")\n",
        "\n",
        "    plugin_id = \"npu\"\n",
        "    max_tokens = 100\n",
        "    system_message = \"You are a helpful assistant that can understand images and text.\"\n",
        "    image_path = '/your/image/path'  # Replace with actual image path if available\n",
        "\n",
        "    print(f\"Loading model: {model_name}\")\n",
        "    print(f\"Using plugin: {plugin_id}\")\n",
        "\n",
        "    # Check for image existence\n",
        "    if not (image_path and os.path.exists(image_path)):\n",
        "        print(\n",
        "            f\"\\033[93mWARNING: The specified image_path ('{image_path}') does not exist or was not provided. Multimodal prompts will not include image input.\\033[0m\")\n",
        "\n",
        "    # Create model instance\n",
        "    config = ModelConfig()\n",
        "    vlm = VLM.from_(model=model_name, config=config, plugin_id=plugin_id)\n",
        "\n",
        "    # Create conversation history\n",
        "    conversation = [\n",
        "        VlmChatMessage(\n",
        "            role=\"system\",\n",
        "            contents=[VlmContent(type=\"text\", text=system_message)]\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    # Example multimodal conversations\n",
        "    test_cases = [\n",
        "        {\n",
        "            \"text\": \"What do you see in this image?\",\n",
        "            \"image_path\": image_path\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    for i, case in enumerate(test_cases, 1):\n",
        "        print(f\"\\n--- Multimodal Conversation {i} ---\")\n",
        "        print(f\"User: {case['text']}\")\n",
        "\n",
        "        # Build message content\n",
        "        contents = []\n",
        "        if case['text']:\n",
        "            contents.append(VlmContent(type=\"text\", text=case['text']))\n",
        "\n",
        "        # Add image content if available\n",
        "        if case['image_path'] and os.path.exists(case['image_path']):\n",
        "            contents.append(VlmContent(type=\"image\", text=case['image_path']))\n",
        "            print(f\"Including image: {case['image_path']}\")\n",
        "\n",
        "        # Add user message\n",
        "        conversation.append(VlmChatMessage(role=\"user\", contents=contents))\n",
        "\n",
        "        # Apply chat template\n",
        "        formatted_prompt = vlm.apply_chat_template(conversation)\n",
        "\n",
        "        # Generate response\n",
        "        print(\"Assistant: \", end=\"\", flush=True)\n",
        "        response_buffer = io.StringIO()\n",
        "\n",
        "        # Prepare image and audio paths\n",
        "        image_paths = [case['image_path']] if case['image_path'] and os.path.exists(case['image_path']) else None\n",
        "        audio_paths = None\n",
        "\n",
        "        gen = vlm.generate_stream(\n",
        "            formatted_prompt,\n",
        "            config=GenerationConfig(\n",
        "                max_tokens=max_tokens,\n",
        "                image_paths=image_paths,\n",
        "                audio_paths=audio_paths\n",
        "            )\n",
        "        )\n",
        "        result = None\n",
        "        try:\n",
        "            while True:\n",
        "                token = next(gen)\n",
        "                print(token, end=\"\", flush=True)\n",
        "                response_buffer.write(token)\n",
        "        except StopIteration as e:\n",
        "            result = e.value\n",
        "\n",
        "        # Get profiling data\n",
        "        if result and hasattr(result, \"profile_data\") and result.profile_data:\n",
        "            print(f\"\\n{result.profile_data}\")\n",
        "\n",
        "        # Add assistant response to conversation history\n",
        "        conversation.append(\n",
        "            VlmChatMessage(\n",
        "                role=\"assistant\",\n",
        "                contents=[\n",
        "                    VlmContent(type=\"text\", text=response_buffer.getvalue())\n",
        "                ]\n",
        "            )\n",
        "        )\n",
        "        print(\"\\n\" + \"=\" * 50)\n",
        "\n",
        "\n",
        "vlm_npu_example()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Embedder NPU Inference\n",
        "\n",
        "Using NPU-accelerated embedding models for text vectorization and similarity computation. embeddinggemma-300m-npu is a lightweight embedding model specifically optimized for NPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from nexaai.embedding import Embedder\n",
        "\n",
        "\n",
        "def embedder_npu_example():\n",
        "    \"\"\"Embedder NPU inference example\"\"\"\n",
        "    print(\"=== Embedder NPU Inference Example ===\")\n",
        "\n",
        "    # Model configuration\n",
        "\n",
        "    # Use huggingface repo ID\n",
        "    model_name = \"NexaAI/embeddinggemma-300m-npu\"\n",
        "    # Alternatively, use local path\n",
        "    # model_name = os.path.expanduser(r\"~\\.cache\\nexa.ai\\nexa_sdk\\models\\NexaAI\\embeddinggemma-300m-npu\\weights-1-2.nexa\")\n",
        "\n",
        "    plugin_id = \"npu\"\n",
        "    batch_size = 2\n",
        "\n",
        "    print(f\"Loading model: {model_name}\")\n",
        "    print(f\"Using plugin: {plugin_id}\")\n",
        "    print(f\"Batch size: {batch_size}\")\n",
        "\n",
        "    # Create embedder instance\n",
        "    embedder = Embedder.from_(model=model_name, plugin_id=plugin_id)\n",
        "    print('Embedder loaded successfully!')\n",
        "\n",
        "    # Get embedding dimension\n",
        "    dim = embedder.embedding_dim()\n",
        "    print(f\"Dimension: {dim}\")\n",
        "\n",
        "    # Example texts\n",
        "    texts = [\n",
        "        \"On-device AI is a type of AI that is processed on the device itself, rather than in the cloud.\",\n",
        "        \"Nexa AI allows you to run state-of-the-art AI models locally on CPU, GPU, or NPU â€” from instant use cases to production deployments.\",\n",
        "        \"A ragdoll is a breed of cat that is known for its long, flowing hair and gentle personality.\",\n",
        "        \"The capital of France is Paris.\",\n",
        "        \"NPU acceleration provides significant performance improvements for AI workloads.\"\n",
        "    ]\n",
        "\n",
        "    query = \"what is on device AI\"\n",
        "\n",
        "    print(f\"\\n=== Generating Embeddings ===\")\n",
        "    print(f\"Processing {len(texts)} texts...\")\n",
        "\n",
        "    # Generate embeddings\n",
        "    result = embedder.embed(\n",
        "        texts=texts,\n",
        "        batch_size=batch_size,\n",
        "    )\n",
        "    embeddings = result.embeddings\n",
        "\n",
        "    print(f\"Successfully generated {len(embeddings)} embeddings\")\n",
        "\n",
        "    # Display embedding information\n",
        "    print(f\"\\n=== Embedding Details ===\")\n",
        "    for i, (text, embedding) in enumerate(zip(texts, embeddings)):\n",
        "        print(f\"\\nText {i + 1}:\")\n",
        "        print(f\"  Content: {text}\")\n",
        "        print(f\"  Embedding shape: {len(embedding)} dimensions\")\n",
        "        print(f\"  First 10 elements: {embedding[:10]}\")\n",
        "        print(\"-\" * 70)\n",
        "\n",
        "    # Query processing\n",
        "    print(f\"\\n=== Query Processing ===\")\n",
        "    print(f\"Query: '{query}'\")\n",
        "\n",
        "    query_result = embedder.embed(\n",
        "        texts=[query],\n",
        "        batch_size=1,\n",
        "    )\n",
        "    query_embedding = query_result.embeddings[0]\n",
        "\n",
        "    print(f\"Query embedding shape: {len(query_embedding)} dimensions\")\n",
        "\n",
        "    # Similarity analysis\n",
        "    print(f\"\\n=== Similarity Analysis (Inner Product) ===\")\n",
        "    similarities = []\n",
        "\n",
        "    for i, (text, embedding) in enumerate(zip(texts, embeddings)):\n",
        "        inner_product = sum(a * b for a, b in zip(query_embedding, embedding))\n",
        "        similarities.append((i, text, inner_product))\n",
        "\n",
        "        print(f\"\\nText {i + 1}:\")\n",
        "        print(f\"  Content: {text}\")\n",
        "        print(f\"  Inner product with query: {inner_product:.6f}\")\n",
        "        print(\"-\" * 70)\n",
        "\n",
        "    # Sort and display most similar texts\n",
        "    similarities.sort(key=lambda x: x[2], reverse=True)\n",
        "\n",
        "    print(f\"\\n=== Similarity Ranking Results ===\")\n",
        "    for rank, (idx, text, score) in enumerate(similarities, 1):\n",
        "        print(f\"Rank {rank}: [{score:.6f}] {text}\")\n",
        "\n",
        "    return embeddings, query_embedding, similarities\n",
        "\n",
        "\n",
        "embeddings, query_emb, similarities = embedder_npu_example()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. ASR (Automatic Speech Recognition) NPU Inference\n",
        "\n",
        "Using NPU-accelerated speech recognition models for speech-to-text transcription. parakeet-npu provides high-quality speech recognition with NPU acceleration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "from nexaai.asr import ASR\n",
        "\n",
        "\n",
        "def asr_npu_example():\n",
        "    \"\"\"ASR NPU inference example\"\"\"\n",
        "    print(\"=== ASR NPU Inference Example ===\")\n",
        "\n",
        "    # Model configuration\n",
        "\n",
        "    # Use huggingface Repo ID\n",
        "    model_name = \"NexaAI/parakeet-npu\"\n",
        "    # Alternatively, use local path\n",
        "    # model_name = os.path.expanduser(r\"~\\.cache\\nexa.ai\\nexa_sdk\\models\\NexaAI\\parakeet-npu\\weights-1-5.nexa\")\n",
        "\n",
        "    plugin_id = \"npu\"\n",
        "    # Example audio file (replace with your actual audio file)\n",
        "    audio_file = r\"path/to/audio\"  # Replace with actual audio file path\n",
        "\n",
        "    print(f\"Loading model: {model_name}\")\n",
        "    print(f\"Using plugin: {plugin_id}\")\n",
        "\n",
        "    audio_path = os.path.expanduser(audio_file)\n",
        "\n",
        "    if not os.path.exists(audio_path):\n",
        "        raise FileNotFoundError(f\"Audio file not found: {audio_path}\")\n",
        "\n",
        "    # Create ASR instance\n",
        "    asr = ASR.from_(\n",
        "        model=os.path.expanduser(model_name),\n",
        "        plugin_id=plugin_id,\n",
        "        device_id=None,\n",
        "    )\n",
        "    print('ASR model loaded successfully!')\n",
        "\n",
        "    print(f\"\\n=== Starting Transcription ===\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Perform transcription\n",
        "    result = asr.transcribe(\n",
        "        audio_path=audio_path,\n",
        "        language=\"en\",\n",
        "        timestamps=\"segment\",\n",
        "        beam_size=5,\n",
        "    )\n",
        "\n",
        "    end_time = time.time()\n",
        "    transcription_time = end_time - start_time\n",
        "\n",
        "    # Display results\n",
        "    print(f\"\\n=== Transcription Results ===\")\n",
        "    print(f\"Transcription: {result.transcript}\")\n",
        "    print(f\"Processing time: {transcription_time:.2f} seconds\")\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "result = asr_npu_example()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Reranker NPU Inference\n",
        "\n",
        "Using NPU-accelerated reranking models for document reranking. jina-v2-rerank-npu can perform precise similarity-based document ranking based on queries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from nexaai.rerank import Reranker\n",
        "\n",
        "\n",
        "def reranker_npu_example():\n",
        "    \"\"\"Reranker NPU inference example\"\"\"\n",
        "    print(\"=== Reranker NPU Inference Example ===\")\n",
        "\n",
        "    # Model configuration\n",
        "\n",
        "    # Use huggingface repo ID\n",
        "    model_name = \"NexaAI/jina-v2-rerank-npu\"\n",
        "    # Alternatively, use local path\n",
        "    # model_name = os.path.expanduser(r\"~\\.cache\\nexa.ai\\nexa_sdk\\models\\NexaAI\\jina-v2-rerank-npu\\weights-1-4.nexa\")\n",
        "\n",
        "    plugin_id = \"npu\"\n",
        "    batch_size = 4\n",
        "\n",
        "    print(f\"Loading model: {model_name}\")\n",
        "    print(f\"Using plugin: {plugin_id}\")\n",
        "    print(f\"Batch size: {batch_size}\")\n",
        "\n",
        "    # Create reranker instance\n",
        "    reranker = Reranker.from_(\n",
        "        model=os.path.expanduser(model_name),\n",
        "        plugin_id=plugin_id,\n",
        "    )\n",
        "\n",
        "    # Example queries and documents\n",
        "    queries = [\n",
        "        \"Where is on-device AI?\",\n",
        "        \"What is NPU acceleration?\",\n",
        "        \"How does machine learning work?\",\n",
        "        \"Tell me about computer vision\"\n",
        "    ]\n",
        "\n",
        "    documents = [\n",
        "        \"On-device AI is a type of AI that is processed on the device itself, rather than in the cloud.\",\n",
        "        \"NPU acceleration provides significant performance improvements for AI workloads on specialized hardware.\",\n",
        "        \"Edge computing brings computation and data storage closer to the sources of data.\",\n",
        "        \"A ragdoll is a breed of cat that is known for its long, flowing hair and gentle personality.\",\n",
        "        \"The capital of France is Paris, a beautiful city known for its art and culture.\",\n",
        "        \"Machine learning is a subset of artificial intelligence that enables computers to learn without being explicitly programmed.\",\n",
        "        \"Computer vision is a field of artificial intelligence that trains computers to interpret and understand visual information.\",\n",
        "        \"Deep learning uses neural networks with multiple layers to model and understand complex patterns in data.\"\n",
        "    ]\n",
        "\n",
        "    print(f\"\\n=== Document Reranking Test ===\")\n",
        "    print(f\"Number of documents: {len(documents)}\")\n",
        "\n",
        "    # Rerank for each query\n",
        "    for i, query in enumerate(queries, 1):\n",
        "        print(f\"\\n--- Query {i} ---\")\n",
        "        print(f\"Query: '{query}'\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        # Perform reranking\n",
        "        result = reranker.rerank(\n",
        "            query=query,\n",
        "            documents=documents,\n",
        "            batch_size=batch_size,\n",
        "        )\n",
        "        scores = result.scores\n",
        "\n",
        "        # Create (document, score) pairs and sort\n",
        "        doc_scores = list(zip(documents, scores))\n",
        "        doc_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Display ranking results\n",
        "        print(\"Reranking results:\")\n",
        "        for rank, (doc, score) in enumerate(doc_scores, 1):\n",
        "            print(f\"  {rank:2d}. [{score:.4f}] {doc}\")\n",
        "\n",
        "        # Display most relevant documents\n",
        "        print(f\"\\nMost relevant documents (top 3):\")\n",
        "        for rank, (doc, score) in enumerate(doc_scores[:3], 1):\n",
        "            print(f\"  {rank}. {doc}\")\n",
        "\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "    return reranker\n",
        "\n",
        "\n",
        "reranker = reranker_npu_example()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Computer Vision (CV) NPU Inference\n",
        "\n",
        "Run NPU-accelerated computer vision tasks (e.g., OCR/text recognition) on images.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from nexaai.cv import CV\n",
        "\n",
        "\n",
        "def cv_ocr_example():\n",
        "    \"\"\"CV OCR example\"\"\"\n",
        "    print(\"=== CV OCR Example ===\")\n",
        "\n",
        "    # Use huggingface repo ID\n",
        "    model_name = \"NexaAI/paddleocr-npu\"\n",
        "    # Alternatively, use local path\n",
        "    # model_name = os.path.expanduser(r\"~\\.cache\\nexa.ai\\nexa_sdk\\models\\NexaAI\\paddleocr-npu\\weights-1-1.nexa\")\n",
        "\n",
        "    image_path = r\"path/to/image\"\n",
        "\n",
        "    image_path = os.path.expanduser(image_path)\n",
        "\n",
        "    if not os.path.exists(image_path):\n",
        "        raise FileNotFoundError(f\"Image file not found: {image_path}\")\n",
        "\n",
        "    cv = CV.from_(\n",
        "        model=os.path.expanduser(model_name),\n",
        "        capabilities=0,  # 0=OCR\n",
        "        plugin_id='npu',\n",
        "    )\n",
        "\n",
        "    results = cv.infer(image_path)\n",
        "\n",
        "    print(f\"Number of results: {len(results.results)}\")\n",
        "    for result in results.results:\n",
        "        print(f\"[{result.confidence:.2f}] {result.text}\")\n",
        "\n",
        "\n",
        "cv_ocr_example()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Speech Diarization NPU Inference\n",
        "\n",
        "Run NPU-accelerated speech diarization tasks on audio files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from nexaai.diarize import Diarize\n",
        "\n",
        "\n",
        "def diarize_example():\n",
        "    \"\"\"Diarize NPU inference example\"\"\"\n",
        "    print(\"=== Diarize NPU Inference Example ===\")\n",
        "\n",
        "    # Use huggingface repo ID\n",
        "    model_name = \"NexaAI/Pyannote-NPU\"\n",
        "    # Alternatively, use local path\n",
        "    # model_name = os.path.expanduser(r\"~\\.cache\\nexa.ai\\nexa_sdk\\models\\NexaAI\\Pyannote-NPU\\weights-1-1.nexa\")\n",
        "\n",
        "    plugin_id = \"npu\"\n",
        "    audio_path = r\"path/to/audio\"  # Replace with actual audio file path\n",
        "\n",
        "    print(f\"Loading model: {model_name}\")\n",
        "    print(f\"Using plugin: {plugin_id}\")\n",
        "\n",
        "    audio_path = os.path.expanduser(audio_path)\n",
        "\n",
        "    if not os.path.exists(audio_path):\n",
        "        raise FileNotFoundError(f\"Audio file not found: {audio_path}\")\n",
        "\n",
        "    # Create Diarize instance\n",
        "    diarize = Diarize.from_(\n",
        "        model=os.path.expanduser(model_name),\n",
        "        plugin_id=plugin_id,\n",
        "        device_id=None,\n",
        "    )\n",
        "    print('Diarize model loaded successfully!')\n",
        "\n",
        "    print(f\"\\n=== Starting Diarization ===\")\n",
        "\n",
        "    # Perform diarization\n",
        "    result = diarize.infer(\n",
        "        audio_path=audio_path,\n",
        "        min_speakers=0,  # Auto-detect\n",
        "        max_speakers=0,  # No limit\n",
        "    )\n",
        "\n",
        "    # Display results\n",
        "    print(f\"\\n=== Diarization Results ===\")\n",
        "    print(f\"Number of speakers: {result.num_speakers}\")\n",
        "    print(f\"Duration: {result.duration:.2f}s\")\n",
        "    print(f\"Number of segments: {len(result.segments)}\")\n",
        "    print(\"\\nSegments:\")\n",
        "    for segment in result.segments:\n",
        "        print(\n",
        "            f\"[{segment.start_time:.2f}s - {segment.end_time:.2f}s] {segment.speaker_label}\"\n",
        "        )\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "result = diarize_example()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
