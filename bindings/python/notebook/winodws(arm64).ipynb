{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NexaAI NPU Inference Examples\n",
        "\n",
        "This notebook demonstrates how to use the NexaAI SDK for various AI inference tasks on NPU devices, including:\n",
        "\n",
        "- **LLM (Large Language Model)**: Text generation and conversation\n",
        "- **VLM (Vision Language Model)**: Multimodal understanding and generation\n",
        "- **Embedder**: Text vectorization and similarity computation\n",
        "- **Reranker**: Document reranking\n",
        "- **ASR (Automatic Speech Recognition)**: Speech-to-text transcription\n",
        "- **CV (Computer Vision)**: OCR/text recognition\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "### 1. Install the correct Python version\n",
        "\n",
        "If you prefer, we also offer a video tutorial for the installation. Check it out [here](https://www.youtube.com/watch?v=ziXKPRX0Ufo).\n",
        "\n",
        "NexaAI requires **Python 3.11 – 3.13 (ARM64 build)** on Windows ARM.\n",
        "Please **download and install the official ARM64 Python** from the [python-3.11.1-arm64.exe](https://www.python.org/ftp/python/3.11.1/python-3.11.1-arm64.exe). Make sure you read the instructions below carefully before proceeding.\n",
        "\n",
        "> ❗ **IMPORTANT**: Make sure you select \"Add python.exe to PATH\" on the first screen of the installation wizard.\n",
        "\n",
        "> 🛑 Make sure you restart the terminal or your IDE after installation.\n",
        "\n",
        "> ⚠️ Do **not** use Conda or x86 builds — they are incompatible with native ARM64 binaries. If you are in a conda environment, run `conda deactivate` first.\n",
        "\n",
        "Verify the installation:\n",
        "\n",
        "In case your environment path gets overriden by some environment manager, we recommend you to run the following commands to restore PATH variable from system settings.\n",
        "```powershell\n",
        "$systemPath = [Environment]::GetEnvironmentVariable('Path', 'Machine')\n",
        "$userPath   = [Environment]::GetEnvironmentVariable('Path', 'User')\n",
        "$env:Path   = \"$userPath;$systemPath\"\n",
        "```\n",
        "\n",
        "Then verify your python executable has the correct architecture and version (3.11 - 3.13)\n",
        "```sh\n",
        "python -c \"import sys, platform; print(f'Python version: {sys.version}')\"\n",
        "```\n",
        "\n",
        "Your output should look like:\n",
        "\n",
        "> Python version: 3.11.0 (main, Oct 24 2022, 18:15:22) [MSC v.1933 64 bit (ARM64)]\n",
        "\n",
        "Expected output must contain version `3.11.0` and architecture `ARM64`.\n",
        "\n",
        "If it does show `AMD64` or incorrect version, try the following:\n",
        "\n",
        "- (If you have conda installed) Run `conda deactivate` to deactivate the current conda environment.\n",
        "- (If your `python` executable points to the x86 version) You may need to make the ARM64 Python come before the x86 Python in your PATH.\n",
        "  - Hit the `Win` key, and type `env`, and hit Enter to select `Edit the system environment variables` setting.\n",
        "  - Click on `Environment Variables...` button.\n",
        "  - Select `Path` and click `Edit...`.\n",
        "  - Find your ARM64 Python installation path, and move it to the top of the list.\n",
        "  - Hit `OK` for several times to close all the dialogs and save the changes.\n",
        "- (If you forgot to select \"Add python.exe to PATH\" on the first screen of the installation wizard)\n",
        "  - Run the installation wizard again, follow the instructions to remove the current installation, and then reinstall from the Wizard. Make sure to select \"Add python.exe to PATH\" this time.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Create and activate a virtual environment\n",
        "\n",
        "`cd` to the current project root directory `cd path/to/nexa-sdk`.\n",
        "\n",
        "```sh\n",
        "python -m venv nexaai-env\n",
        "nexaai-env\\Scripts\\activate\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Install the NexaAI SDK\n",
        "\n",
        "```bash\n",
        "pip install nexaai\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 4. Select the venv as your Jupyter Notebook kernel\n",
        "\n",
        "- Depending the editor you are using, the way to change kernel might be different. For Cursor / VS Code, they are located at the top right corner of you code window.\n",
        "- Look for and select the `nexaai-env`, or the custom virtual environment you have created. The kernel should automatically reload in most IDEs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Verification of Kernel\n",
        "\n",
        "Run the following code to ensure you have the right kernel running.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import platform\n",
        "\n",
        "# ANSI color codes\n",
        "RED = \"\\033[91m\"\n",
        "GREEN = \"\\033[92m\"\n",
        "YELLOW = \"\\033[93m\"\n",
        "BOLD = \"\\033[1m\"\n",
        "RESET = \"\\033[0m\"\n",
        "\n",
        "min_ver = (3, 11)\n",
        "max_ver = (3, 13)\n",
        "current_ver = sys.version_info\n",
        "arch = platform.machine()\n",
        "\n",
        "if not (min_ver <= (current_ver.major, current_ver.minor) < max_ver) or arch.lower() != \"arm64\":\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(f\"{BOLD}{RED}WARNING: Your Python version or architecture is not compatible.{RESET}\")\n",
        "    print(f\"Detected version: {current_ver.major}.{current_ver.minor}, architecture: {arch}\")\n",
        "    print(f\"{YELLOW}Required: Python 3.11 - 3.13 & architecture 'arm64'.{RESET}\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"{RED}DO NOT continue to the following code!{RESET}\\n\")\n",
        "    print(\"To install arm64 Python:\")\n",
        "    print(\"  - Download Python 3.11-3.13 for arm64 from https://www.python.org/downloads/\")\n",
        "    print(\"  - Install and verify by running: python3 --version and python3 -c 'import platform; print(platform.machine())'\")\n",
        "    print(\"  - Launch Jupyter and make sure to select the arm64 Python kernel in 'Kernel > Change kernel'.\")\n",
        "    sys.exit(1)\n",
        "else:\n",
        "    print(f\"{GREEN}[VERIFICATION PASSED] Python version and architecture are correct. You may continue to the following sections.{RESET}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Authentication Setup\n",
        "\n",
        "Before running any examples, you need to set up your NexaAI authentication token.\n",
        "\n",
        "### Set Token in Code\n",
        "\n",
        "Replace `\"YOUR_NEXA_TOKEN_HERE\"` with your actual NexaAI token from [https://sdk.nexa.ai/](https://sdk.nexa.ai/):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Replace \"YOUR_NEXA_TOKEN_HERE\" with your actual token from https://sdk.nexa.ai/\n",
        "os.environ[\"NEXA_TOKEN\"] = \"YOUR_NEXA_TOKEN_HERE\"\n",
        "\n",
        "assert os.environ.get(\"NEXA_TOKEN\", \"\").startswith(\n",
        "    \"key/\"), \"ERROR: NEXA_TOKEN must start with 'key/'. Please check your token.\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. LLM (Large Language Model) NPU Inference\n",
        "\n",
        "Using NPU-accelerated large language models for text generation and conversation. Llama3.2-3B-NPU-Turbo is specifically optimized for NPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import io\n",
        "import os\n",
        "\n",
        "from nexaai.common import GenerationConfig, ModelConfig, ChatMessage\n",
        "from nexaai.llm import LLM\n",
        "\n",
        "\n",
        "def llm_npu_example():\n",
        "    \"\"\"LLM NPU inference example\"\"\"\n",
        "    print(\"=== LLM NPU Inference Example ===\")\n",
        "\n",
        "    # Model configuration\n",
        "\n",
        "    # Use huggingface Repo ID\n",
        "    model_name = \"NexaAI/Llama3.2-3B-NPU-Turbo\"\n",
        "    # Alternatively, use local path\n",
        "    # model_name = os.path.expanduser(r\"~\\.cache\\nexa.ai\\nexa_sdk\\models\\NexaAI\\Llama3.2-3B-NPU-Turbo\\weights-1-3.nexa\")\n",
        "\n",
        "    plugin_id = \"npu\"\n",
        "    device = \"npu\"\n",
        "    max_tokens = 100\n",
        "    system_message = \"You are a helpful assistant.\"\n",
        "\n",
        "    print(f\"Loading model: {model_name}\")\n",
        "    print(f\"Using plugin: {plugin_id}\")\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    # Create model instance\n",
        "    m_cfg = ModelConfig()\n",
        "    llm = LLM.from_(model_name, plugin_id=plugin_id, device_id=device, m_cfg=m_cfg)\n",
        "\n",
        "    # Create conversation history\n",
        "    conversation = [ChatMessage(role=\"system\", content=system_message)]\n",
        "\n",
        "    # Example conversations\n",
        "    test_prompts = [\n",
        "        \"What is artificial intelligence?\",\n",
        "        \"Explain the benefits of on-device AI processing.\",\n",
        "        \"How does NPU acceleration work?\"\n",
        "    ]\n",
        "\n",
        "    for i, prompt in enumerate(test_prompts, 1):\n",
        "        print(f\"\\n--- Conversation {i} ---\")\n",
        "        print(f\"User: {prompt}\")\n",
        "\n",
        "        # Add user message\n",
        "        conversation.append(ChatMessage(role=\"user\", content=prompt))\n",
        "\n",
        "        # Apply chat template\n",
        "        formatted_prompt = llm.apply_chat_template(conversation)\n",
        "\n",
        "        # Generate response\n",
        "        print(\"Assistant: \", end=\"\", flush=True)\n",
        "        response_buffer = io.StringIO()\n",
        "\n",
        "        for token in llm.generate_stream(formatted_prompt, g_cfg=GenerationConfig(max_tokens=max_tokens)):\n",
        "            print(token, end=\"\", flush=True)\n",
        "            response_buffer.write(token)\n",
        "\n",
        "        # Get profiling data\n",
        "        profiling_data = llm.get_profiling_data()\n",
        "        if profiling_data:\n",
        "            print(f\"\\nProfiling data: {profiling_data}\")\n",
        "\n",
        "        # Add assistant response to conversation history\n",
        "        conversation.append(ChatMessage(role=\"assistant\", content=response_buffer.getvalue()))\n",
        "        print(\"\\n\" + \"=\" * 50)\n",
        "\n",
        "\n",
        "llm_npu_example()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. VLM (Vision Language Model) NPU Inference\n",
        "\n",
        "Using NPU-accelerated vision language models for multimodal understanding and generation. OmniNeural-4B supports joint processing of images and text.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import io\n",
        "\n",
        "from nexaai.vlm import VLM\n",
        "from nexaai.common import GenerationConfig, ModelConfig, MultiModalMessage, MultiModalMessageContent\n",
        "\n",
        "\n",
        "def vlm_npu_example():\n",
        "    \"\"\"VLM NPU inference example\"\"\"\n",
        "    print(\"=== VLM NPU Inference Example ===\")\n",
        "\n",
        "    # Model configuration\n",
        "\n",
        "    # Use huggingface repo ID\n",
        "    model_name = \"NexaAI/OmniNeural-4B\"\n",
        "    # Alternatively, use local path\n",
        "    # model_name = os.path.expanduser(r\"~\\.cache\\nexa.ai\\nexa_sdk\\models\\NexaAI\\OmniNeural-4B\\weights-1-8.nexa\")\n",
        "\n",
        "    plugin_id = \"npu\"\n",
        "    device = \"npu\"\n",
        "    max_tokens = 100\n",
        "    system_message = \"You are a helpful assistant that can understand images and text.\"\n",
        "    image_path = '/your/image/path'  # Replace with actual image path if available\n",
        "\n",
        "    print(f\"Loading model: {model_name}\")\n",
        "    print(f\"Using plugin: {plugin_id}\")\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    # Check for image existence\n",
        "    if not (image_path and os.path.exists(image_path)):\n",
        "        print(\n",
        "            f\"\\033[93mWARNING: The specified image_path ('{image_path}') does not exist or was not provided. Multimodal prompts will not include image input.\\033[0m\")\n",
        "\n",
        "    # Create model instance\n",
        "    m_cfg = ModelConfig()\n",
        "    vlm = VLM.from_(name_or_path=model_name, m_cfg=m_cfg, plugin_id=plugin_id, device_id=device)\n",
        "\n",
        "    # Create conversation history\n",
        "    conversation = [MultiModalMessage(role=\"system\",\n",
        "                                      content=[MultiModalMessageContent(type=\"text\", text=system_message)])]\n",
        "\n",
        "    # Example multimodal conversations\n",
        "    test_cases = [\n",
        "        {\n",
        "            \"text\": \"What do you see in this image?\",\n",
        "            \"image_path\": image_path\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    for i, case in enumerate(test_cases, 1):\n",
        "        print(f\"\\n--- Multimodal Conversation {i} ---\")\n",
        "        print(f\"User: {case['text']}\")\n",
        "\n",
        "        # Build message content\n",
        "        contents = [MultiModalMessageContent(type=\"text\", text=case['text'])]\n",
        "\n",
        "        # Add image content if available\n",
        "        if case['image_path'] and os.path.exists(case['image_path']):\n",
        "            contents.append(MultiModalMessageContent(type=\"image\", path=case['image_path']))\n",
        "            print(f\"Including image: {case['image_path']}\")\n",
        "\n",
        "        # Add user message\n",
        "        conversation.append(MultiModalMessage(role=\"user\", content=contents))\n",
        "\n",
        "        # Apply chat template\n",
        "        formatted_prompt = vlm.apply_chat_template(conversation)\n",
        "\n",
        "        # Generate response\n",
        "        print(\"Assistant: \", end=\"\", flush=True)\n",
        "        response_buffer = io.StringIO()\n",
        "\n",
        "        # Prepare image and audio paths\n",
        "        image_paths = [case['image_path']] if case['image_path'] and os.path.exists(case['image_path']) else None\n",
        "        audio_paths = None\n",
        "\n",
        "        for token in vlm.generate_stream(formatted_prompt,\n",
        "                                         g_cfg=GenerationConfig(max_tokens=max_tokens,\n",
        "                                                                image_paths=image_paths,\n",
        "                                                                audio_paths=audio_paths)):\n",
        "            print(token, end=\"\", flush=True)\n",
        "            response_buffer.write(token)\n",
        "\n",
        "        # Get profiling data\n",
        "        profiling_data = vlm.get_profiling_data()\n",
        "        if profiling_data:\n",
        "            print(f\"\\nProfiling data: {profiling_data}\")\n",
        "\n",
        "        # Add assistant response to conversation history\n",
        "        conversation.append(MultiModalMessage(role=\"assistant\",\n",
        "                                              content=[MultiModalMessageContent(type=\"text\", text=response_buffer.getvalue())]))\n",
        "        print(\"\\n\" + \"=\" * 50)\n",
        "\n",
        "\n",
        "vlm_npu_example()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Embedder NPU Inference\n",
        "\n",
        "Using NPU-accelerated embedding models for text vectorization and similarity computation. embeddinggemma-300m-npu is a lightweight embedding model specifically optimized for NPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from nexaai.embedder import Embedder, EmbeddingConfig\n",
        "\n",
        "# Embedder NPU Inference Example\n",
        "\n",
        "\n",
        "def embedder_npu_example():\n",
        "    \"\"\"Embedder NPU inference example\"\"\"\n",
        "    print(\"=== Embedder NPU Inference Example ===\")\n",
        "\n",
        "    # Model configuration\n",
        "\n",
        "    # Use huggingface repo ID\n",
        "    model_name = \"NexaAI/embeddinggemma-300m-npu\"\n",
        "    # Alternatively, use local path\n",
        "    # model_name = os.path.expanduser(r\"~\\.cache\\nexa.ai\\nexa_sdk\\models\\NexaAI\\embeddinggemma-300m-npu\\weights-1-2.nexa\")\n",
        "\n",
        "    plugin_id = \"npu\"\n",
        "    batch_size = 2\n",
        "\n",
        "    print(f\"Loading model: {model_name}\")\n",
        "    print(f\"Using plugin: {plugin_id}\")\n",
        "    print(f\"Batch size: {batch_size}\")\n",
        "\n",
        "    # Create embedder instance\n",
        "    embedder = Embedder.from_(name_or_path=model_name, plugin_id=plugin_id)\n",
        "    print('Embedder loaded successfully!')\n",
        "\n",
        "    # Get embedding dimension\n",
        "    dim = embedder.get_embedding_dim()\n",
        "    print(f\"Embedding dimension: {dim}\")\n",
        "\n",
        "    # Example texts\n",
        "    texts = [\n",
        "        \"On-device AI is a type of AI that is processed on the device itself, rather than in the cloud.\",\n",
        "        \"Nexa AI allows you to run state-of-the-art AI models locally on CPU, GPU, or NPU.\",\n",
        "        \"A ragdoll is a breed of cat that is known for its long, flowing hair and gentle personality.\",\n",
        "        \"The capital of France is Paris.\",\n",
        "        \"NPU acceleration provides significant performance improvements for AI workloads.\"\n",
        "    ]\n",
        "\n",
        "    query = \"what is on device AI\"\n",
        "\n",
        "    print(f\"\\n=== Generating Embeddings ===\")\n",
        "    print(f\"Processing {len(texts)} texts...\")\n",
        "\n",
        "    # Generate embeddings\n",
        "    embeddings = embedder.generate(\n",
        "        texts=texts,\n",
        "        config=EmbeddingConfig(batch_size=batch_size)\n",
        "    )\n",
        "\n",
        "    print(f\"Successfully generated {len(embeddings)} embeddings\")\n",
        "\n",
        "    # Display embedding information\n",
        "    print(f\"\\n=== Embedding Details ===\")\n",
        "    for i, (text, embedding) in enumerate(zip(texts, embeddings)):\n",
        "        print(f\"\\nText {i + 1}:\")\n",
        "        print(f\"  Content: {text}\")\n",
        "        print(f\"  Embedding dimension: {len(embedding)}\")\n",
        "        print(f\"  First 10 elements: {embedding[:10]}\")\n",
        "        print(\"-\" * 70)\n",
        "\n",
        "    # Query processing\n",
        "    print(f\"\\n=== Query Processing ===\")\n",
        "    print(f\"Query: '{query}'\")\n",
        "\n",
        "    query_embedding = embedder.generate(\n",
        "        texts=[query],\n",
        "        config=EmbeddingConfig(batch_size=1)\n",
        "    )[0]\n",
        "\n",
        "    print(f\"Query embedding dimension: {len(query_embedding)}\")\n",
        "\n",
        "    # Similarity analysis\n",
        "    print(f\"\\n=== Similarity Analysis (Inner Product) ===\")\n",
        "    similarities = []\n",
        "\n",
        "    for i, (text, embedding) in enumerate(zip(texts, embeddings)):\n",
        "        query_vec = np.array(query_embedding)\n",
        "        text_vec = np.array(embedding)\n",
        "        inner_product = np.dot(query_vec, text_vec)\n",
        "        similarities.append((i, text, inner_product))\n",
        "\n",
        "        print(f\"\\nText {i + 1}:\")\n",
        "        print(f\"  Content: {text}\")\n",
        "        print(f\"  Inner product with query: {inner_product:.6f}\")\n",
        "        print(\"-\" * 70)\n",
        "\n",
        "    # Sort and display most similar texts\n",
        "    similarities.sort(key=lambda x: x[2], reverse=True)\n",
        "\n",
        "    print(f\"\\n=== Similarity Ranking Results ===\")\n",
        "    for rank, (idx, text, score) in enumerate(similarities, 1):\n",
        "        print(f\"Rank {rank}: [{score:.6f}] {text}\")\n",
        "\n",
        "    return embeddings, query_embedding, similarities\n",
        "\n",
        "\n",
        "embeddings, query_emb, similarities = embedder_npu_example()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. ASR (Automatic Speech Recognition) NPU Inference\n",
        "\n",
        "Using NPU-accelerated speech recognition models for speech-to-text transcription. parakeet-npu provides high-quality speech recognition with NPU acceleration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "from nexaai.asr import ASR, ASRConfig\n",
        "\n",
        "\n",
        "def asr_npu_example():\n",
        "    \"\"\"ASR NPU inference example\"\"\"\n",
        "    print(\"=== ASR NPU Inference Example ===\")\n",
        "\n",
        "    # Model configuration\n",
        "\n",
        "    # Use huggingface Repo ID\n",
        "    model_name = \"NexaAI/parakeet-npu\"\n",
        "    # Alternatively, use local path\n",
        "    # model_name = os.path.expanduser(r\"~\\.cache\\nexa.ai\\nexa_sdk\\models\\NexaAI\\parakeet-npu\\weights-1-5.nexa\")\n",
        "\n",
        "    plugin_id = \"npu\"\n",
        "    device = \"npu\"\n",
        "    # Example audio file (replace with your actual audio file)\n",
        "    audio_file = r\"path/to/audio\"  # Replace with actual audio file path\n",
        "\n",
        "    print(f\"Loading model: {model_name}\")\n",
        "    print(f\"Using plugin: {plugin_id}\")\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    assert os.path.exists(\n",
        "        audio_file), f\"ERROR: The specified audio_file ('{audio_file}') does not exist. Please provide a valid audio file path to test ASR functionality.\"\n",
        "\n",
        "    # Create ASR instance\n",
        "    asr = ASR.from_(name_or_path=model_name, plugin_id=plugin_id, device_id=device)\n",
        "    print('ASR model loaded successfully!')\n",
        "\n",
        "    print(f\"\\nNote: Please update the audio_file path to point to your audio file\")\n",
        "    print(f\"Current audio_file: {audio_file}\")\n",
        "\n",
        "    # Check if audio file exists\n",
        "    if not os.path.exists(audio_file):\n",
        "        print(f\"Error: Audio file not found: {audio_file}\")\n",
        "        print(\"Please provide a valid audio file path to test ASR functionality.\")\n",
        "        return None\n",
        "\n",
        "    # Basic ASR configuration\n",
        "    config = ASRConfig(\n",
        "        timestamps=\"segment\",  # Get segment-level timestamps\n",
        "        beam_size=5,\n",
        "        stream=False\n",
        "    )\n",
        "\n",
        "    print(f\"\\n=== Starting Transcription ===\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Perform transcription\n",
        "    result = asr.transcribe(audio_path=audio_file, language=\"en\", config=config)\n",
        "\n",
        "    end_time = time.time()\n",
        "    transcription_time = end_time - start_time\n",
        "\n",
        "    # Display results\n",
        "    print(f\"\\n=== Transcription Results ===\")\n",
        "    print(f\"Transcription: {result.transcript}\")\n",
        "    print(f\"Processing time: {transcription_time:.2f} seconds\")\n",
        "\n",
        "    # Display segment information if available\n",
        "    if hasattr(result, 'segments') and result.segments:\n",
        "        print(f\"\\nSegments ({len(result.segments)}):\")\n",
        "        for i, segment in enumerate(result.segments[:3]):  # Show first 3 segments\n",
        "            start_time = segment.get('start', 'N/A')\n",
        "            end_time = segment.get('end', 'N/A')\n",
        "            text = segment.get('text', '').strip()\n",
        "            print(f\"  {i +1}. [{start_time:.2f}s - {end_time:.2f}s] {text}\")\n",
        "        if len(result.segments) > 3:\n",
        "            print(f\"  ... and {len(result.segments) - 3} more segments\")\n",
        "\n",
        "    # Get profiling data\n",
        "    profiling_data = asr.get_profiling_data()\n",
        "    if profiling_data:\n",
        "        print(f\"\\nProfiling data: {profiling_data}\")\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "result = asr_npu_example()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Reranker NPU Inference\n",
        "\n",
        "Using NPU-accelerated reranking models for document reranking. jina-v2-rerank-npu can perform precise similarity-based document ranking based on queries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nexaai.rerank import Reranker, RerankConfig\n",
        "\n",
        "\n",
        "def reranker_npu_example():\n",
        "    \"\"\"Reranker NPU inference example\"\"\"\n",
        "    print(\"=== Reranker NPU Inference Example ===\")\n",
        "\n",
        "    # Model configuration\n",
        "\n",
        "    # Use huggingface repo ID\n",
        "    model_name = \"NexaAI/jina-v2-rerank-npu\"\n",
        "    # Alternatively, use local path\n",
        "    # model_name = os.path.expanduser(r\"~\\.cache\\nexa.ai\\nexa_sdk\\models\\NexaAI\\jina-v2-rerank-npu\\weights-1-4.nexa\")\n",
        "\n",
        "    plugin_id = \"npu\"\n",
        "    batch_size = 4\n",
        "\n",
        "    print(f\"Loading model: {model_name}\")\n",
        "    print(f\"Using plugin: {plugin_id}\")\n",
        "    print(f\"Batch size: {batch_size}\")\n",
        "\n",
        "    # Create reranker instance\n",
        "    reranker = Reranker.from_(name_or_path=model_name, plugin_id=plugin_id)\n",
        "    print('Reranker loaded successfully!')\n",
        "\n",
        "    # Example queries and documents\n",
        "    queries = [\n",
        "        \"Where is on-device AI?\",\n",
        "        \"What is NPU acceleration?\",\n",
        "        \"How does machine learning work?\",\n",
        "        \"Tell me about computer vision\"\n",
        "    ]\n",
        "\n",
        "    documents = [\n",
        "        \"On-device AI is a type of AI that is processed on the device itself, rather than in the cloud.\",\n",
        "        \"NPU acceleration provides significant performance improvements for AI workloads on specialized hardware.\",\n",
        "        \"Edge computing brings computation and data storage closer to the sources of data.\",\n",
        "        \"A ragdoll is a breed of cat that is known for its long, flowing hair and gentle personality.\",\n",
        "        \"The capital of France is Paris, a beautiful city known for its art and culture.\",\n",
        "        \"Machine learning is a subset of artificial intelligence that enables computers to learn without being explicitly programmed.\",\n",
        "        \"Computer vision is a field of artificial intelligence that trains computers to interpret and understand visual information.\",\n",
        "        \"Deep learning uses neural networks with multiple layers to model and understand complex patterns in data.\"\n",
        "    ]\n",
        "\n",
        "    print(f\"\\n=== Document Reranking Test ===\")\n",
        "    print(f\"Number of documents: {len(documents)}\")\n",
        "\n",
        "    # Rerank for each query\n",
        "    for i, query in enumerate(queries, 1):\n",
        "        print(f\"\\n--- Query {i} ---\")\n",
        "        print(f\"Query: '{query}'\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        # Perform reranking\n",
        "        scores = reranker.rerank(\n",
        "            query=query,\n",
        "            documents=documents,\n",
        "            config=RerankConfig(batch_size=batch_size)\n",
        "        )\n",
        "\n",
        "        # Create (document, score) pairs and sort\n",
        "        doc_scores = list(zip(documents, scores))\n",
        "        doc_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Display ranking results\n",
        "        print(\"Reranking results:\")\n",
        "        for rank, (doc, score) in enumerate(doc_scores, 1):\n",
        "            print(f\"  {rank:2d}. [{score:.4f}] {doc}\")\n",
        "\n",
        "        # Display most relevant documents\n",
        "        print(f\"\\nMost relevant documents (top 3):\")\n",
        "        for rank, (doc, score) in enumerate(doc_scores[:3], 1):\n",
        "            print(f\"  {rank}. {doc}\")\n",
        "\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "    return reranker\n",
        "\n",
        "\n",
        "reranker = reranker_npu_example()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Computer Vision (CV) NPU Inference\n",
        "\n",
        "Run NPU-accelerated computer vision tasks (e.g., OCR/text recognition) on images.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from nexaai.cv import CVCapabilities, CVModel, CVModelConfig, CVResults\n",
        "\n",
        "\n",
        "def cv_ocr_example():\n",
        "\n",
        "    # Use huggingface repo ID\n",
        "    model_name = \"NexaAI/paddleocr-npu\"\n",
        "    # Alternatively, use local path\n",
        "    # model_name = os.path.expanduser(r\"~\\.cache\\nexa.ai\\nexa_sdk\\models\\NexaAI\\paddleocr-npu\\weights-1-1.nexa\")\n",
        "\n",
        "    image_path = r\"path/to/image\"\n",
        "\n",
        "    config = CVModelConfig(capabilities=CVCapabilities.OCR)\n",
        "    cv = CVModel.from_(name_or_path=model_name, config=config, plugin_id='npu')\n",
        "\n",
        "    assert os.path.exists(image_path), f\"ERROR: Image file not found: {image_path}\"\n",
        "\n",
        "    results = cv.infer(image_path)\n",
        "\n",
        "    print(f\"Number of results: {results.result_count}\")\n",
        "    for result in results.results:\n",
        "        print(f\"[{result.confidence:.2f}] {result.text}\")\n",
        "\n",
        "\n",
        "cv_ocr_example()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
